{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Coperr/PatentMatch/blob/main/Task%201/dense_retrieval_2.0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3E4iAVd-eqO5"
      },
      "source": [
        "## !! Important: Do not forget to save your notebook frequently to avoid losing your progress if the colab session crashes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CVR1DtqCeqO7"
      },
      "source": [
        "### Download precalculated embedding tables\n",
        "\n",
        "We have preencoded all the texts into vectors and saved them as numpy tables to save time. You can download this zipped file to your current Google Colab working space under the `content` directory by following the steps below, ensuring it won't occupy any space in your Google Drive.\n",
        "\n",
        "However, please note that you will need to redownload it each time you quit Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yxEkWrtVeqO8",
        "outputId": "7415e185-f187-4e55-ef57-76e1debcecb2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1DHw-DdRB8xjTqvK-jU3Sol_-8VG4ACZK\n",
            "From (redirected): https://drive.google.com/uc?id=1DHw-DdRB8xjTqvK-jU3Sol_-8VG4ACZK&confirm=t&uuid=d4799421-fee8-4a09-bcf3-0cca59c4565e\n",
            "To: /content/embeddings.tar.gz\n",
            "100% 316M/316M [00:03<00:00, 105MB/s]\n"
          ]
        }
      ],
      "source": [
        "# download precalculated embeddings via a google drive shared link\n",
        "!gdown 'https://drive.google.com/uc?id=1DHw-DdRB8xjTqvK-jU3Sol_-8VG4ACZK'\n",
        "\n",
        "# if error \"failed to retrieve file url: too many users ....\" ==> solution:\n",
        "# https://stackoverflow.com/questions/65312867/how-to-download-large-file-from-google-drive-from-terminal-gdown-doesnt-work"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q-oVL1rHeqO9"
      },
      "source": [
        "if error \"failed to retrieve file url: too many users ....\" => solution:\n",
        "https://stackoverflow.com/questions/65312867/how-to-download-large-file-from-google-drive-from-terminal-gdown-doesnt-work\n",
        "\n",
        "\n",
        "**TODO: replace ACCESS_TOKEN in the next block** (your personal token generateed following the steps in previous solution)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "n4Sc7jQDeqO9"
      },
      "outputs": [],
      "source": [
        "# ACCESS_TOKEN = \"ya29.a0Ad52N39RvcgSxR64dCWPRT2zf7ryi0ib0oPhXi7lte1Nho0Agtz0eTf7cmND4VDIQGSHw0IA6RqL_Zi1T7MsYiObSIrdn3anaSMSZPUBN1To4WSDcuPcYRkUC6xoZOdU2MkHTb_IZCQI5nOJEdY0UH20rNAG4cAXBdsvaCgYKAUgSARMSFQHGX2Mi3HfwxDpNGH4ku8ghMpfEPg0171\" # CHANGE TO YOUR OWN TOKEN\n",
        "# FILE_ID1 = \"1DHw-DdRB8xjTqvK-jU3Sol_-8VG4ACZK\" # DO NOT CHANGE\n",
        "# FILE_NAME1 = \"embeddings.tar.gz\" # DO NOT CHANGE\n",
        "\n",
        "# # print the command that you are going to run\n",
        "# print(f'! curl -H \"Authorization: Bearer {ACCESS_TOKEN}\" https://www.googleapis.com/drive/v3/files/{FILE_ID1}?alt=media -o {FILE_NAME1}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "q6jYp-QBeqO9"
      },
      "outputs": [],
      "source": [
        "# # run the command\n",
        "# ! curl -H \"Authorization: Bearer ya29.a0Ad52N39RvcgSxR64dCWPRT2zf7ryi0ib0oPhXi7lte1Nho0Agtz0eTf7cmND4VDIQGSHw0IA6RqL_Zi1T7MsYiObSIrdn3anaSMSZPUBN1To4WSDcuPcYRkUC6xoZOdU2MkHTb_IZCQI5nOJEdY0UH20rNAG4cAXBdsvaCgYKAUgSARMSFQHGX2Mi3HfwxDpNGH4ku8ghMpfEPg0171\" https://www.googleapis.com/drive/v3/files/1ZNHJE4qXLgRLlQGYjGtpufd-56ydZfM4?alt=media -o embeddings.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QDZGu7p1eqO9",
        "outputId": "39d5cbe1-e76c-4da9-ea80-2189a23e178c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "embeddings_precalculated_docs/\n",
            "embeddings_precalculated_docs/embeddings_PatentSBERTa_mean_TA.npy\n",
            "embeddings_precalculated_docs/app_ids_PatentSBERTa_mean_TA.json\n",
            "embeddings_precalculated_docs/embeddings_PatentSBERTa_mean_claims.npy\n",
            "embeddings_precalculated_docs/app_ids_PatentSBERTa_mean_claims.json\n",
            "embeddings_precalculated_docs/embeddings_PatentSBERTa_mean_TAC.npy\n",
            "embeddings_precalculated_docs/app_ids_PatentSBERTa_mean_TAC.json\n",
            "embeddings_precalculated_docs/embeddings_all-MiniLM-L6-v2_mean_TA.npy\n",
            "embeddings_precalculated_docs/app_ids_all-MiniLM-L6-v2_mean_TA.json\n",
            "embeddings_precalculated_docs/embeddings_all-MiniLM-L6-v2_mean_claims.npy\n",
            "embeddings_precalculated_docs/app_ids_all-MiniLM-L6-v2_mean_claims.json\n",
            "embeddings_precalculated_docs/embeddings_all-MiniLM-L6-v2_mean_TAC.npy\n",
            "embeddings_precalculated_docs/app_ids_all-MiniLM-L6-v2_mean_TAC.json\n",
            "embeddings_precalculated_train/\n",
            "embeddings_precalculated_train/embeddings_PatentSBERTa_mean_TA.npy\n",
            "embeddings_precalculated_train/app_ids_PatentSBERTa_mean_TA.json\n",
            "embeddings_precalculated_train/embeddings_PatentSBERTa_mean_claims.npy\n",
            "embeddings_precalculated_train/app_ids_PatentSBERTa_mean_claims.json\n",
            "embeddings_precalculated_train/embeddings_PatentSBERTa_mean_TAC.npy\n",
            "embeddings_precalculated_train/app_ids_PatentSBERTa_mean_TAC.json\n",
            "embeddings_precalculated_train/embeddings_all-MiniLM-L6-v2_mean_TA.npy\n",
            "embeddings_precalculated_train/app_ids_all-MiniLM-L6-v2_mean_TA.json\n",
            "embeddings_precalculated_train/embeddings_all-MiniLM-L6-v2_mean_claims.npy\n",
            "embeddings_precalculated_train/app_ids_all-MiniLM-L6-v2_mean_claims.json\n",
            "embeddings_precalculated_train/embeddings_all-MiniLM-L6-v2_mean_TAC.npy\n",
            "embeddings_precalculated_train/app_ids_all-MiniLM-L6-v2_mean_TAC.json\n",
            "embeddings_precalculated_test/\n",
            "embeddings_precalculated_test/embeddings_PatentSBERTa_mean_TA.npy\n",
            "embeddings_precalculated_test/app_ids_PatentSBERTa_mean_TA.json\n",
            "embeddings_precalculated_test/embeddings_PatentSBERTa_mean_claims.npy\n",
            "embeddings_precalculated_test/app_ids_PatentSBERTa_mean_claims.json\n",
            "embeddings_precalculated_test/embeddings_PatentSBERTa_mean_TAC.npy\n",
            "embeddings_precalculated_test/app_ids_PatentSBERTa_mean_TAC.json\n",
            "embeddings_precalculated_test/embeddings_all-MiniLM-L6-v2_mean_TA.npy\n",
            "embeddings_precalculated_test/app_ids_all-MiniLM-L6-v2_mean_TA.json\n",
            "embeddings_precalculated_test/embeddings_all-MiniLM-L6-v2_mean_claims.npy\n",
            "embeddings_precalculated_test/app_ids_all-MiniLM-L6-v2_mean_claims.json\n",
            "embeddings_precalculated_test/embeddings_all-MiniLM-L6-v2_mean_TAC.npy\n",
            "embeddings_precalculated_test/app_ids_all-MiniLM-L6-v2_mean_TAC.json\n"
          ]
        }
      ],
      "source": [
        "# unzip the file  (may take around 10 mins)\n",
        "! tar -xvzf ./embeddings.tar.gz\n",
        "\n",
        "# remove the zip file\n",
        "! rm embeddings.tar.gz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C6oSSGD_eqO9"
      },
      "source": [
        "### Download citation mapping\n",
        "\n",
        "You can download them in the same manner as embedding tables, or alternatively, drag them directly into the `content` directory if you've already downloaded the file to your local PC."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-a_gfogheqO-",
        "outputId": "97a66a7d-7aa9-4425-aa93-190cac4edace"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1cbXtZBzBMRmLsBX4W08pCWnK0dJNYTun\n",
            "To: /content/citation_mapping.tar.gz\n",
            "\r  0% 0.00/264k [00:00<?, ?B/s]\r100% 264k/264k [00:00<00:00, 71.9MB/s]\n"
          ]
        }
      ],
      "source": [
        "! gdown 'https://drive.google.com/uc?id=1cbXtZBzBMRmLsBX4W08pCWnK0dJNYTun'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "_Nk8ZeFZeqO-"
      },
      "outputs": [],
      "source": [
        "# # same solution if error \"failed to retrieve file url: too many users ....\"\n",
        "\n",
        "# FILE_ID2 = \"1cbXtZBzBMRmLsBX4W08pCWnK0dJNYTun\" # DO NOT CHANGE\n",
        "# FILE_NAME2 = \"citation_mapping.tar.gz\" # DO NOT CHANGE\n",
        "\n",
        "# # print the command that you are going to run\n",
        "# print(f'! curl -H \"Authorization: Bearer {ACCESS_TOKEN}\" https://www.googleapis.com/drive/v3/files/{FILE_ID2}?alt=media -o {FILE_NAME2}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "156Lk_CpeqO-"
      },
      "outputs": [],
      "source": [
        "## run the command\n",
        "# ! curl -H \"Authorization: Bearer ya29.a0Ad52N38YT1rg1pylsmsVav74fF8nFNaHSaIQMRNEmu47r4fdmNqf1R6GzzgavIVgLJlkwB2s1K5d2oaxq15VVb9dMIBLPhY9Ul0G4GSrP_jNgYwljBCV7oEMIAVkIatlCuoM5DBqZEfOu05aaLSvYU63Y_qsvhFWn9AaaCgYKAQMSARMSFQHGX2Mi8czeY5M2cvOV-OUbAFmYkw0171\" https://www.googleapis.com/drive/v3/files/1mOCcOmM_AcLG3qK_mxM_SYp-2OgDys-L?alt=media -o citation_mapping.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8_M9T0jieqO-",
        "outputId": "1c8dd6d9-c061-41eb-dabe-85a327aa1b1d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Citation_JSONs/\n",
            "Citation_JSONs/Citation_Train.json\n"
          ]
        }
      ],
      "source": [
        "# unzip the file  (may take around 10 mins)\n",
        "! tar -xvzf ./citation_mapping.tar.gz\n",
        "\n",
        "# remove the zip file\n",
        "! rm citation_mapping.tar.gz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qM1Pj-ioeqO-"
      },
      "source": [
        "# Dense retrieval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "cHHm-AV8eqO-"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "from pathlib import Path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "si8uyrk9eqO_"
      },
      "outputs": [],
      "source": [
        "# Model settings\n",
        "MODEL_NAME = \"all-MiniLM-L6-v2\"  # Choose from: \"all-MiniLM-L6-v2\" or \"PatentSBERTa\"\n",
        "CONTENT_TYPE = \"TA\"              # Choose from: \"TA\", \"claims\", or \"TAC\"\n",
        "POOLING = \"mean\"                 # The pooling strategy used in create_embeddings.py\n",
        "QUERY_SET = \"train\"              # Choose from: \"train\" or \"test\"\n",
        "SAVE_RESULTS = False\n",
        "\n",
        "# Retrieval settings\n",
        "TOP_N = 100  # Number of documents to retrieve for each query"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "leXBaamIeqO_"
      },
      "outputs": [],
      "source": [
        "# Colab paths\n",
        "BASE_DIR = \"/content\"\n",
        "DOC_EMBEDDING_DIR = os.path.join(BASE_DIR, \"embeddings_precalculated_docs\")\n",
        "TRAIN_EMBEDDING_DIR = os.path.join(BASE_DIR, \"embeddings_precalculated_train\")\n",
        "TEST_EMBEDDING_DIR = os.path.join(BASE_DIR, \"embeddings_precalculated_test\")\n",
        "OUTPUT_DIR = os.path.join(BASE_DIR, \"results\")\n",
        "CITATION_FILE = os.path.join(BASE_DIR, \"Citation_JSONs/Citation_Train.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "rsFfnqzleqO_"
      },
      "outputs": [],
      "source": [
        "# Embedding files\n",
        "DOC_EMBEDDING_FILE = os.path.join(DOC_EMBEDDING_DIR, f\"embeddings_{MODEL_NAME}_{POOLING}_{CONTENT_TYPE}.npy\")\n",
        "DOC_APP_IDS_FILE = os.path.join(DOC_EMBEDDING_DIR, f\"app_ids_{MODEL_NAME}_{POOLING}_{CONTENT_TYPE}.json\")\n",
        "\n",
        "# Select query embedding directory based on QUERY_SET\n",
        "QUERY_EMBEDDING_DIR = TRAIN_EMBEDDING_DIR if QUERY_SET == \"train\" else TEST_EMBEDDING_DIR\n",
        "QUERY_EMBEDDING_FILE = os.path.join(QUERY_EMBEDDING_DIR, f\"embeddings_{MODEL_NAME}_{POOLING}_{CONTENT_TYPE}.npy\")\n",
        "QUERY_APP_IDS_FILE = os.path.join(QUERY_EMBEDDING_DIR, f\"app_ids_{MODEL_NAME}_{POOLING}_{CONTENT_TYPE}.json\")\n",
        "\n",
        "# Evaluation settings\n",
        "K_VALUE = 10  # K value for Recall@K evaluation\n",
        "METRICS_TYPE = \"all\"  # Metrics to calculate: \"recall_at_k\", \"mean_ranking\", \"mean_inv_ranking\", or \"all\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w64uFYNoeqO_"
      },
      "source": [
        "## Utilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "5uh1OV-OeqO_"
      },
      "outputs": [],
      "source": [
        "# === CELL: Utility functions for cosine similarity ===\n",
        "def cos_sim(a, b):\n",
        "    \"\"\"\n",
        "    Computes the cosine similarity cos_sim(a[i], b[j]) for all i and j.\n",
        "    :return: Matrix with res[i][j] = cos_sim(a[i], b[j])\n",
        "    \"\"\"\n",
        "    if not isinstance(a, torch.Tensor):\n",
        "        a = torch.tensor(a)\n",
        "\n",
        "    if not isinstance(b, torch.Tensor):\n",
        "        b = torch.tensor(b)\n",
        "\n",
        "    if len(a.shape) == 1:\n",
        "        a = a.unsqueeze(0)\n",
        "\n",
        "    if len(b.shape) == 1:\n",
        "        b = b.unsqueeze(0)\n",
        "\n",
        "    a_norm = torch.nn.functional.normalize(a, p=2, dim=1)\n",
        "    b_norm = torch.nn.functional.normalize(b, p=2, dim=1)\n",
        "    return torch.mm(a_norm, b_norm.transpose(0, 1))\n",
        "\n",
        "def pytorch_cos_sim(a, b):\n",
        "    \"\"\"\n",
        "    Computes the cosine similarity cos_sim(a[i], b[j]) for all i and j.\n",
        "    :return: Matrix with res[i][j] = cos_sim(a[i], b[j])\n",
        "    \"\"\"\n",
        "    return cos_sim(a, b)\n",
        "\n",
        "# === CELL: Utility functions for evaluation metrics ===\n",
        "def mean_recall_at_k(true_labels, predicted_labels, k=10):\n",
        "    \"\"\"\n",
        "    Calculate the mean Recall@k for a list of recommendations.\n",
        "    \"\"\"\n",
        "    recalls_at_k = []\n",
        "\n",
        "    for true, pred in zip(true_labels, predicted_labels):\n",
        "        # Calculate Recall@k for each recommendation list\n",
        "        true_set = set(true)\n",
        "        k = min(k, len(pred))\n",
        "        relevant_count = sum(1 for item in pred[:k] if item in true_set)\n",
        "        recalls_at_k.append(relevant_count / len(true_set) if len(true_set) > 0 else 0)\n",
        "\n",
        "    # Calculate the mean Recall@k\n",
        "    mean_recall = sum(recalls_at_k) / len(recalls_at_k) if recalls_at_k else 0\n",
        "\n",
        "    return mean_recall\n",
        "\n",
        "def mean_inv_ranking(true_labels, predicted_labels):\n",
        "    \"\"\"\n",
        "    Calculate the mean of lists of the mean inverse rank of true relevant items\n",
        "    in the lists of sorted recommended items.\n",
        "    \"\"\"\n",
        "    mean_ranks = []\n",
        "\n",
        "    for true, pred in zip(true_labels, predicted_labels):\n",
        "        # Calculate the inverse rank of true relevant items\n",
        "        # in the recommendation list\n",
        "        ranks = []\n",
        "        for item in true:\n",
        "            try:\n",
        "                rank = 1 / (pred.index(item) + 1)\n",
        "            except ValueError:\n",
        "                rank = 0  # If item not found, assign 0\n",
        "            ranks.append(rank)\n",
        "\n",
        "        # Calculate the mean inverse rank of true relevant items\n",
        "        # in the recommendation list\n",
        "        mean_rank = sum(ranks) / len(ranks) if ranks else 0\n",
        "        mean_ranks.append(mean_rank)\n",
        "\n",
        "    # Calculate the mean of the mean inverse ranks across all recommendation lists\n",
        "    mean_of_mean_ranks = sum(mean_ranks) / len(mean_ranks) if mean_ranks else 0\n",
        "\n",
        "    return mean_of_mean_ranks\n",
        "\n",
        "def mean_ranking(true_labels, predicted_labels):\n",
        "    \"\"\"\n",
        "    Calculate the mean of lists of the mean rank of true relevant items\n",
        "    in the lists of sorted recommended items.\n",
        "    \"\"\"\n",
        "    mean_ranks = []\n",
        "\n",
        "    for true, pred in zip(true_labels, predicted_labels):\n",
        "        # Calculate the rank of true relevant items\n",
        "        # in the recommendation list\n",
        "        ranks = []\n",
        "        for item in true:\n",
        "            try:\n",
        "                rank = pred.index(item) + 1\n",
        "            except ValueError:\n",
        "                rank = len(pred)  # If item not found, assign the length of the list\n",
        "            ranks.append(rank)\n",
        "\n",
        "        # Calculate the mean rank of true relevant items\n",
        "        # in the recommendation list\n",
        "        mean_rank = sum(ranks) / len(ranks) if ranks else 0\n",
        "        mean_ranks.append(mean_rank)\n",
        "\n",
        "    # Calculate the mean of the mean ranks across all recommendation lists\n",
        "    mean_of_mean_ranks = sum(mean_ranks) / len(mean_ranks) if mean_ranks else 0\n",
        "\n",
        "    return mean_of_mean_ranks\n",
        "\n",
        "# === CELL: Citation utility functions ===\n",
        "def citation_to_citing_to_cited_dict(citations):\n",
        "    \"\"\"\n",
        "    Put a citation mapping in a dict format\n",
        "    \"\"\"\n",
        "    # Initialize an empty dictionary to store the results\n",
        "    citing_to_cited_dict = {}\n",
        "\n",
        "    # Iterate over the items in the JSON list\n",
        "    for citation in citations:\n",
        "        # Check if the citing id already exists in the resulting dictionary\n",
        "        if citation[0] in citing_to_cited_dict:\n",
        "            # If the citing id exists, append the cited id to the existing list\n",
        "            citing_to_cited_dict[citation[0]].append(citation[2])\n",
        "        else:\n",
        "            # If the citing id doesn't exist, create a new list with the cited id for that citing id\n",
        "            citing_to_cited_dict[citation[0]] = [citation[2]]\n",
        "\n",
        "    return citing_to_cited_dict\n",
        "\n",
        "def get_true_and_predicted(citing_to_cited_dict, recommendations_dict):\n",
        "    \"\"\"\n",
        "    Get the true and predicted labels for the metrics calculation.\n",
        "    \"\"\"\n",
        "    # Initialize lists to store true labels and predicted labels\n",
        "    true_labels = []\n",
        "    predicted_labels = []\n",
        "    not_in_citation_mapping = 0\n",
        "\n",
        "    # Iterate over the items in both dictionaries\n",
        "    for citing_id in recommendations_dict.keys():\n",
        "        # Check if the citing_id is present in both dictionaries\n",
        "        if citing_id in citing_to_cited_dict:\n",
        "            # If yes, append the recommended items from both dictionaries to the respective lists\n",
        "            true_labels.append(citing_to_cited_dict[citing_id])\n",
        "            predicted_labels.append(recommendations_dict[citing_id])\n",
        "        else:\n",
        "            print(citing_id, \"not in citation mapping\")\n",
        "            not_in_citation_mapping += 1\n",
        "\n",
        "    return true_labels, predicted_labels, not_in_citation_mapping\n",
        "\n",
        "# === CELL: Function to load embeddings ===\n",
        "def load_embeddings_and_ids(embedding_file, app_ids_file):\n",
        "    \"\"\"\n",
        "    Load the embeddings and application IDs from saved files\n",
        "    \"\"\"\n",
        "    print(f\"Loading embeddings from {embedding_file}\")\n",
        "    embeddings = torch.from_numpy(np.load(embedding_file))\n",
        "\n",
        "    print(f\"Loading app_ids from {app_ids_file}\")\n",
        "    with open(app_ids_file, 'r') as f:\n",
        "        app_ids = json.load(f)\n",
        "\n",
        "    print(f\"Loaded {len(embeddings)} embeddings and {len(app_ids)} app_ids\")\n",
        "    return embeddings, app_ids"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_57rjN4LeqO_"
      },
      "source": [
        "## Load document and query embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4-G7iCVreqPA",
        "outputId": "1b54c66d-8f24-4525-af5c-48c168c3a949"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading embeddings from /content/embeddings_precalculated_docs/embeddings_all-MiniLM-L6-v2_mean_TA.npy\n",
            "Loading app_ids from /content/embeddings_precalculated_docs/app_ids_all-MiniLM-L6-v2_mean_TA.json\n",
            "Loaded 16837 embeddings and 16837 app_ids\n",
            "Loading embeddings from /content/embeddings_precalculated_train/embeddings_all-MiniLM-L6-v2_mean_TA.npy\n",
            "Loading app_ids from /content/embeddings_precalculated_train/app_ids_all-MiniLM-L6-v2_mean_TA.json\n",
            "Loaded 6831 embeddings and 6831 app_ids\n",
            "Running retrieval with 6831 queries against 16837 documents\n"
          ]
        }
      ],
      "source": [
        "# Create output directory if it doesn't exist\n",
        "if not os.path.exists(OUTPUT_DIR):\n",
        "    os.makedirs(OUTPUT_DIR)\n",
        "\n",
        "# Load document embeddings and app_ids\n",
        "doc_embeddings, doc_app_ids = load_embeddings_and_ids(DOC_EMBEDDING_FILE, DOC_APP_IDS_FILE)\n",
        "\n",
        "# Load query embeddings and app_ids\n",
        "query_embeddings, query_app_ids = load_embeddings_and_ids(QUERY_EMBEDDING_FILE, QUERY_APP_IDS_FILE)\n",
        "\n",
        "# Move tensors to GPU if available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "doc_embeddings = doc_embeddings.to(device)\n",
        "query_embeddings = query_embeddings.to(device)\n",
        "\n",
        "print(f\"Running retrieval with {len(query_embeddings)} queries against {len(doc_embeddings)} documents\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4_4KnUoieqPA",
        "outputId": "db1a748d-0196-47dc-c958-f394dd5ec525"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 100 results for query 3650293A1:\n",
            "App ID: 3225473A1, Similarity: 0.8639\n",
            "App ID: 3085590A1, Similarity: 0.7953\n",
            "App ID: 2974926A1, Similarity: 0.7210\n",
            "App ID: 3202629A1, Similarity: 0.6818\n",
            "App ID: 3075617A1, Similarity: 0.6802\n",
            "App ID: 1979185B1, Similarity: 0.6651\n",
            "App ID: 2072359B1, Similarity: 0.6554\n",
            "App ID: 2500221A1, Similarity: 0.6351\n",
            "App ID: 2871104A1, Similarity: 0.6329\n",
            "App ID: 2072363B1, Similarity: 0.6329\n",
            "App ID: 3000672A1, Similarity: 0.6318\n",
            "App ID: 1965363B1, Similarity: 0.6194\n",
            "App ID: 2824007A1, Similarity: 0.6174\n",
            "App ID: 2022690B1, Similarity: 0.6143\n",
            "App ID: 3000631A1, Similarity: 0.6134\n",
            "App ID: 3056400A1, Similarity: 0.6122\n",
            "App ID: 1731789B1, Similarity: 0.6047\n",
            "App ID: 2982556A1, Similarity: 0.6004\n",
            "App ID: 2024208B1, Similarity: 0.5889\n",
            "App ID: 2719605A1, Similarity: 0.5880\n",
            "App ID: 2794368B1, Similarity: 0.5849\n",
            "App ID: 2378154A1, Similarity: 0.5849\n",
            "App ID: 3225475A1, Similarity: 0.5836\n",
            "App ID: 3243715A1, Similarity: 0.5805\n",
            "App ID: 3175691A1, Similarity: 0.5755\n",
            "App ID: 2623386A1, Similarity: 0.5722\n",
            "App ID: 2665177A2, Similarity: 0.5698\n",
            "App ID: 3202630A1, Similarity: 0.5685\n",
            "App ID: 2591926A1, Similarity: 0.5671\n",
            "App ID: 2689957A1, Similarity: 0.5664\n",
            "App ID: 2692555A1, Similarity: 0.5640\n",
            "App ID: 3124370A2, Similarity: 0.5624\n",
            "App ID: 3292959A1, Similarity: 0.5572\n",
            "App ID: 3299660A1, Similarity: 0.5543\n",
            "App ID: 3225087A2, Similarity: 0.5516\n",
            "App ID: 2319291A1, Similarity: 0.5507\n",
            "App ID: 3301319A1, Similarity: 0.5500\n",
            "App ID: 2338821A1, Similarity: 0.5489\n",
            "App ID: 2070788B1, Similarity: 0.5486\n",
            "App ID: 3258277A1, Similarity: 0.5457\n",
            "App ID: 3179674A1, Similarity: 0.5446\n",
            "App ID: 3222129A1, Similarity: 0.5435\n",
            "App ID: 1938905B1, Similarity: 0.5415\n",
            "App ID: 2036745B1, Similarity: 0.5410\n",
            "App ID: 3020874A1, Similarity: 0.5401\n",
            "App ID: 2223836A1, Similarity: 0.5397\n",
            "App ID: 1781516B1, Similarity: 0.5390\n",
            "App ID: 1751446B1, Similarity: 0.5381\n",
            "App ID: 2565093A1, Similarity: 0.5377\n",
            "App ID: 2910401A1, Similarity: 0.5377\n",
            "App ID: 2835293A1, Similarity: 0.5366\n",
            "App ID: 3006283A1, Similarity: 0.5361\n",
            "App ID: 2425933A2, Similarity: 0.5350\n",
            "App ID: 2878501A1, Similarity: 0.5344\n",
            "App ID: 3388380A1, Similarity: 0.5294\n",
            "App ID: 2805868A2, Similarity: 0.5293\n",
            "App ID: 1795414B1, Similarity: 0.5273\n",
            "App ID: 2821302A1, Similarity: 0.5252\n",
            "App ID: 3239092A1, Similarity: 0.5232\n",
            "App ID: 3470707A1, Similarity: 0.5214\n",
            "App ID: 3330123A1, Similarity: 0.5210\n",
            "App ID: 1912313B1, Similarity: 0.5189\n",
            "App ID: 2537790A1, Similarity: 0.5188\n",
            "App ID: 3326955A1, Similarity: 0.5166\n",
            "App ID: 3184383A1, Similarity: 0.5149\n",
            "App ID: 3088641A1, Similarity: 0.5146\n",
            "App ID: 2910440A1, Similarity: 0.5124\n",
            "App ID: 2990239A1, Similarity: 0.5106\n",
            "App ID: 3093197A1, Similarity: 0.5078\n",
            "App ID: 2778113A1, Similarity: 0.5076\n",
            "App ID: 2980317A1, Similarity: 0.5063\n",
            "App ID: 3225578A1, Similarity: 0.5058\n",
            "App ID: 2910402A1, Similarity: 0.5056\n",
            "App ID: 3081829A1, Similarity: 0.5056\n",
            "App ID: 2422607A1, Similarity: 0.5054\n",
            "App ID: 3287657A1, Similarity: 0.5047\n",
            "App ID: 1979550B1, Similarity: 0.5032\n",
            "App ID: 2851267A1, Similarity: 0.5019\n",
            "App ID: 3078837A1, Similarity: 0.5016\n",
            "App ID: 2644015A2, Similarity: 0.5015\n",
            "App ID: 3420798A1, Similarity: 0.5014\n",
            "App ID: 3000456A1, Similarity: 0.4996\n",
            "App ID: 2878504A1, Similarity: 0.4994\n",
            "App ID: 1768891B1, Similarity: 0.4989\n",
            "App ID: 3150046A1, Similarity: 0.4980\n",
            "App ID: 2955104A1, Similarity: 0.4967\n",
            "App ID: 2632031A2, Similarity: 0.4958\n",
            "App ID: 3048035A1, Similarity: 0.4937\n",
            "App ID: 3056404A1, Similarity: 0.4935\n",
            "App ID: 3278842A2, Similarity: 0.4916\n",
            "App ID: 2415504A1, Similarity: 0.4910\n",
            "App ID: 3257712A1, Similarity: 0.4892\n",
            "App ID: 3115602A2, Similarity: 0.4877\n",
            "App ID: 2298689A2, Similarity: 0.4873\n",
            "App ID: 2949539A1, Similarity: 0.4865\n",
            "App ID: 1752739B1, Similarity: 0.4859\n",
            "App ID: 1975470B1, Similarity: 0.4855\n",
            "App ID: 2048046B1, Similarity: 0.4855\n",
            "App ID: 3239801A1, Similarity: 0.4852\n",
            "App ID: 3269604A1, Similarity: 0.4846\n"
          ]
        }
      ],
      "source": [
        "def retrieve_by_app_id(target_app_id, query_app_ids, query_embeddings, doc_embeddings, doc_app_ids, top_n=10):\n",
        "    \"\"\"\n",
        "    Retrieve top N documents for a single query by its app_id\n",
        "\n",
        "    Parameters:\n",
        "    - target_app_id: The application ID of the query to search for\n",
        "    - query_app_ids: List of all query application IDs\n",
        "    - query_embeddings: Embeddings for all queries\n",
        "    - doc_embeddings: Embeddings for all documents in the corpus\n",
        "    - doc_app_ids: Application IDs for all documents\n",
        "    - top_n: Number of results to return\n",
        "\n",
        "    Returns:\n",
        "    - List of top N document app_ids similar to the query\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Find the index of the target app_id in query_app_ids\n",
        "        query_index = query_app_ids.index(target_app_id)\n",
        "\n",
        "        # Get the corresponding embedding\n",
        "        query_embedding = query_embeddings[query_index].unsqueeze(0)\n",
        "\n",
        "        # Compute cosine similarity\n",
        "        cos_scores = pytorch_cos_sim(query_embedding, doc_embeddings)[0].cpu()\n",
        "\n",
        "        # Sort results and get top N\n",
        "        top_n_index = torch.argsort(cos_scores, descending=True)[:top_n].numpy()\n",
        "\n",
        "        # Get application IDs of top N documents\n",
        "        top_n_app_ids = [doc_app_ids[i] for i in top_n_index]\n",
        "        top_n_scores = [cos_scores[i].item() for i in top_n_index]\n",
        "\n",
        "        return top_n_app_ids, top_n_scores\n",
        "\n",
        "    except ValueError:\n",
        "        print(f\"Error: Application ID '{target_app_id}' not found in query_app_ids\")\n",
        "        return [], []\n",
        "\n",
        "# Example usage\n",
        "target_app_id = query_app_ids[0]  # Replace with the specific app_id you want to query\n",
        "retrieved_app_ids, similarity_scores = retrieve_by_app_id(\n",
        "    target_app_id,\n",
        "    query_app_ids,\n",
        "    query_embeddings,\n",
        "    doc_embeddings,\n",
        "    doc_app_ids,\n",
        "    top_n=TOP_N\n",
        ")\n",
        "\n",
        "# Display results\n",
        "print(f\"Top {TOP_N} results for query {target_app_id}:\")\n",
        "for app_id, score in zip(retrieved_app_ids, similarity_scores):\n",
        "    print(f\"App ID: {app_id}, Similarity: {score:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n6IoJ7FHeqPA"
      },
      "source": [
        "## Perform retrieval for each query"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "KIxU_ZLOeqPA",
        "outputId": "ff8aa1a0-b905-4245-ad0e-c62d23d89b02",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 6831/6831 [01:30<00:00, 75.07it/s]\n"
          ]
        }
      ],
      "source": [
        "results = {}\n",
        "for i, (query_embedding, query_id) in enumerate(tqdm(zip(query_embeddings, query_app_ids), total=len(query_embeddings))):\n",
        "    # Compute cosine similarity\n",
        "    query_embedding = query_embedding.unsqueeze(0)\n",
        "    cos_scores = pytorch_cos_sim(query_embedding, doc_embeddings)[0].cpu()\n",
        "\n",
        "    # Sort results and get top N\n",
        "    top_n_index = torch.argsort(cos_scores, descending=True)[:TOP_N].numpy()\n",
        "\n",
        "    # Get application IDs of top N documents\n",
        "    top_n_app_ids = [doc_app_ids[i] for i in top_n_index]\n",
        "    results[query_id] = top_n_app_ids"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UVtXsgC0eqPA"
      },
      "source": [
        "## Save results (if applicable)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "tzMZZgU7eqPA",
        "outputId": "656237b8-eb9c-4856-ff57-5a42bb34e4b3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results not saved (SAVE_RESULTS=False)\n"
          ]
        }
      ],
      "source": [
        "if QUERY_SET == \"train\":\n",
        "    output_file = f\"{OUTPUT_DIR}/{MODEL_NAME}_{CONTENT_TYPE}_{QUERY_SET}_retrieved.json\"\n",
        "else:\n",
        "    output_file = f\"{OUTPUT_DIR}/prediction1.json\"  # Standard filename for test predictions\n",
        "\n",
        "if SAVE_RESULTS:\n",
        "    with open(output_file, 'w') as f:\n",
        "        json.dump(results, f)\n",
        "    print(f\"Saved retrieval results to {output_file}\")\n",
        "else:\n",
        "    print(f\"Results not saved (SAVE_RESULTS={SAVE_RESULTS})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LVZ9FwoReqPA"
      },
      "source": [
        "## Evaluation (for training set only)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "4LZOd-6JeqPA",
        "outputId": "1ee0de90-98f6-4ab7-8012-032965734366",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating train set results...\n",
            "Loading citation data from /content/Citation_JSONs/Citation_Train.json\n",
            "\n",
            "Evaluation Results:\n",
            "-------------------\n",
            "Recall@10: 0.5459\n",
            "Mean Ranking: 29.5900\n",
            "Mean Inverse Ranking: 0.3453\n",
            "\n",
            "Number of patents measured: 6831\n",
            "Number of patents not in citation mapping: 0\n"
          ]
        }
      ],
      "source": [
        "if QUERY_SET == \"train\":\n",
        "    print(\"Evaluating train set results...\")\n",
        "\n",
        "    # Load citation mapping\n",
        "    print(f\"Loading citation data from {CITATION_FILE}\")\n",
        "    with open(CITATION_FILE, 'r') as f:\n",
        "        citations = json.load(f)\n",
        "\n",
        "    # Convert citations to citing-to-cited dictionary\n",
        "    citing_to_cited_dict = citation_to_citing_to_cited_dict(citations)\n",
        "\n",
        "    # Get true and predicted labels\n",
        "    true_labels, predicted_labels, not_in_citation_mapping = get_true_and_predicted(\n",
        "        citing_to_cited_dict, results\n",
        "    )\n",
        "\n",
        "    # Calculate metrics\n",
        "    print(\"\\nEvaluation Results:\")\n",
        "    print(\"-------------------\")\n",
        "\n",
        "    if METRICS_TYPE in ['recall_at_k', 'all']:\n",
        "        recall_at_k = mean_recall_at_k(true_labels, predicted_labels, k=K_VALUE)\n",
        "        print(f\"Recall@{K_VALUE}: {recall_at_k:.4f}\")\n",
        "\n",
        "    if METRICS_TYPE in ['mean_ranking', 'all']:\n",
        "        mean_rank = mean_ranking(true_labels, predicted_labels)\n",
        "        print(f\"Mean Ranking: {mean_rank:.4f}\")\n",
        "\n",
        "    if METRICS_TYPE in ['mean_inv_ranking', 'all']:\n",
        "        mean_inv_rank = mean_inv_ranking(true_labels, predicted_labels)\n",
        "        print(f\"Mean Inverse Ranking: {mean_inv_rank:.4f}\")\n",
        "\n",
        "    print(f\"\\nNumber of patents measured: {len(predicted_labels)}\")\n",
        "    print(f\"Number of patents not in citation mapping: {not_in_citation_mapping}\")\n",
        "else:\n",
        "    print(\"\\nTest set retrieval completed. No evaluation performed.\")\n",
        "    if SAVE_RESULTS:\n",
        "        print(f\"Predictions saved to {OUTPUT_DIR}/prediction1.json\")\n",
        "    print(\"Note: For the test set, citation data is not available for evaluation.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XcXigwGceqPA"
      },
      "source": [
        "# TODOs : Reciprocal Rank Fusion and Embedding Combinations\n",
        "\n",
        "## Reciprocal Rank Fusion (RRF)\n",
        "\n",
        "Reciprocal Rank Fusion is a simple yet effective method for combining multiple ranking lists. Instead of using raw scores from different retrieval systems, RRF uses the *positions* of documents in each ranking list:\n",
        "\n",
        "1. For each document in any of the ranking lists, calculate:\n",
        "   ```\n",
        "   RRF_score(d) = ∑ 1/(k + rank_i(d))\n",
        "   ```\n",
        "   Where `k` is a constant (typically 60) that mitigates the impact of high rankings, and `rank_i(d)` is the rank of document `d` in the i-th ranking list.\n",
        "\n",
        "2. If a document doesn't appear in a ranking list, its contribution to the sum is zero.\n",
        "\n",
        "3. Re-rank documents by their RRF scores in descending order.\n",
        "\n",
        "## Implementation Expectations\n",
        "\n",
        "Students are expected to:\n",
        "\n",
        "* Implement RRF to combine rankings from different retrieval methods (sparse, dense)\n",
        "* Experiment with embedding combinations using various aggregation strategies:\n",
        "  * **Sum**: Adding embedding vectors\n",
        "  * **Average**: Taking the mean of embedding vectors\n",
        "  * **Concatenation**: Joining vectors end-to-end\n",
        "  * **Weighted combinations**: Assigning different weights to different embeddings\n",
        "\n",
        "* Try combining embeddings of the same model but different content types:\n",
        "  * PatentSBERTa TA with PatentSBERTa claims\n",
        "  * PatentSBERTa TA with PatentSBERTa TAC\n",
        "  * And other combinations\n",
        "\n",
        "## Analysis and Evaluation\n",
        "\n",
        "Your submission will be evaluated on:\n",
        "\n",
        "* **Creativity**: Exploring novel combinations and beyond standard methods\n",
        "* **Research depth**: Investigating why certain combinations work better\n",
        "* **Analysis quality**: Providing insights about which strategies perform best and why\n",
        "* **Experimental rigor**: Systematically testing different approaches\n",
        "\n",
        "The information retrieval field is rapidly evolving - creative approaches that surpass traditional methods are strongly encouraged. The most innovative and effective solution will earn the challenge bonus and valuable CV credentials."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "QUERY_SET = \"test\"\n",
        "\n",
        "QUERY_EMBEDDING_DIR = TRAIN_EMBEDDING_DIR if QUERY_SET == \"train\" else TEST_EMBEDDING_DIR\n",
        "QUERY_EMBEDDING_FILE = os.path.join(QUERY_EMBEDDING_DIR, f\"embeddings_{MODEL_NAME}_{POOLING}_{CONTENT_TYPE}.npy\")\n",
        "QUERY_APP_IDS_FILE = os.path.join(QUERY_EMBEDDING_DIR, f\"app_ids_{MODEL_NAME}_{POOLING}_{CONTENT_TYPE}.json\")\n"
      ],
      "metadata": {
        "id": "Dnnc16-d9NmD"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "def reciprocal_rank_fusion(*ranked_results, k=60):\n",
        "    fused_results = {}\n",
        "\n",
        "    # Get all query IDs from all runs\n",
        "    all_query_ids = set()\n",
        "    for result in ranked_results:\n",
        "        all_query_ids.update(result.keys())\n",
        "\n",
        "    for qid in all_query_ids:\n",
        "        scores = {}\n",
        "\n",
        "        for run in ranked_results:\n",
        "            if qid not in run:\n",
        "                continue\n",
        "            for rank, doc_id in enumerate(run[qid]):\n",
        "                scores[doc_id] = scores.get(doc_id, 0) + 1 / (k + rank + 1)\n",
        "\n",
        "        # Sort by fused RRF score\n",
        "        ranked_docs = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
        "        fused_results[qid] = [doc_id for doc_id, _ in ranked_docs[:100]]\n",
        "\n",
        "    return fused_results\n",
        "\n",
        "final_results = reciprocal_rank_fusion(results)\n"
      ],
      "metadata": {
        "id": "08VakHwiDJFD"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def combine_embeddings(emb1, emb2, method='mean', weight1=0.5, weight2=0.5):\n",
        "    \"\"\"\n",
        "    Combine two embeddings with a specified strategy.\n",
        "\n",
        "    Methods:\n",
        "    - 'sum': Add vectors\n",
        "    - 'mean': Average of vectors\n",
        "    - 'concat': Concatenate vectors\n",
        "    - 'weighted': Weighted average of vectors\n",
        "    \"\"\"\n",
        "    if method == 'sum':\n",
        "        return emb1 + emb2\n",
        "    elif method == 'mean':\n",
        "        return (emb1 + emb2) / 2\n",
        "    elif method == 'concat':\n",
        "        return torch.cat((emb1, emb2), dim=1)\n",
        "    elif method == 'weighted':\n",
        "        return weight1 * emb1 + weight2 * emb2\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown combination method: {method}\")\n",
        "\n",
        "#combined_query_embeddings = combine_embeddings(query_embeddings_ta, query_embeddings_claims, method='weighted', weight1=0.7, weight2=0.3)\n",
        "#combined_doc_embeddings = combine_embeddings(doc_embeddings_ta, doc_embeddings_claims, method='weighted', weight1=0.7, weight2=0.3)\n",
        ""
      ],
      "metadata": {
        "id": "CHL1v8etDT6w"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1yBdUcZAMPdW"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "sentence_transformer",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}