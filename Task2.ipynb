{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers torch\n",
        "\n",
        "import os\n",
        "import json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Dq4ZzzFroSe",
        "outputId": "ad316475-0780-4443-cebb-57b6817be7d9"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.50.3)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m78.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# PATHS\n",
        "train_queries_file = \"train_queries.json\"\n",
        "test_queries_file = \"test_queries.json\"\n",
        "train_gold_mapping_file = \"train_gold_mapping.json\"\n",
        "shuffled_pre_ranking_file = \"shuffled_pre_ranking.json\"\n",
        "queries_content_file = \"queries_content_with_features.json\"\n",
        "documents_content_file = \"documents_content_with_features.json\"\n",
        "\n",
        "test_predictions_file = \"prediction2.json\"\n",
        "\n",
        "if not all(os.path.exists(f) for f in [test_queries_file, shuffled_pre_ranking_file, queries_content_file, documents_content_file]):\n",
        "    print(\"Error: One or more necessary data files for the test set are missing.\")\n",
        "    exit()\n",
        "else:\n",
        "    print(\"Necessary data files for the test set found.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zx5y8Z_-rrbn",
        "outputId": "073fc5ef-a395-4513-e81f-f22cd08b12a0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Necessary data files for the test set found.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# LOAD\n",
        "\n",
        "def load_json_file(file_path):\n",
        "    with open(file_path, 'r') as f:\n",
        "        return json.load(f)\n",
        "\n",
        "test_queries = load_json_file(test_queries_file)\n",
        "print(f\"Loaded {len(test_queries)} test queries.\")\n",
        "\n",
        "pre_ranking_test = load_json_file(shuffled_pre_ranking_file)\n",
        "# filter pre-ranking to include only test queries (important!)\n",
        "pre_ranking_test_filtered = {fan: docs for fan, docs in pre_ranking_test.items() if fan in test_queries}\n",
        "print(f\"Filtered pre-ranking to {len(pre_ranking_test_filtered)} test queries.\")\n",
        "\n",
        "\n",
        "queries_content = load_json_file(queries_content_file)\n",
        "documents_content = load_json_file(documents_content_file)\n",
        "\n",
        "\n",
        "if not os.path.exists(\"cross_encoder_reranking_train.py\"):\n",
        "    print(\"Error: The 'cross_encoder_reranking_train.py' script is missing. Please upload it.\")\n",
        "    exit()\n",
        "else:\n",
        "    print(\"Reranking script found.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X5F18z5rsAn_",
        "outputId": "020522b1-72e5-488b-d6e4-ec9aadae603d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 10 test queries.\n",
            "Filtered pre-ranking to 10 test queries.\n",
            "Reranking script found.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# E5 LARGE ta\n",
        "\n",
        "best_model_name = \"intfloat/e5-large-v2\"\n",
        "best_text_type = \"ta\"\n",
        "max_length = 512\n",
        "\n",
        "print(\"\\nRunning reranking on the test set...\")\n",
        "!python cross_encoder_reranking_train.py \\\n",
        "    --model_name \"{best_model_name}\" \\\n",
        "    --text_type \"{best_text_type}\" \\\n",
        "    --pre_ranking \"{shuffled_pre_ranking_file}\" \\\n",
        "    --queries_list \"{test_queries_file}\" \\\n",
        "    --queries_content \"{queries_content_file}\" \\\n",
        "    --documents_content \"{documents_content_file}\" \\\n",
        "    --output \"{test_predictions_file}\" \\\n",
        "    --max_length {max_length}\n",
        "\n",
        "print(f\"\\nTest set predictions saved to: {test_predictions_file}\")"
      ],
      "metadata": {
        "id": "SciWrL5J6-8Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# E5 LARGE tac1\n",
        "\n",
        "best_model_name = \"intfloat/e5-large-v2\"\n",
        "best_text_type = \"tac1\"\n",
        "max_length = 512\n",
        "\n",
        "print(\"\\nRunning reranking on the test set...\")\n",
        "!python cross_encoder_reranking_train.py \\\n",
        "    --model_name \"{best_model_name}\" \\\n",
        "    --text_type \"{best_text_type}\" \\\n",
        "    --pre_ranking \"{shuffled_pre_ranking_file}\" \\\n",
        "    --queries_list \"{test_queries_file}\" \\\n",
        "    --queries_content \"{queries_content_file}\" \\\n",
        "    --documents_content \"{documents_content_file}\" \\\n",
        "    --output \"{test_predictions_file}\" \\\n",
        "    --max_length {max_length}\n",
        "\n",
        "print(f\"\\nTest set predictions saved to: {test_predictions_file}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nHgGozrztq7I",
        "outputId": "cb6a12b8-0e8e-42f2-e461-1ee3a0291a94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Running reranking on the test set...\n",
            "Loading training queries from test_queries.json...\n",
            "Loaded 10 training queries\n",
            "Loading pre-ranking data from shuffled_pre_ranking.json...\n",
            "Filtered pre-ranking to 10 training queries\n",
            "Loading query content from queries_content_with_features.json...\n",
            "Loading document content from documents_content_with_features.json...\n",
            "Loading model intfloat/e5-large-v2...\n",
            "2025-04-07 10:15:39.328529: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1744020939.720766   39376 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1744020939.816623   39376 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-04-07 10:15:40.624398: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Starting re-ranking process for training queries...\n",
            "Processing queries:   0% 0/10 [00:00<?, ?it/s]\n",
            "Re-ranking 30 documents for training query 103964109\n",
            "Original pre-ranking (first 3): ['94596291', '65451984', '81098918']\n",
            "\n",
            "Scoring documents:   0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "Scoring documents:  12% 1/8 [00:35<04:09, 35.66s/it]\u001b[A\n",
            "Scoring documents:  25% 2/8 [01:14<03:46, 37.68s/it]\u001b[A\n",
            "Scoring documents:  38% 3/8 [01:48<02:59, 35.85s/it]\u001b[A\n",
            "Scoring documents:  50% 4/8 [02:15<02:08, 32.23s/it]\u001b[A\n",
            "Scoring documents:  62% 5/8 [02:44<01:33, 31.33s/it]\u001b[A\n",
            "Scoring documents:  75% 6/8 [03:11<00:59, 29.72s/it]\u001b[A\n",
            "Scoring documents:  88% 7/8 [03:44<00:30, 30.82s/it]\u001b[A\n",
            "Scoring documents: 100% 8/8 [04:00<00:00, 26.22s/it]\u001b[A\n",
            "Processing queries:  10% 1/10 [04:00<36:08, 240.90s/it]\n",
            "Re-ranking 30 documents for training query 72214279\n",
            "Original pre-ranking (first 3): ['4112132', '7126783', '66704221']\n",
            "\n",
            "Scoring documents:   0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "Scoring documents:  12% 1/8 [00:32<03:46, 32.42s/it]\u001b[A\n",
            "Scoring documents:  25% 2/8 [01:05<03:18, 33.05s/it]\u001b[A\n",
            "Scoring documents:  38% 3/8 [01:38<02:43, 32.74s/it]\u001b[A\n",
            "Scoring documents:  50% 4/8 [02:04<02:00, 30.10s/it]\u001b[A\n",
            "Scoring documents:  62% 5/8 [02:37<01:34, 31.34s/it]\u001b[A\n",
            "Scoring documents:  75% 6/8 [03:10<01:03, 31.74s/it]\u001b[A\n",
            "Scoring documents:  88% 7/8 [03:41<00:31, 31.66s/it]\u001b[A\n",
            "Scoring documents: 100% 8/8 [03:55<00:00, 25.86s/it]\u001b[A\n",
            "Processing queries:  20% 2/10 [07:56<31:40, 237.62s/it]\n",
            "Re-ranking 30 documents for training query 68249923\n",
            "Original pre-ranking (first 3): ['35246469', '80619246', '7191590']\n",
            "\n",
            "Scoring documents:   0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "Scoring documents:  12% 1/8 [00:32<03:45, 32.25s/it]\u001b[A\n",
            "Scoring documents:  25% 2/8 [01:05<03:17, 32.88s/it]\u001b[A\n",
            "Scoring documents:  38% 3/8 [01:37<02:42, 32.59s/it]\u001b[A\n",
            "Scoring documents:  50% 4/8 [02:11<02:11, 32.96s/it]\u001b[A\n",
            "Scoring documents:  62% 5/8 [02:43<01:38, 32.71s/it]\u001b[A\n",
            "Scoring documents:  75% 6/8 [03:23<01:10, 35.19s/it]\u001b[A\n",
            "Scoring documents:  88% 7/8 [03:57<00:34, 34.86s/it]\u001b[A\n",
            "Scoring documents: 100% 8/8 [04:16<00:00, 29.87s/it]\u001b[A\n",
            "Processing queries:  30% 3/10 [12:13<28:45, 246.46s/it]\n",
            "Re-ranking 30 documents for training query 79740635\n",
            "Original pre-ranking (first 3): ['72282319', '80126462', '6507475']\n",
            "\n",
            "Scoring documents:   0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "Scoring documents:  12% 1/8 [00:33<03:55, 33.67s/it]\u001b[A\n",
            "Scoring documents:  25% 2/8 [01:05<03:16, 32.80s/it]\u001b[A\n",
            "Scoring documents:  38% 3/8 [01:39<02:45, 33.17s/it]\u001b[A\n",
            "Scoring documents:  50% 4/8 [02:11<02:11, 32.82s/it]\u001b[A\n",
            "Scoring documents:  62% 5/8 [02:45<01:39, 33.02s/it]\u001b[A\n",
            "Scoring documents:  75% 6/8 [03:17<01:05, 32.73s/it]\u001b[A\n",
            "Scoring documents:  88% 7/8 [03:50<00:32, 32.90s/it]\u001b[A\n",
            "Scoring documents: 100% 8/8 [04:09<00:00, 28.41s/it]\u001b[A\n",
            "Processing queries:  40% 4/10 [16:22<24:45, 247.60s/it]\n",
            "Re-ranking 30 documents for training query 100251983\n",
            "Original pre-ranking (first 3): ['82028496', '89295990', '78872192']\n",
            "\n",
            "Scoring documents:   0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "Scoring documents:  12% 1/8 [00:32<03:48, 32.64s/it]\u001b[A\n",
            "Scoring documents:  25% 2/8 [01:05<03:16, 32.67s/it]\u001b[A\n",
            "Scoring documents:  38% 3/8 [01:37<02:43, 32.66s/it]\u001b[A\n",
            "Scoring documents:  50% 4/8 [02:08<02:07, 31.99s/it]\u001b[A\n",
            "Scoring documents:  62% 5/8 [02:41<01:36, 32.04s/it]\u001b[A\n",
            "Scoring documents:  75% 6/8 [03:14<01:05, 32.51s/it]\u001b[A\n",
            "Scoring documents:  88% 7/8 [03:43<00:31, 31.26s/it]\u001b[A\n",
            "Scoring documents: 100% 8/8 [04:00<00:00, 26.97s/it]\u001b[A\n",
            "Processing queries:  50% 5/10 [20:23<20:26, 245.21s/it]\n",
            "Re-ranking 30 documents for training query 76109416\n",
            "Original pre-ranking (first 3): ['67929958', '1212129', '74871145']\n",
            "\n",
            "Scoring documents:   0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "Scoring documents:  12% 1/8 [00:33<03:53, 33.41s/it]\u001b[A\n",
            "Scoring documents:  25% 2/8 [01:05<03:16, 32.75s/it]\u001b[A\n",
            "Scoring documents:  38% 3/8 [01:39<02:45, 33.07s/it]\u001b[A\n",
            "Scoring documents:  50% 4/8 [02:11<02:11, 32.75s/it]\u001b[A\n",
            "Scoring documents:  62% 5/8 [02:44<01:38, 32.97s/it]\u001b[A\n",
            "Scoring documents:  75% 6/8 [03:17<01:05, 32.73s/it]\u001b[A\n",
            "Scoring documents:  88% 7/8 [03:50<00:33, 33.00s/it]\u001b[A\n",
            "Scoring documents: 100% 8/8 [04:09<00:00, 28.41s/it]\u001b[A\n",
            "Processing queries:  60% 6/10 [24:32<16:26, 246.56s/it]\n",
            "Re-ranking 30 documents for training query 85685768\n",
            "Original pre-ranking (first 3): ['5399125', '103242931', '96992323']\n",
            "\n",
            "Scoring documents:   0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "Scoring documents:  12% 1/8 [00:32<03:50, 32.94s/it]\u001b[A\n",
            "Scoring documents:  25% 2/8 [01:04<03:13, 32.27s/it]\u001b[A\n",
            "Scoring documents:  38% 3/8 [01:38<02:43, 32.73s/it]\u001b[A\n",
            "Scoring documents:  50% 4/8 [02:09<02:09, 32.31s/it]\u001b[A\n",
            "Scoring documents:  62% 5/8 [02:42<01:37, 32.53s/it]\u001b[A\n",
            "Scoring documents:  75% 6/8 [03:14<01:04, 32.26s/it]\u001b[A\n",
            "Scoring documents:  88% 7/8 [03:46<00:32, 32.39s/it]\u001b[A\n",
            "Scoring documents: 100% 8/8 [04:05<00:00, 28.07s/it]\u001b[A\n",
            "Processing queries:  70% 7/10 [28:38<12:18, 246.31s/it]\n",
            "Re-ranking 30 documents for training query 70563808\n",
            "Original pre-ranking (first 3): ['76578541', '66791385', '44519425']\n",
            "\n",
            "Scoring documents:   0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "Scoring documents:  12% 1/8 [00:25<02:59, 25.60s/it]\u001b[A\n",
            "Scoring documents:  25% 2/8 [00:49<02:27, 24.60s/it]\u001b[A\n",
            "Scoring documents:  38% 3/8 [01:21<02:19, 27.86s/it]\u001b[A\n",
            "Scoring documents:  50% 4/8 [01:54<01:59, 29.88s/it]\u001b[A\n",
            "Scoring documents:  62% 5/8 [02:25<01:31, 30.53s/it]\u001b[A\n",
            "Scoring documents:  75% 6/8 [02:49<00:56, 28.25s/it]\u001b[A\n",
            "Scoring documents:  88% 7/8 [03:21<00:29, 29.36s/it]\u001b[A\n",
            "Scoring documents: 100% 8/8 [03:41<00:00, 26.32s/it]\u001b[A\n",
            "Processing queries:  80% 8/10 [32:19<07:56, 238.31s/it]\n",
            "Re-ranking 30 documents for training query 79482665\n",
            "Original pre-ranking (first 3): ['87571623', '92503480', '68914227']\n",
            "\n",
            "Scoring documents:   0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "Scoring documents:  12% 1/8 [00:31<03:41, 31.61s/it]\u001b[A\n",
            "Scoring documents:  25% 2/8 [01:04<03:14, 32.37s/it]\u001b[A\n",
            "Scoring documents:  38% 3/8 [01:36<02:40, 32.07s/it]\u001b[A\n",
            "Scoring documents:  50% 4/8 [02:09<02:10, 32.51s/it]\u001b[A\n",
            "Scoring documents:  62% 5/8 [02:41<01:36, 32.18s/it]\u001b[A\n",
            "Scoring documents:  75% 6/8 [03:13<01:04, 32.34s/it]\u001b[A\n",
            "Scoring documents:  88% 7/8 [03:45<00:32, 32.24s/it]\u001b[A\n",
            "Scoring documents: 100% 8/8 [04:04<00:00, 27.97s/it]\u001b[A\n",
            "Processing queries:  90% 9/10 [36:24<04:00, 240.26s/it]\n",
            "Re-ranking 30 documents for training query 75800075\n",
            "Original pre-ranking (first 3): ['33464411', '34284570', '74966633']\n",
            "\n",
            "Scoring documents:   0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "Scoring documents:  12% 1/8 [00:32<03:47, 32.55s/it]\u001b[A\n",
            "Scoring documents:  25% 2/8 [01:02<03:07, 31.31s/it]\u001b[A\n",
            "Scoring documents:  38% 3/8 [01:34<02:37, 31.50s/it]\u001b[A\n",
            "Scoring documents:  50% 4/8 [02:06<02:06, 31.55s/it]\u001b[A\n",
            "Scoring documents:  62% 5/8 [02:38<01:35, 31.67s/it]\u001b[A\n",
            "Scoring documents:  75% 6/8 [03:09<01:02, 31.48s/it]\u001b[A\n",
            "Scoring documents:  88% 7/8 [03:41<00:31, 31.56s/it]\u001b[A\n",
            "Scoring documents: 100% 8/8 [04:00<00:00, 27.63s/it]\u001b[A\n",
            "Processing queries: 100% 10/10 [40:24<00:00, 242.45s/it]\n",
            "Saving re-ranked results to prediction2.json...\n",
            "Re-ranking complete!\n",
            "Number of training queries processed: 10\n",
            "\n",
            "Test set predictions saved to: prediction2.json\n",
            "python3: can't open file '/content/evaluate_train_ranking.py': [Errno 2] No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# E5 LARGE CLAIMS\n",
        "\n",
        "best_model_name = \"intfloat/e5-large-v2\"\n",
        "best_text_type = \"claims\"\n",
        "max_length = 512\n",
        "\n",
        "print(\"\\nRunning reranking on the test set...\")\n",
        "!python cross_encoder_reranking_train.py \\\n",
        "    --model_name \"{best_model_name}\" \\\n",
        "    --text_type \"{best_text_type}\" \\\n",
        "    --pre_ranking \"{shuffled_pre_ranking_file}\" \\\n",
        "    --queries_list \"{test_queries_file}\" \\\n",
        "    --queries_content \"{queries_content_file}\" \\\n",
        "    --documents_content \"{documents_content_file}\" \\\n",
        "    --output \"{test_predictions_file}\" \\\n",
        "    --max_length {max_length}\n",
        "\n",
        "print(f\"\\nTest set predictions saved to: {test_predictions_file}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ksH7DVKFsSKm",
        "outputId": "09081d6a-37c3-4f67-c14b-7df5258ef4aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Running reranking on the test set...\n",
            "Loading training queries from test_queries.json...\n",
            "Loaded 10 training queries\n",
            "Loading pre-ranking data from shuffled_pre_ranking.json...\n",
            "Filtered pre-ranking to 10 training queries\n",
            "Loading query content from queries_content_with_features.json...\n",
            "Loading document content from documents_content_with_features.json...\n",
            "Loading model intfloat/e5-large-v2...\n",
            "tokenizer_config.json: 100% 314/314 [00:00<00:00, 1.67MB/s]\n",
            "vocab.txt: 100% 232k/232k [00:00<00:00, 651kB/s]\n",
            "tokenizer.json: 100% 711k/711k [00:00<00:00, 3.87MB/s]\n",
            "special_tokens_map.json: 100% 125/125 [00:00<00:00, 700kB/s]\n",
            "config.json: 100% 616/616 [00:00<00:00, 3.94MB/s]\n",
            "2025-04-08 12:11:09.842809: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1744114270.115109    3762 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1744114270.190474    3762 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-04-08 12:11:10.769273: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "model.safetensors: 100% 1.34G/1.34G [00:05<00:00, 250MB/s]\n",
            "Starting re-ranking process for training queries...\n",
            "Processing queries:   0% 0/10 [00:00<?, ?it/s]\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/cross_encoder_reranking_train.py\", line 298, in <module>\n",
            "    main()\n",
            "  File \"/content/cross_encoder_reranking_train.py\", line 230, in main\n",
            "    query_text = extract_text(queries_content[query_fan], args.text_type)\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/cross_encoder_reranking_train.py\", line 80, in extract_text\n",
            "    return \" \".join(all_text)\n",
            "           ^^^^^^^^^^^^^^^^^^\n",
            "TypeError: sequence item 254: expected str instance, dict found\n",
            "\n",
            "Test set predictions saved to: prediction2.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# E5 LARGE DESCRIPTION\n",
        "\n",
        "best_model_name = \"intfloat/e5-large-v2\"\n",
        "best_text_type = \"description\"\n",
        "max_length = 512\n",
        "\n",
        "print(\"\\nRunning reranking on the test set...\")\n",
        "!python cross_encoder_reranking_train.py \\\n",
        "    --model_name \"{best_model_name}\" \\\n",
        "    --text_type \"{best_text_type}\" \\\n",
        "    --pre_ranking \"{shuffled_pre_ranking_file}\" \\\n",
        "    --queries_list \"{test_queries_file}\" \\\n",
        "    --queries_content \"{queries_content_file}\" \\\n",
        "    --documents_content \"{documents_content_file}\" \\\n",
        "    --output \"{test_predictions_file}\" \\\n",
        "    --max_length {max_length}\n",
        "\n",
        "print(f\"\\nTest set predictions saved to: {test_predictions_file}\")"
      ],
      "metadata": {
        "id": "HXegk4HZW0wz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# MPNET TA\n",
        "\n",
        "best_model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
        "\n",
        "best_text_type = \"TA\"\n",
        "max_length = 512\n",
        "\n",
        "print(\"\\nRunning reranking on the test set...\")\n",
        "!python cross_encoder_reranking_train.py \\\n",
        "    --model_name \"{best_model_name}\" \\\n",
        "    --text_type \"{best_text_type}\" \\\n",
        "    --pre_ranking \"{shuffled_pre_ranking_file}\" \\\n",
        "    --queries_list \"{test_queries_file}\" \\\n",
        "    --queries_content \"{queries_content_file}\" \\\n",
        "    --documents_content \"{documents_content_file}\" \\\n",
        "    --output \"{test_predictions_file}\" \\\n",
        "    --max_length {max_length}\n",
        "\n",
        "print(f\"\\nTest set predictions saved to: {test_predictions_file}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NjvHOL-ju0xA",
        "outputId": "954cd2bc-0a06-461b-efbc-a1d90467ef4d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Running reranking on the test set...\n",
            "Loading training queries from test_queries.json...\n",
            "Loaded 10 training queries\n",
            "Loading pre-ranking data from shuffled_pre_ranking.json...\n",
            "Filtered pre-ranking to 10 training queries\n",
            "Loading query content from queries_content_with_features.json...\n",
            "Loading document content from documents_content_with_features.json...\n",
            "Loading model sentence-transformers/all-mpnet-base-v2...\n",
            "tokenizer_config.json: 100% 363/363 [00:00<00:00, 1.97MB/s]\n",
            "vocab.txt: 100% 232k/232k [00:00<00:00, 676kB/s]\n",
            "tokenizer.json: 100% 466k/466k [00:00<00:00, 1.35MB/s]\n",
            "special_tokens_map.json: 100% 239/239 [00:00<00:00, 1.38MB/s]\n",
            "config.json: 100% 571/571 [00:00<00:00, 3.50MB/s]\n",
            "2025-04-08 19:23:34.115914: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1744140214.371511    1258 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1744140214.445504    1258 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-04-08 19:23:35.009585: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
            "model.safetensors: 100% 438M/438M [00:01<00:00, 267MB/s]\n",
            "Starting re-ranking process for training queries...\n",
            "Processing queries:   0% 0/10 [00:00<?, ?it/s]\n",
            "Re-ranking 30 documents for training query 103964109\n",
            "Original pre-ranking (first 3): ['94596291', '65451984', '81098918']\n",
            "\n",
            "Scoring documents:   0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "Scoring documents:  12% 1/8 [00:11<01:17, 11.05s/it]\u001b[A\n",
            "Scoring documents:  25% 2/8 [00:21<01:04, 10.83s/it]\u001b[A\n",
            "Scoring documents:  38% 3/8 [00:33<00:55, 11.14s/it]\u001b[A\n",
            "Scoring documents:  50% 4/8 [00:44<00:44, 11.17s/it]\u001b[A\n",
            "Scoring documents:  62% 5/8 [00:54<00:32, 10.67s/it]\u001b[A\n",
            "Scoring documents:  75% 6/8 [01:04<00:21, 10.69s/it]\u001b[A\n",
            "Scoring documents:  88% 7/8 [01:15<00:10, 10.75s/it]\u001b[A\n",
            "Scoring documents: 100% 8/8 [01:22<00:00,  9.54s/it]\u001b[A\n",
            "Processing queries:  10% 1/10 [01:22<12:25, 82.80s/it]\n",
            "Re-ranking 30 documents for training query 72214279\n",
            "Original pre-ranking (first 3): ['4112132', '7126783', '66704221']\n",
            "\n",
            "Scoring documents:   0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "Scoring documents:  12% 1/8 [00:09<01:09,  9.98s/it]\u001b[A\n",
            "Scoring documents:  25% 2/8 [00:20<01:01, 10.26s/it]\u001b[A\n",
            "Scoring documents:  38% 3/8 [00:31<00:52, 10.44s/it]\u001b[A\n",
            "Scoring documents:  50% 4/8 [00:41<00:42, 10.56s/it]\u001b[A\n",
            "Scoring documents:  62% 5/8 [00:52<00:31, 10.60s/it]\u001b[A\n",
            "Scoring documents:  75% 6/8 [01:03<00:21, 10.63s/it]\u001b[A\n",
            "Scoring documents:  88% 7/8 [01:13<00:10, 10.50s/it]\u001b[A\n",
            "Scoring documents: 100% 8/8 [01:19<00:00,  9.19s/it]\u001b[A\n",
            "Processing queries:  20% 2/10 [02:42<10:48, 81.03s/it]\n",
            "Re-ranking 30 documents for training query 68249923\n",
            "Original pre-ranking (first 3): ['35246469', '80619246', '7191590']\n",
            "\n",
            "Scoring documents:   0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "Scoring documents:  12% 1/8 [00:10<01:14, 10.70s/it]\u001b[A\n",
            "Scoring documents:  25% 2/8 [00:21<01:03, 10.67s/it]\u001b[A\n",
            "Scoring documents:  38% 3/8 [00:31<00:52, 10.47s/it]\u001b[A\n",
            "Scoring documents:  50% 4/8 [00:41<00:41, 10.35s/it]\u001b[A\n",
            "Scoring documents:  62% 5/8 [00:53<00:32, 10.85s/it]\u001b[A\n",
            "Scoring documents:  75% 6/8 [01:04<00:21, 10.80s/it]\u001b[A\n",
            "Scoring documents:  88% 7/8 [01:14<00:10, 10.74s/it]\u001b[A\n",
            "Scoring documents: 100% 8/8 [01:20<00:00,  9.22s/it]\u001b[A\n",
            "Processing queries:  30% 3/10 [04:03<09:26, 80.92s/it]\n",
            "Re-ranking 30 documents for training query 79740635\n",
            "Original pre-ranking (first 3): ['72282319', '80126462', '6507475']\n",
            "\n",
            "Scoring documents:   0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "Scoring documents:  12% 1/8 [00:13<01:33, 13.35s/it]\u001b[A\n",
            "Scoring documents:  25% 2/8 [00:25<01:14, 12.46s/it]\u001b[A\n",
            "Scoring documents:  38% 3/8 [00:36<00:58, 11.77s/it]\u001b[A\n",
            "Scoring documents:  50% 4/8 [00:46<00:45, 11.41s/it]\u001b[A\n",
            "Scoring documents:  62% 5/8 [00:57<00:33, 11.26s/it]\u001b[A\n",
            "Scoring documents:  75% 6/8 [01:08<00:22, 11.15s/it]\u001b[A\n",
            "Scoring documents:  88% 7/8 [01:20<00:11, 11.26s/it]\u001b[A\n",
            "Scoring documents: 100% 8/8 [01:26<00:00,  9.76s/it]\u001b[A\n",
            "Processing queries:  40% 4/10 [05:30<08:19, 83.31s/it]\n",
            "Re-ranking 30 documents for training query 100251983\n",
            "Original pre-ranking (first 3): ['82028496', '89295990', '78872192']\n",
            "\n",
            "Scoring documents:   0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "Scoring documents:  12% 1/8 [00:11<01:17, 11.05s/it]\u001b[A\n",
            "Scoring documents:  25% 2/8 [00:21<01:05, 10.95s/it]\u001b[A\n",
            "Scoring documents:  38% 3/8 [00:32<00:54, 10.96s/it]\u001b[A\n",
            "Scoring documents:  50% 4/8 [00:43<00:44, 11.01s/it]\u001b[A\n",
            "Scoring documents:  62% 5/8 [00:54<00:32, 10.70s/it]\u001b[A\n",
            "Scoring documents:  75% 6/8 [01:04<00:21, 10.74s/it]\u001b[A\n",
            "Scoring documents:  88% 7/8 [01:15<00:10, 10.79s/it]\u001b[A\n",
            "Scoring documents: 100% 8/8 [01:22<00:00,  9.56s/it]\u001b[A\n",
            "Processing queries:  50% 5/10 [06:53<06:55, 83.13s/it]\n",
            "Re-ranking 30 documents for training query 76109416\n",
            "Original pre-ranking (first 3): ['67929958', '1212129', '74871145']\n",
            "\n",
            "Scoring documents:   0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "Scoring documents:  12% 1/8 [00:10<01:16, 11.00s/it]\u001b[A\n",
            "Scoring documents:  25% 2/8 [00:21<01:03, 10.50s/it]\u001b[A\n",
            "Scoring documents:  38% 3/8 [00:32<00:53, 10.72s/it]\u001b[A\n",
            "Scoring documents:  50% 4/8 [00:43<00:43, 10.94s/it]\u001b[A\n",
            "Scoring documents:  62% 5/8 [00:54<00:32, 10.93s/it]\u001b[A\n",
            "Scoring documents:  75% 6/8 [01:05<00:22, 11.10s/it]\u001b[A\n",
            "Scoring documents:  88% 7/8 [01:17<00:11, 11.46s/it]\u001b[A\n",
            "Scoring documents: 100% 8/8 [01:23<00:00,  9.73s/it]\u001b[A\n",
            "Processing queries:  60% 6/10 [08:17<05:33, 83.41s/it]\n",
            "Re-ranking 30 documents for training query 85685768\n",
            "Original pre-ranking (first 3): ['5399125', '103242931', '96992323']\n",
            "\n",
            "Scoring documents:   0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "Scoring documents:  12% 1/8 [00:10<01:15, 10.80s/it]\u001b[A\n",
            "Scoring documents:  25% 2/8 [00:21<01:05, 10.84s/it]\u001b[A\n",
            "Scoring documents:  38% 3/8 [00:32<00:54, 10.89s/it]\u001b[A\n",
            "Scoring documents:  50% 4/8 [00:43<00:43, 10.87s/it]\u001b[A\n",
            "Scoring documents:  62% 5/8 [00:54<00:32, 10.89s/it]\u001b[A\n",
            "Scoring documents:  75% 6/8 [01:04<00:21, 10.73s/it]\u001b[A\n",
            "Scoring documents:  88% 7/8 [01:15<00:10, 10.62s/it]\u001b[A\n",
            "Scoring documents: 100% 8/8 [01:22<00:00,  9.45s/it]\u001b[A\n",
            "Processing queries:  70% 7/10 [09:39<04:09, 83.00s/it]\n",
            "Re-ranking 30 documents for training query 70563808\n",
            "Original pre-ranking (first 3): ['76578541', '66791385', '44519425']\n",
            "\n",
            "Scoring documents:   0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "Scoring documents:  12% 1/8 [00:10<01:16, 10.89s/it]\u001b[A\n",
            "Scoring documents:  25% 2/8 [00:21<01:04, 10.78s/it]\u001b[A\n",
            "Scoring documents:  38% 3/8 [00:31<00:52, 10.44s/it]\u001b[A\n",
            "Scoring documents:  50% 4/8 [00:42<00:42, 10.65s/it]\u001b[A\n",
            "Scoring documents:  62% 5/8 [00:53<00:32, 10.73s/it]\u001b[A\n",
            "Scoring documents:  75% 6/8 [01:04<00:21, 10.78s/it]\u001b[A\n",
            "Scoring documents:  88% 7/8 [01:15<00:10, 10.81s/it]\u001b[A\n",
            "Scoring documents: 100% 8/8 [01:21<00:00,  9.25s/it]\u001b[A\n",
            "Processing queries:  80% 8/10 [11:00<02:44, 82.42s/it]\n",
            "Re-ranking 30 documents for training query 79482665\n",
            "Original pre-ranking (first 3): ['87571623', '92503480', '68914227']\n",
            "\n",
            "Scoring documents:   0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "Scoring documents:  12% 1/8 [00:10<01:15, 10.79s/it]\u001b[A\n",
            "Scoring documents:  25% 2/8 [00:21<01:05, 10.84s/it]\u001b[A\n",
            "Scoring documents:  38% 3/8 [00:32<00:54, 10.89s/it]\u001b[A\n",
            "Scoring documents:  50% 4/8 [00:43<00:43, 10.87s/it]\u001b[A\n",
            "Scoring documents:  62% 5/8 [00:54<00:32, 10.82s/it]\u001b[A\n",
            "Scoring documents:  75% 6/8 [01:04<00:21, 10.64s/it]\u001b[A\n",
            "Scoring documents:  88% 7/8 [01:15<00:10, 10.70s/it]\u001b[A\n",
            "Scoring documents: 100% 8/8 [01:22<00:00,  9.51s/it]\u001b[A\n",
            "Processing queries:  90% 9/10 [12:22<01:22, 82.37s/it]\n",
            "Re-ranking 30 documents for training query 75800075\n",
            "Original pre-ranking (first 3): ['33464411', '34284570', '74966633']\n",
            "\n",
            "Scoring documents:   0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "Scoring documents:  12% 1/8 [00:11<01:23, 11.88s/it]\u001b[A\n",
            "Scoring documents:  25% 2/8 [00:22<01:07, 11.26s/it]\u001b[A\n",
            "Scoring documents:  38% 3/8 [00:32<00:53, 10.68s/it]\u001b[A\n",
            "Scoring documents:  50% 4/8 [00:43<00:42, 10.74s/it]\u001b[A\n",
            "Scoring documents:  62% 5/8 [00:54<00:32, 10.78s/it]\u001b[A\n",
            "Scoring documents:  75% 6/8 [01:05<00:21, 10.82s/it]\u001b[A\n",
            "Scoring documents:  88% 7/8 [01:16<00:10, 10.84s/it]\u001b[A\n",
            "Scoring documents: 100% 8/8 [01:22<00:00,  9.30s/it]\u001b[A\n",
            "Processing queries: 100% 10/10 [13:44<00:00, 82.49s/it]\n",
            "Saving re-ranked results to prediction2.json...\n",
            "Re-ranking complete!\n",
            "Number of training queries processed: 10\n",
            "\n",
            "Test set predictions saved to: prediction2.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# MPNET CLAIMS\n",
        "\n",
        "best_model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
        "\n",
        "best_text_type = \"claims\"\n",
        "max_length = 512\n",
        "\n",
        "print(\"\\nRunning reranking on the test set...\")\n",
        "!python cross_encoder_reranking_train.py \\\n",
        "    --model_name \"{best_model_name}\" \\\n",
        "    --text_type \"{best_text_type}\" \\\n",
        "    --pre_ranking \"{shuffled_pre_ranking_file}\" \\\n",
        "    --queries_list \"{test_queries_file}\" \\\n",
        "    --queries_content \"{queries_content_file}\" \\\n",
        "    --documents_content \"{documents_content_file}\" \\\n",
        "    --output \"{test_predictions_file}\" \\\n",
        "    --max_length {max_length}\n",
        "\n",
        "print(f\"\\nTest set predictions saved to: {test_predictions_file}\")"
      ],
      "metadata": {
        "id": "EbxxPdIlskVt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60c89d26-a515-426d-f362-0f617e13edee"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Running reranking on the test set...\n",
            "Loading training queries from test_queries.json...\n",
            "Loaded 10 training queries\n",
            "Loading pre-ranking data from shuffled_pre_ranking.json...\n",
            "Filtered pre-ranking to 10 training queries\n",
            "Loading query content from queries_content_with_features.json...\n",
            "Loading document content from documents_content_with_features.json...\n",
            "Loading model sentence-transformers/all-mpnet-base-v2...\n",
            "2025-04-08 19:45:35.508232: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1744141535.568049    6654 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1744141535.586182    6654 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-04-08 19:45:35.648911: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Starting re-ranking process for training queries...\n",
            "Processing queries:   0% 0/10 [00:00<?, ?it/s]\n",
            "Re-ranking 30 documents for training query 103964109\n",
            "Original pre-ranking (first 3): ['94596291', '65451984', '81098918']\n",
            "\n",
            "Scoring documents:   0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "Scoring documents:  12% 1/8 [00:11<01:22, 11.77s/it]\u001b[A\n",
            "Scoring documents:  25% 2/8 [00:22<01:07, 11.27s/it]\u001b[A\n",
            "Scoring documents:  38% 3/8 [00:33<00:55, 11.13s/it]\u001b[A\n",
            "Scoring documents:  50% 4/8 [00:43<00:43, 10.80s/it]\u001b[A\n",
            "Scoring documents:  62% 5/8 [00:53<00:31, 10.45s/it]\u001b[A\n",
            "Scoring documents:  75% 6/8 [01:04<00:21, 10.62s/it]\u001b[A\n",
            "Scoring documents:  88% 7/8 [01:15<00:10, 10.69s/it]\u001b[A\n",
            "Scoring documents: 100% 8/8 [01:21<00:00,  9.21s/it]\u001b[A\n",
            "Processing queries:  10% 1/10 [01:21<12:14, 81.61s/it]\n",
            "Re-ranking 30 documents for training query 72214279\n",
            "Original pre-ranking (first 3): ['4112132', '7126783', '66704221']\n",
            "\n",
            "Scoring documents:   0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "Scoring documents:  12% 1/8 [00:10<01:10, 10.03s/it]\u001b[A\n",
            "Scoring documents:  25% 2/8 [00:21<01:03, 10.59s/it]\u001b[A\n",
            "Scoring documents:  38% 3/8 [00:31<00:53, 10.70s/it]\u001b[A\n",
            "Scoring documents:  50% 4/8 [00:42<00:43, 10.77s/it]\u001b[A\n",
            "Scoring documents:  62% 5/8 [00:53<00:32, 10.83s/it]\u001b[A\n",
            "Scoring documents:  75% 6/8 [01:04<00:21, 10.88s/it]\u001b[A\n",
            "Scoring documents:  88% 7/8 [01:17<00:11, 11.36s/it]\u001b[A\n",
            "Scoring documents: 100% 8/8 [01:24<00:00, 10.25s/it]\u001b[A\n",
            "Processing queries:  20% 2/10 [02:46<11:08, 83.53s/it]\n",
            "Re-ranking 30 documents for training query 68249923\n",
            "Original pre-ranking (first 3): ['35246469', '80619246', '7191590']\n",
            "\n",
            "Scoring documents:   0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "Scoring documents:  12% 1/8 [00:11<01:18, 11.28s/it]\u001b[A\n",
            "Scoring documents:  25% 2/8 [00:22<01:07, 11.21s/it]\u001b[A\n",
            "Scoring documents:  38% 3/8 [00:33<00:55, 11.14s/it]\u001b[A\n",
            "Scoring documents:  50% 4/8 [00:44<00:43, 10.99s/it]\u001b[A\n",
            "Scoring documents:  62% 5/8 [00:54<00:32, 10.80s/it]\u001b[A\n",
            "Scoring documents:  75% 6/8 [01:06<00:22, 11.13s/it]\u001b[A\n",
            "Scoring documents:  88% 7/8 [01:17<00:10, 10.96s/it]\u001b[A\n",
            "Scoring documents: 100% 8/8 [01:23<00:00,  9.38s/it]\u001b[A\n",
            "Processing queries:  30% 3/10 [04:09<09:43, 83.34s/it]\n",
            "Re-ranking 30 documents for training query 79740635\n",
            "Original pre-ranking (first 3): ['72282319', '80126462', '6507475']\n",
            "\n",
            "Scoring documents:   0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "Scoring documents:  12% 1/8 [00:10<01:11, 10.26s/it]\u001b[A\n",
            "Scoring documents:  25% 2/8 [00:21<01:03, 10.60s/it]\u001b[A\n",
            "Scoring documents:  38% 3/8 [00:31<00:52, 10.58s/it]\u001b[A\n",
            "Scoring documents:  50% 4/8 [00:42<00:42, 10.67s/it]\u001b[A\n",
            "Scoring documents:  62% 5/8 [00:52<00:31, 10.51s/it]\u001b[A\n",
            "Scoring documents:  75% 6/8 [01:01<00:20, 10.08s/it]\u001b[A\n",
            "Scoring documents:  88% 7/8 [01:11<00:10, 10.03s/it]\u001b[A\n",
            "Scoring documents: 100% 8/8 [01:18<00:00,  9.04s/it]\u001b[A\n",
            "Processing queries:  40% 4/10 [05:28<08:09, 81.54s/it]\n",
            "Re-ranking 30 documents for training query 100251983\n",
            "Original pre-ranking (first 3): ['82028496', '89295990', '78872192']\n",
            "\n",
            "Scoring documents:   0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "Scoring documents:  12% 1/8 [00:11<01:17, 11.12s/it]\u001b[A\n",
            "Scoring documents:  25% 2/8 [00:21<01:03, 10.63s/it]\u001b[A\n",
            "Scoring documents:  38% 3/8 [00:32<00:54, 10.81s/it]\u001b[A\n",
            "Scoring documents:  50% 4/8 [00:43<00:44, 11.00s/it]\u001b[A\n",
            "Scoring documents:  62% 5/8 [00:54<00:33, 11.07s/it]\u001b[A\n",
            "Scoring documents:  75% 6/8 [01:05<00:22, 11.06s/it]\u001b[A\n",
            "Scoring documents:  88% 7/8 [01:17<00:11, 11.12s/it]\u001b[A\n",
            "Scoring documents: 100% 8/8 [01:22<00:00,  9.23s/it]\u001b[A\n",
            "Processing queries:  50% 5/10 [06:50<06:49, 81.84s/it]\n",
            "Re-ranking 30 documents for training query 76109416\n",
            "Original pre-ranking (first 3): ['67929958', '1212129', '74871145']\n",
            "\n",
            "Scoring documents:   0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "Scoring documents:  12% 1/8 [00:11<01:17, 11.02s/it]\u001b[A\n",
            "Scoring documents:  25% 2/8 [00:22<01:06, 11.05s/it]\u001b[A\n",
            "Scoring documents:  38% 3/8 [00:33<00:55, 11.09s/it]\u001b[A\n",
            "Scoring documents:  50% 4/8 [00:44<00:44, 11.03s/it]\u001b[A\n",
            "Scoring documents:  62% 5/8 [00:55<00:32, 10.97s/it]\u001b[A\n",
            "Scoring documents:  75% 6/8 [01:05<00:21, 10.65s/it]\u001b[A\n",
            "Scoring documents:  88% 7/8 [01:16<00:11, 11.06s/it]\u001b[A\n",
            "Scoring documents: 100% 8/8 [01:23<00:00,  9.78s/it]\u001b[A\n",
            "Processing queries:  60% 6/10 [08:14<05:30, 82.57s/it]\n",
            "Re-ranking 30 documents for training query 85685768\n",
            "Original pre-ranking (first 3): ['5399125', '103242931', '96992323']\n",
            "\n",
            "Scoring documents:   0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "Scoring documents:  12% 1/8 [00:11<01:18, 11.18s/it]\u001b[A\n",
            "Scoring documents:  25% 2/8 [00:22<01:06, 11.11s/it]\u001b[A\n",
            "Scoring documents:  38% 3/8 [00:32<00:54, 10.91s/it]\u001b[A\n",
            "Scoring documents:  50% 4/8 [00:43<00:42, 10.73s/it]\u001b[A\n",
            "Scoring documents:  62% 5/8 [00:54<00:32, 10.72s/it]\u001b[A\n",
            "Scoring documents:  75% 6/8 [01:05<00:21, 10.87s/it]\u001b[A\n",
            "Scoring documents:  88% 7/8 [01:16<00:10, 10.95s/it]\u001b[A\n",
            "Scoring documents: 100% 8/8 [01:22<00:00,  9.53s/it]\u001b[A\n",
            "Processing queries:  70% 7/10 [09:37<04:07, 82.66s/it]\n",
            "Re-ranking 30 documents for training query 70563808\n",
            "Original pre-ranking (first 3): ['76578541', '66791385', '44519425']\n",
            "\n",
            "Scoring documents:   0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "Scoring documents:  12% 1/8 [00:10<01:14, 10.60s/it]\u001b[A\n",
            "Scoring documents:  25% 2/8 [00:21<01:04, 10.68s/it]\u001b[A\n",
            "Scoring documents:  38% 3/8 [00:31<00:53, 10.67s/it]\u001b[A\n",
            "Scoring documents:  50% 4/8 [00:42<00:42, 10.72s/it]\u001b[A\n",
            "Scoring documents:  62% 5/8 [00:53<00:31, 10.55s/it]\u001b[A\n",
            "Scoring documents:  75% 6/8 [01:02<00:20, 10.24s/it]\u001b[A\n",
            "Scoring documents:  88% 7/8 [01:13<00:10, 10.40s/it]\u001b[A\n",
            "Scoring documents: 100% 8/8 [01:19<00:00,  9.14s/it]\u001b[A\n",
            "Processing queries:  80% 8/10 [10:57<02:43, 81.76s/it]\n",
            "Re-ranking 30 documents for training query 79482665\n",
            "Original pre-ranking (first 3): ['87571623', '92503480', '68914227']\n",
            "\n",
            "Scoring documents:   0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "Scoring documents:  12% 1/8 [00:10<01:16, 10.89s/it]\u001b[A\n",
            "Scoring documents:  25% 2/8 [00:22<01:06, 11.04s/it]\u001b[A\n",
            "Scoring documents:  38% 3/8 [00:32<00:53, 10.76s/it]\u001b[A\n",
            "Scoring documents:  50% 4/8 [00:43<00:43, 10.86s/it]\u001b[A\n",
            "Scoring documents:  62% 5/8 [00:54<00:32, 10.92s/it]\u001b[A\n",
            "Scoring documents:  75% 6/8 [01:05<00:21, 10.95s/it]\u001b[A\n",
            "Scoring documents:  88% 7/8 [01:16<00:11, 11.00s/it]\u001b[A\n",
            "Scoring documents: 100% 8/8 [01:24<00:00,  9.88s/it]\u001b[A\n",
            "Processing queries:  90% 9/10 [12:21<01:22, 82.50s/it]\n",
            "Re-ranking 30 documents for training query 75800075\n",
            "Original pre-ranking (first 3): ['33464411', '34284570', '74966633']\n",
            "\n",
            "Scoring documents:   0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "Scoring documents:  12% 1/8 [00:10<01:14, 10.61s/it]\u001b[A\n",
            "Scoring documents:  25% 2/8 [00:21<01:05, 10.95s/it]\u001b[A\n",
            "Scoring documents:  38% 3/8 [00:38<01:07, 13.41s/it]\u001b[A\n",
            "Scoring documents:  50% 4/8 [00:49<00:49, 12.44s/it]\u001b[A\n",
            "Scoring documents:  62% 5/8 [01:00<00:35, 11.99s/it]\u001b[A\n",
            "Scoring documents:  75% 6/8 [01:11<00:23, 11.67s/it]\u001b[A\n",
            "Scoring documents:  88% 7/8 [01:22<00:11, 11.49s/it]\u001b[A\n",
            "Scoring documents: 100% 8/8 [01:28<00:00,  9.70s/it]\u001b[A\n",
            "Processing queries: 100% 10/10 [13:49<00:00, 82.98s/it]\n",
            "Saving re-ranked results to prediction2.json...\n",
            "Re-ranking complete!\n",
            "Number of training queries processed: 10\n",
            "\n",
            "Test set predictions saved to: prediction2.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# MPNET tac1\n",
        "\n",
        "best_model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
        "\n",
        "best_text_type = \"tac1\"\n",
        "max_length = 512\n",
        "\n",
        "print(\"\\nRunning reranking on the test set...\")\n",
        "!python cross_encoder_reranking_train.py \\\n",
        "    --model_name \"{best_model_name}\" \\\n",
        "    --text_type \"{best_text_type}\" \\\n",
        "    --pre_ranking \"{shuffled_pre_ranking_file}\" \\\n",
        "    --queries_list \"{test_queries_file}\" \\\n",
        "    --queries_content \"{queries_content_file}\" \\\n",
        "    --documents_content \"{documents_content_file}\" \\\n",
        "    --output \"{test_predictions_file}\" \\\n",
        "    --max_length {max_length}\n",
        "\n",
        "print(f\"\\nTest set predictions saved to: {test_predictions_file}\")"
      ],
      "metadata": {
        "id": "Ld8y5NfyXAzm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# MPNET DESCRIPTION\n",
        "\n",
        "best_model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
        "\n",
        "best_text_type = \"description\"\n",
        "max_length = 512\n",
        "\n",
        "print(\"\\nRunning reranking on the test set...\")\n",
        "!python cross_encoder_reranking_train.py \\\n",
        "    --model_name \"{best_model_name}\" \\\n",
        "    --text_type \"{best_text_type}\" \\\n",
        "    --pre_ranking \"{shuffled_pre_ranking_file}\" \\\n",
        "    --queries_list \"{test_queries_file}\" \\\n",
        "    --queries_content \"{queries_content_file}\" \\\n",
        "    --documents_content \"{documents_content_file}\" \\\n",
        "    --output \"{test_predictions_file}\" \\\n",
        "    --max_length {max_length}\n",
        "\n",
        "print(f\"\\nTest set predictions saved to: {test_predictions_file}\")"
      ],
      "metadata": {
        "id": "szrZRXfWXDuH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# BGE TA\n",
        "\n",
        "best_model_name = \"BAAI/bge-large-en\"\n",
        "\n",
        "best_text_type = \"TA\"\n",
        "max_length = 512\n",
        "\n",
        "print(\"\\nRunning reranking on the test set...\")\n",
        "!python cross_encoder_reranking_train.py \\\n",
        "    --model_name \"{best_model_name}\" \\\n",
        "    --text_type \"{best_text_type}\" \\\n",
        "    --pre_ranking \"{shuffled_pre_ranking_file}\" \\\n",
        "    --queries_list \"{test_queries_file}\" \\\n",
        "    --queries_content \"{queries_content_file}\" \\\n",
        "    --documents_content \"{documents_content_file}\" \\\n",
        "    --output \"{test_predictions_file}\" \\\n",
        "    --max_length {max_length}\n",
        "\n",
        "print(f\"\\nTest set predictions saved to: {test_predictions_file}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mj086sJHGIfc",
        "outputId": "a9c1747e-0866-46d7-a576-b213f15c618a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Running reranking on the test set...\n",
            "Loading training queries from test_queries.json...\n",
            "Loaded 10 training queries\n",
            "Loading pre-ranking data from shuffled_pre_ranking.json...\n",
            "Filtered pre-ranking to 10 training queries\n",
            "Loading query content from queries_content_with_features.json...\n",
            "Loading document content from documents_content_with_features.json...\n",
            "Loading model BAAI/bge-large-en...\n",
            "2025-04-08 20:58:06.225130: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1744145886.294874   23973 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1744145886.316726   23973 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-04-08 20:58:06.502939: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Starting re-ranking process for training queries...\n",
            "Processing queries:   0% 0/10 [00:00<?, ?it/s]\n",
            "Re-ranking 30 documents for training query 103964109\n",
            "Original pre-ranking (first 3): ['94596291', '65451984', '81098918']\n",
            "\n",
            "Scoring documents:   0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "Scoring documents:  12% 1/8 [00:20<02:24, 20.61s/it]\u001b[A\n",
            "Scoring documents:  25% 2/8 [00:35<01:44, 17.37s/it]\u001b[A\n",
            "Scoring documents:  38% 3/8 [00:50<01:20, 16.02s/it]\u001b[A\n",
            "Scoring documents:  50% 4/8 [01:04<01:01, 15.35s/it]\u001b[A\n",
            "Scoring documents:  62% 5/8 [01:18<00:44, 14.99s/it]\u001b[A\n",
            "Scoring documents:  75% 6/8 [01:33<00:29, 14.77s/it]\u001b[A\n",
            "Scoring documents:  88% 7/8 [01:47<00:14, 14.65s/it]\u001b[A\n",
            "Scoring documents: 100% 8/8 [01:56<00:00, 12.81s/it]\u001b[A\n",
            "Processing queries:  10% 1/10 [01:56<17:27, 116.41s/it]\n",
            "Re-ranking 30 documents for training query 72214279\n",
            "Original pre-ranking (first 3): ['4112132', '7126783', '66704221']\n",
            "\n",
            "Scoring documents:   0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "Scoring documents:  12% 1/8 [00:14<01:43, 14.77s/it]\u001b[A\n",
            "Scoring documents:  25% 2/8 [00:26<01:17, 12.98s/it]\u001b[A\n",
            "Scoring documents:  38% 3/8 [00:39<01:05, 13.05s/it]\u001b[A\n",
            "Scoring documents:  50% 4/8 [00:49<00:46, 11.65s/it]\u001b[A\n",
            "Scoring documents:  62% 5/8 [01:00<00:34, 11.41s/it]\u001b[A\n",
            "Scoring documents:  75% 6/8 [01:15<00:25, 12.82s/it]\u001b[A\n",
            "Scoring documents:  88% 7/8 [01:26<00:12, 12.01s/it]\u001b[A\n",
            "Scoring documents: 100% 8/8 [01:31<00:00,  9.84s/it]\u001b[A\n",
            "Processing queries:  20% 2/10 [03:27<13:32, 101.59s/it]\n",
            "Re-ranking 30 documents for training query 68249923\n",
            "Original pre-ranking (first 3): ['35246469', '80619246', '7191590']\n",
            "\n",
            "Scoring documents:   0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "Scoring documents:  12% 1/8 [00:12<01:26, 12.36s/it]\u001b[A\n",
            "Scoring documents:  25% 2/8 [00:24<01:14, 12.45s/it]\u001b[A\n",
            "Scoring documents:  38% 3/8 [00:38<01:04, 12.87s/it]\u001b[A\n",
            "Scoring documents:  50% 4/8 [00:50<00:50, 12.67s/it]\u001b[A\n",
            "Scoring documents:  62% 5/8 [01:02<00:37, 12.52s/it]\u001b[A\n",
            "Scoring documents:  75% 6/8 [01:15<00:24, 12.44s/it]\u001b[A\n",
            "Scoring documents:  88% 7/8 [01:27<00:12, 12.39s/it]\u001b[A\n",
            "Scoring documents: 100% 8/8 [01:35<00:00, 10.96s/it]\u001b[A\n",
            "Processing queries:  30% 3/10 [05:02<11:31, 98.73s/it] \n",
            "Re-ranking 30 documents for training query 79740635\n",
            "Original pre-ranking (first 3): ['72282319', '80126462', '6507475']\n",
            "\n",
            "Scoring documents:   0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "Scoring documents:  12% 1/8 [00:10<01:13, 10.52s/it]\u001b[A\n",
            "Scoring documents:  25% 2/8 [00:20<00:59,  9.93s/it]\u001b[A\n",
            "Scoring documents:  38% 3/8 [00:31<00:53, 10.73s/it]\u001b[A\n",
            "Scoring documents:  50% 4/8 [00:43<00:44, 11.00s/it]\u001b[A\n",
            "Scoring documents:  62% 5/8 [00:53<00:32, 10.85s/it]\u001b[A\n",
            "Scoring documents:  75% 6/8 [01:06<00:23, 11.61s/it]\u001b[A\n",
            "Scoring documents:  88% 7/8 [01:17<00:11, 11.26s/it]\u001b[A\n",
            "Scoring documents: 100% 8/8 [01:23<00:00,  9.53s/it]\u001b[A\n",
            "Processing queries:  40% 4/10 [06:26<09:15, 92.58s/it]\n",
            "Re-ranking 30 documents for training query 100251983\n",
            "Original pre-ranking (first 3): ['82028496', '89295990', '78872192']\n",
            "\n",
            "Scoring documents:   0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "Scoring documents:  12% 1/8 [00:11<01:21, 11.61s/it]\u001b[A\n",
            "Scoring documents:  25% 2/8 [00:23<01:09, 11.65s/it]\u001b[A\n",
            "Scoring documents:  38% 3/8 [00:34<00:57, 11.41s/it]\u001b[A\n",
            "Scoring documents:  50% 4/8 [00:47<00:48, 12.22s/it]\u001b[A\n",
            "Scoring documents:  62% 5/8 [01:00<00:37, 12.44s/it]\u001b[A\n",
            "Scoring documents:  75% 6/8 [01:13<00:24, 12.45s/it]\u001b[A\n",
            "Scoring documents:  88% 7/8 [01:25<00:12, 12.38s/it]\u001b[A\n",
            "Scoring documents: 100% 8/8 [01:32<00:00, 10.74s/it]\u001b[A\n",
            "Processing queries:  50% 5/10 [07:58<07:43, 92.60s/it]\n",
            "Re-ranking 30 documents for training query 76109416\n",
            "Original pre-ranking (first 3): ['67929958', '1212129', '74871145']\n",
            "\n",
            "Scoring documents:   0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "Scoring documents:  12% 1/8 [00:11<01:21, 11.65s/it]\u001b[A\n",
            "Scoring documents:  25% 2/8 [00:22<01:08, 11.34s/it]\u001b[A\n",
            "Scoring documents:  38% 3/8 [00:33<00:56, 11.23s/it]\u001b[A\n",
            "Scoring documents:  50% 4/8 [00:45<00:45, 11.39s/it]\u001b[A\n",
            "Scoring documents:  62% 5/8 [00:57<00:34, 11.50s/it]\u001b[A\n",
            "Scoring documents:  75% 6/8 [01:08<00:23, 11.54s/it]\u001b[A\n",
            "Scoring documents:  88% 7/8 [01:20<00:11, 11.58s/it]\u001b[A\n",
            "Scoring documents: 100% 8/8 [01:29<00:00, 10.81s/it]\u001b[A\n",
            "Processing queries:  60% 6/10 [09:28<06:06, 91.60s/it]\n",
            "Re-ranking 30 documents for training query 85685768\n",
            "Original pre-ranking (first 3): ['5399125', '103242931', '96992323']\n",
            "\n",
            "Scoring documents:   0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "Scoring documents:  12% 1/8 [00:14<01:44, 14.88s/it]\u001b[A\n",
            "Scoring documents:  25% 2/8 [00:32<01:39, 16.59s/it]\u001b[A\n",
            "Scoring documents:  38% 3/8 [00:47<01:18, 15.65s/it]\u001b[A\n",
            "Scoring documents:  50% 4/8 [01:02<01:02, 15.60s/it]\u001b[A\n",
            "Scoring documents:  62% 5/8 [01:17<00:45, 15.27s/it]\u001b[A\n",
            "Scoring documents:  75% 6/8 [01:31<00:30, 15.02s/it]\u001b[A\n",
            "Scoring documents:  88% 7/8 [01:46<00:14, 14.86s/it]\u001b[A\n",
            "Scoring documents: 100% 8/8 [01:55<00:00, 13.11s/it]\u001b[A\n",
            "Processing queries:  70% 7/10 [11:24<04:58, 99.52s/it]\n",
            "Re-ranking 30 documents for training query 70563808\n",
            "Original pre-ranking (first 3): ['76578541', '66791385', '44519425']\n",
            "\n",
            "Scoring documents:   0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "Scoring documents:  12% 1/8 [00:12<01:29, 12.81s/it]\u001b[A\n",
            "Scoring documents:  25% 2/8 [00:25<01:16, 12.82s/it]\u001b[A\n",
            "Scoring documents:  38% 3/8 [00:37<01:01, 12.34s/it]\u001b[A\n",
            "Scoring documents:  50% 4/8 [00:49<00:48, 12.10s/it]\u001b[A\n",
            "Scoring documents:  62% 5/8 [01:02<00:37, 12.40s/it]\u001b[A\n",
            "Scoring documents:  75% 6/8 [01:13<00:24, 12.14s/it]\u001b[A\n",
            "Scoring documents:  88% 7/8 [01:26<00:12, 12.19s/it]\u001b[A\n",
            "Scoring documents: 100% 8/8 [01:33<00:00, 10.73s/it]\u001b[A\n",
            "Processing queries:  80% 8/10 [12:57<03:15, 97.64s/it]\n",
            "Re-ranking 30 documents for training query 79482665\n",
            "Original pre-ranking (first 3): ['87571623', '92503480', '68914227']\n",
            "\n",
            "Scoring documents:   0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "Scoring documents:  12% 1/8 [00:12<01:26, 12.38s/it]\u001b[A\n",
            "Scoring documents:  25% 2/8 [00:24<01:13, 12.30s/it]\u001b[A\n",
            "Scoring documents:  38% 3/8 [00:37<01:03, 12.66s/it]\u001b[A\n",
            "Scoring documents:  50% 4/8 [00:51<00:52, 13.04s/it]\u001b[A\n",
            "Scoring documents:  62% 5/8 [01:03<00:38, 12.80s/it]\u001b[A\n",
            "Scoring documents:  75% 6/8 [01:21<00:29, 14.59s/it]\u001b[A\n",
            "Scoring documents:  88% 7/8 [01:33<00:13, 13.75s/it]\u001b[A\n",
            "Scoring documents: 100% 8/8 [01:40<00:00, 11.61s/it]\u001b[A\n",
            "Processing queries:  90% 9/10 [14:38<01:38, 98.63s/it]\n",
            "Re-ranking 30 documents for training query 75800075\n",
            "Original pre-ranking (first 3): ['33464411', '34284570', '74966633']\n",
            "\n",
            "Scoring documents:   0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "Scoring documents:  12% 1/8 [00:13<01:32, 13.18s/it]\u001b[A\n",
            "Scoring documents:  25% 2/8 [00:24<01:11, 11.88s/it]\u001b[A\n",
            "Scoring documents:  38% 3/8 [00:36<00:59, 11.94s/it]\u001b[A\n",
            "Scoring documents:  50% 4/8 [00:47<00:47, 11.77s/it]\u001b[A\n",
            "Scoring documents:  62% 5/8 [01:00<00:36, 12.14s/it]\u001b[A\n",
            "Scoring documents:  75% 6/8 [01:15<00:26, 13.21s/it]\u001b[A\n",
            "Scoring documents:  88% 7/8 [01:26<00:12, 12.49s/it]\u001b[A\n",
            "Scoring documents: 100% 8/8 [01:35<00:00, 11.22s/it]\u001b[A\n",
            "Processing queries: 100% 10/10 [16:13<00:00, 97.39s/it]\n",
            "Saving re-ranked results to prediction2.json...\n",
            "Re-ranking complete!\n",
            "Number of training queries processed: 10\n",
            "\n",
            "Test set predictions saved to: prediction2.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# BGE CLAIMS\n",
        "\n",
        "best_model_name = \"BAAI/bge-large-en\"\n",
        "\n",
        "best_text_type = \"claims\"\n",
        "max_length = 512\n",
        "\n",
        "print(\"\\nRunning reranking on the test set...\")\n",
        "!python cross_encoder_reranking_train.py \\\n",
        "    --model_name \"{best_model_name}\" \\\n",
        "    --text_type \"{best_text_type}\" \\\n",
        "    --pre_ranking \"{shuffled_pre_ranking_file}\" \\\n",
        "    --queries_list \"{test_queries_file}\" \\\n",
        "    --queries_content \"{queries_content_file}\" \\\n",
        "    --documents_content \"{documents_content_file}\" \\\n",
        "    --output \"{test_predictions_file}\" \\\n",
        "    --max_length {max_length}\n",
        "\n",
        "print(f\"\\nTest set predictions saved to: {test_predictions_file}\")"
      ],
      "metadata": {
        "id": "dok5I2O-XJ2A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# BGE tac1\n",
        "\n",
        "best_model_name = \"BAAI/bge-large-en\"\n",
        "\n",
        "best_text_type = \"tac1\"\n",
        "max_length = 512\n",
        "\n",
        "print(\"\\nRunning reranking on the test set...\")\n",
        "!python cross_encoder_reranking_train.py \\\n",
        "    --model_name \"{best_model_name}\" \\\n",
        "    --text_type \"{best_text_type}\" \\\n",
        "    --pre_ranking \"{shuffled_pre_ranking_file}\" \\\n",
        "    --queries_list \"{test_queries_file}\" \\\n",
        "    --queries_content \"{queries_content_file}\" \\\n",
        "    --documents_content \"{documents_content_file}\" \\\n",
        "    --output \"{test_predictions_file}\" \\\n",
        "    --max_length {max_length}\n",
        "\n",
        "print(f\"\\nTest set predictions saved to: {test_predictions_file}\")"
      ],
      "metadata": {
        "id": "O5QJEzdMXL75"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# BGE DESCRIPTION\n",
        "\n",
        "best_model_name = \"BAAI/bge-large-en\"\n",
        "\n",
        "best_text_type = \"description\"\n",
        "max_length = 512\n",
        "\n",
        "print(\"\\nRunning reranking on the test set...\")\n",
        "!python cross_encoder_reranking_train.py \\\n",
        "    --model_name \"{best_model_name}\" \\\n",
        "    --text_type \"{best_text_type}\" \\\n",
        "    --pre_ranking \"{shuffled_pre_ranking_file}\" \\\n",
        "    --queries_list \"{test_queries_file}\" \\\n",
        "    --queries_content \"{queries_content_file}\" \\\n",
        "    --documents_content \"{documents_content_file}\" \\\n",
        "    --output \"{test_predictions_file}\" \\\n",
        "    --max_length {max_length}\n",
        "\n",
        "print(f\"\\nTest set predictions saved to: {test_predictions_file}\")"
      ],
      "metadata": {
        "id": "r_AYuJh-XOIo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# GEMINI TRY\n",
        "\n",
        "import time\n",
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from google import genai\n",
        "\n",
        "# Replace with your actual Gemini API key\n",
        "GEMINI_API_KEY = \"ton api\"\n",
        "\n",
        "# Initialize Gemini Client\n",
        "client = genai.Client(api_key=GEMINI_API_KEY)\n",
        "embedding_model_name = \"gemini-embedding-exp-03-07\"\n",
        "\n",
        "# Define the paths to your data files\n",
        "train_queries_file = \"train_queries.json\"\n",
        "test_queries_file = \"test_queries.json\"\n",
        "train_gold_mapping_file = \"train_gold_mapping.json\"\n",
        "shuffled_pre_ranking_file = \"shuffled_pre_ranking.json\"\n",
        "queries_content_file = \"queries_content_with_features.json\"\n",
        "documents_content_file = \"documents_content_with_features.json\"\n",
        "\n",
        "# Define the output file for test predictions\n",
        "test_predictions_file = \"predictions_gemini_exp.json\"\n",
        "\n",
        "# Check if necessary data files exist\n",
        "if not all(os.path.exists(f) for f in [test_queries_file, shuffled_pre_ranking_file, queries_content_file, documents_content_file]):\n",
        "    print(\"Error: One or more necessary data files for the test set are missing.\")\n",
        "    exit()\n",
        "else:\n",
        "    print(\"Necessary data files for the test set found.\")\n",
        "\n",
        "def load_json_file(file_path):\n",
        "    \"\"\"Load JSON data from a file\"\"\"\n",
        "    with open(file_path, 'r') as f:\n",
        "        return json.load(f)\n",
        "\n",
        "def save_json_file(data, file_path):\n",
        "    \"\"\"Save data to a JSON file\"\"\"\n",
        "    with open(file_path, 'w') as f:\n",
        "        json.dump(data, f, indent=2)\n",
        "\n",
        "def load_content_data(file_path):\n",
        "    \"\"\"Load content data from a JSON file\"\"\"\n",
        "    with open(file_path, 'r') as f:\n",
        "        data = json.load(f)\n",
        "    # Create a dictionary mapping FAN to Content\n",
        "    content_dict = {item['FAN']: item['Content'] for item in data}\n",
        "    return content_dict\n",
        "\n",
        "def extract_text(content_dict, text_type=\"full\"):\n",
        "    \"\"\"Extract text from patent content based on text_type\"\"\"\n",
        "    if text_type == \"TA\" or text_type == \"title_abstract\":\n",
        "        title = content_dict.get(\"title\", \"\")\n",
        "        abstract = content_dict.get(\"pa01\", \"\")\n",
        "        return f\"{title} {abstract}\".strip()\n",
        "    elif text_type == \"claims\":\n",
        "        claims = \" \".join([v for k, v in content_dict.items() if k.startswith('c-')])\n",
        "        return claims.strip()\n",
        "    elif text_type == \"description\":\n",
        "        description = \" \".join([v for k, v in content_dict.items() if k.startswith('p')])\n",
        "        return description.strip()\n",
        "    elif text_type == \"full\":\n",
        "        all_text = []\n",
        "        if \"title\" in content_dict:\n",
        "            all_text.append(content_dict[\"title\"])\n",
        "        if \"pa01\" in content_dict:\n",
        "            all_text.append(content_dict[\"pa01\"])\n",
        "        for key, value in content_dict.items():\n",
        "            if key not in [\"title\", \"pa01\"]:\n",
        "                all_text.append(value)\n",
        "        return \" \".join(all_text).strip()\n",
        "    elif text_type == \"tac1\":\n",
        "        title = content_dict.get(\"title\", \"\")\n",
        "        abstract = content_dict.get(\"pa01\", \"\")\n",
        "        first_claim = next((v for k, v in content_dict.items() if k.startswith('c-')), \"\")\n",
        "        return f\"{title} {abstract} {first_claim}\".strip()\n",
        "    return \"\"\n",
        "\n",
        "def get_embedding_gemini(text_list, batch_size=1):  # Adjust batch_size\n",
        "    \"\"\"Get embeddings for a list of texts using the specified Gemini embedding model with batching.\"\"\"\n",
        "    embeddings = []\n",
        "    for i in range(0, len(text_list), batch_size):\n",
        "        batch = text_list[i:i + batch_size]\n",
        "        try:\n",
        "            result = client.models.embed_content(\n",
        "                model=embedding_model_name,\n",
        "                contents=batch,  # Send a list of texts\n",
        "            )\n",
        "            if result.embeddings:\n",
        "                for embedding in result.embeddings:\n",
        "                    embeddings.append(embedding.values)\n",
        "            else:\n",
        "                print(f\"Warning: No embeddings returned for the batch starting at index {i}\")\n",
        "                return None\n",
        "            time.sleep(1)  # Add a small delay to avoid rate limits\n",
        "        except Exception as e:\n",
        "            print(f\"Error getting Gemini embeddings for batch starting at index {i}: {e}\")\n",
        "            return None\n",
        "    return embeddings\n",
        "\n",
        "# Load test queries and pre-ranking\n",
        "test_queries = load_json_file(test_queries_file)\n",
        "print(f\"Loaded {len(test_queries)} test queries.\")\n",
        "pre_ranking_test = load_json_file(shuffled_pre_ranking_file)\n",
        "pre_ranking_test_filtered = {fan: docs for fan, docs in pre_ranking_test.items() if fan in test_queries}\n",
        "print(f\"Filtered pre-ranking to {len(pre_ranking_test_filtered)} test queries.\")\n",
        "\n",
        "# Load content data\n",
        "queries_content = load_content_data(queries_content_file)\n",
        "documents_content = load_content_data(documents_content_file)\n",
        "\n",
        "# Rerank using Gemini embeddings\n",
        "re_ranked_predictions = {}\n",
        "best_text_type = \"claims\"  # You can experiment with other text types\n",
        "\n",
        "print(\"\\nStarting reranking process for test queries using Gemini Embedding Model...\")\n",
        "for query_fan, pre_ranked_docs in tqdm(pre_ranking_test_filtered.items(), desc=\"Processing queries\"):\n",
        "    if query_fan not in queries_content:\n",
        "        print(f\"Warning: Query FAN {query_fan} not found in content.\")\n",
        "        re_ranked_predictions[query_fan] = pre_ranked_docs  # Keep original ranking\n",
        "        continue\n",
        "\n",
        "    query_text = extract_text(queries_content[query_fan], best_text_type)\n",
        "    doc_texts = []\n",
        "    doc_fans = []\n",
        "    for doc_fan in pre_ranked_docs:\n",
        "        if doc_fan in documents_content:\n",
        "            doc_texts.append(extract_text(documents_content[doc_fan], best_text_type))\n",
        "            doc_fans.append(doc_fan)\n",
        "        else:\n",
        "            print(f\"Warning: Document FAN {doc_fan} not found in content.\")\n",
        "\n",
        "    if not doc_texts:\n",
        "        re_ranked_predictions[query_fan] = []\n",
        "        continue\n",
        "\n",
        "    all_texts = [query_text] + doc_texts\n",
        "    embeddings = get_embedding_gemini(all_texts)\n",
        "\n",
        "    if embeddings and len(embeddings) == len(all_texts):\n",
        "        query_embedding = embeddings[0]\n",
        "        doc_embeddings = embeddings[1:]\n",
        "        similarity_scores = cosine_similarity([query_embedding], doc_embeddings)[0]\n",
        "        ranked_indices = np.argsort(similarity_scores)[::-1]\n",
        "        re_ranked_predictions[query_fan] = [doc_fans[i] for i in ranked_indices]\n",
        "    else:\n",
        "        print(f\"Warning: Could not get embeddings for query {query_fan}. Keeping original ranking.\")\n",
        "        re_ranked_predictions[query_fan] = pre_ranked_docs\n",
        "\n",
        "# Save the re-ranked predictions\n",
        "save_json_file(re_ranked_predictions, test_predictions_file)\n",
        "print(f\"\\nTest set predictions saved to: {test_predictions_file}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CvRu6UgbN9yi",
        "outputId": "635dc882-4090-48b0-d0ab-66b7778333d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Necessary data files for the test set found.\n",
            "Loaded 10 test queries.\n",
            "Filtered pre-ranking to 10 test queries.\n",
            "\n",
            "Starting reranking process for test queries using Gemini Embedding Model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing queries:  10%|█         | 1/10 [00:12<01:53, 12.65s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error getting Gemini embeddings for batch starting at index 6: 429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'Resource has been exhausted (e.g. check quota).', 'status': 'RESOURCE_EXHAUSTED'}}\n",
            "Warning: Could not get embeddings for query 103964109. Keeping original ranking.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing queries:  40%|████      | 4/10 [00:13<00:12,  2.04s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error getting Gemini embeddings for batch starting at index 0: 429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'Resource has been exhausted (e.g. check quota).', 'status': 'RESOURCE_EXHAUSTED'}}\n",
            "Warning: Could not get embeddings for query 72214279. Keeping original ranking.\n",
            "Error getting Gemini embeddings for batch starting at index 0: 429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'Resource has been exhausted (e.g. check quota).', 'status': 'RESOURCE_EXHAUSTED'}}\n",
            "Warning: Could not get embeddings for query 68249923. Keeping original ranking.\n",
            "Error getting Gemini embeddings for batch starting at index 0: 429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'Resource has been exhausted (e.g. check quota).', 'status': 'RESOURCE_EXHAUSTED'}}\n",
            "Warning: Could not get embeddings for query 79740635. Keeping original ranking.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing queries:  60%|██████    | 6/10 [00:13<00:04,  1.11s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error getting Gemini embeddings for batch starting at index 0: 429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'Resource has been exhausted (e.g. check quota).', 'status': 'RESOURCE_EXHAUSTED'}}\n",
            "Warning: Could not get embeddings for query 100251983. Keeping original ranking.\n",
            "Error getting Gemini embeddings for batch starting at index 0: 429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'Resource has been exhausted (e.g. check quota).', 'status': 'RESOURCE_EXHAUSTED'}}\n",
            "Warning: Could not get embeddings for query 76109416. Keeping original ranking.\n",
            "Error getting Gemini embeddings for batch starting at index 0: 429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'Resource has been exhausted (e.g. check quota).', 'status': 'RESOURCE_EXHAUSTED'}}\n",
            "Warning: Could not get embeddings for query 85685768. Keeping original ranking.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing queries: 100%|██████████| 10/10 [00:13<00:00,  1.35s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error getting Gemini embeddings for batch starting at index 0: 429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'Resource has been exhausted (e.g. check quota).', 'status': 'RESOURCE_EXHAUSTED'}}\n",
            "Warning: Could not get embeddings for query 70563808. Keeping original ranking.\n",
            "Error getting Gemini embeddings for batch starting at index 0: 429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'Resource has been exhausted (e.g. check quota).', 'status': 'RESOURCE_EXHAUSTED'}}\n",
            "Warning: Could not get embeddings for query 79482665. Keeping original ranking.\n",
            "Error getting Gemini embeddings for batch starting at index 0: 429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'Resource has been exhausted (e.g. check quota).', 'status': 'RESOURCE_EXHAUSTED'}}\n",
            "Warning: Could not get embeddings for query 75800075. Keeping original ranking.\n",
            "\n",
            "Test set predictions saved to: predictions_gemini_exp.json\n",
            "\n",
            "Remember to download '{test_predictions_file}' and submit it to Codabench.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fuzzywuzzy\n",
        "import os\n",
        "import json\n",
        "import math\n",
        "from collections import Counter\n",
        "from fuzzywuzzy import fuzz  # pip install fuzzywuzzy\n",
        "from nltk.corpus import wordnet  # pip install nltk (and download wordnet data: import nltk; nltk.download('wordnet'))\n",
        "\n",
        "# PATHS\n",
        "train_queries_file = \"train_queries.json\"\n",
        "test_queries_file = \"test_queries.json\"\n",
        "train_gold_mapping_file = \"train_gold_mapping.json\"\n",
        "shuffled_pre_ranking_file = \"shuffled_pre_ranking.json\"\n",
        "queries_content_file = \"queries_content_with_features.json\"\n",
        "documents_content_file = \"documents_content_with_features.json\"\n",
        "test_predictions_file = \"creative_predictions.json\"\n",
        "\n",
        "def load_json_file(file_path):\n",
        "    with open(file_path, 'r') as f:\n",
        "        return json.load(f)\n",
        "\n",
        "if not all(os.path.exists(f) for f in [test_queries_file, shuffled_pre_ranking_file, queries_content_file, documents_content_file, train_gold_mapping_file, queries_content_file, documents_content_file, train_queries_file]):\n",
        "    print(\"Error: One or more necessary data files are missing.\")\n",
        "    exit()\n",
        "else:\n",
        "    print(\"Necessary data files found.\")\n",
        "\n",
        "test_queries = load_json_file(test_queries_file)\n",
        "pre_ranking_test = load_json_file(shuffled_pre_ranking_file)\n",
        "pre_ranking_test_filtered = {fan: docs for fan, docs in pre_ranking_test.items() if fan in test_queries}\n",
        "train_gold_mapping = load_json_file(train_gold_mapping_file)\n",
        "train_queries = load_json_file(train_queries_file)\n",
        "\n",
        "# --- Load and Transform queries_content ---\n",
        "queries_content_list = load_json_file(queries_content_file)\n",
        "queries_content = {}\n",
        "for item in queries_content_list:\n",
        "    patent_id = item.get(\"patent_id\")\n",
        "    if patent_id:\n",
        "        queries_content[patent_id] = item\n",
        "print(f\"Loaded and processed {len(queries_content)} query content items.\")\n",
        "\n",
        "# --- Load and Transform documents_content ---\n",
        "documents_content_list = load_json_file(documents_content_file)\n",
        "documents_content = {}\n",
        "for item in documents_content_list:\n",
        "    patent_id = item.get(\"patent_id\")\n",
        "    if patent_id:\n",
        "        documents_content[patent_id] = item\n",
        "print(f\"Loaded and processed {len(documents_content)} document content items.\")\n",
        "\n",
        "queries_content_train = {k: v for k, v in queries_content.items() if k in train_queries}\n",
        "documents_content_train = documents_content\n",
        "\n",
        "def prepare_feature_weights_tfidf(train_gold_mapping, queries_content, documents_content):\n",
        "    doc_feature_counts = {}\n",
        "    all_docs = {**queries_content, **documents_content}\n",
        "    total_num_docs = len(all_docs)\n",
        "\n",
        "    for doc_id, content in all_docs.items():\n",
        "        features = content.get('features', [])\n",
        "        doc_feature_counts[doc_id] = Counter(features)\n",
        "\n",
        "    feature_doc_frequency = Counter()\n",
        "    for doc_id, counts in doc_feature_counts.items():\n",
        "        for feature in counts:\n",
        "            feature_doc_frequency[feature] += 1\n",
        "\n",
        "    feature_weights_tfidf = {}\n",
        "    for doc_id, counts in doc_feature_counts.items():\n",
        "        for feature, count in counts.items():\n",
        "            tf = count / (sum(counts.values()) + 1e-6)\n",
        "            idf = math.log(total_num_docs / (feature_doc_frequency[feature] + 1) + 1e-6)\n",
        "            feature_weights_tfidf[feature] = feature_weights_tfidf.get(feature, 0) + tf * idf\n",
        "\n",
        "    return dict(feature_weights_tfidf)\n",
        "\n",
        "def calculate_feature_similarity_improved(query_features, doc_features, feature_weights_tfidf=None):\n",
        "    score = 0\n",
        "\n",
        "    # 1. TF-IDF Weighted Overlap\n",
        "    if feature_weights_tfidf:\n",
        "        common_features = set(query_features) & set(doc_features)\n",
        "        for feature in common_features:\n",
        "            score += feature_weights_tfidf.get(feature, 0)\n",
        "\n",
        "    # 2. Fuzzy Matching\n",
        "    fuzzy_score = 0\n",
        "    for q_feature in query_features:\n",
        "        for d_feature in doc_features:\n",
        "            ratio = fuzz.ratio(q_feature, d_feature)\n",
        "            if ratio > 85:  # Increased threshold\n",
        "                fuzzy_score += ratio / 100.0 * 0.2  # Reduced weight\n",
        "\n",
        "    score += fuzzy_score\n",
        "\n",
        "    # 3. N-gram Overlap (bi-grams)\n",
        "    def get_ngrams(text, n):\n",
        "        n_grams = set()\n",
        "        words = text.split()\n",
        "        for i in range(len(words) - n + 1):\n",
        "            n_grams.add(\" \".join(words[i:i+n]))\n",
        "        return n_grams\n",
        "\n",
        "    ngram_overlap_score = 0\n",
        "    for q_feature in query_features:\n",
        "        for d_feature in doc_features:\n",
        "            q_2grams = get_ngrams(q_feature, 2)\n",
        "            d_2grams = get_ngrams(d_feature, 2)\n",
        "            overlap = len(q_2grams & d_2grams)\n",
        "            union = len(q_2grams | d_2grams)\n",
        "            if union > 0:\n",
        "                ngram_overlap_score += overlap / union * 0.1  # Jaccard-like\n",
        "\n",
        "    score += ngram_overlap_score\n",
        "\n",
        "    return score\n",
        "\n",
        "def creative_reranking(pre_ranking, queries_content, documents_content, feature_weights_tfidf=None):\n",
        "    ranked_results = {}\n",
        "    for query_id, initial_ranking in pre_ranking.items():\n",
        "        if query_id in queries_content:\n",
        "            query_features = queries_content[query_id].get('features', [])\n",
        "            scored_documents = []\n",
        "            for doc_id in initial_ranking:\n",
        "                if doc_id in documents_content:\n",
        "                    doc_features = documents_content[doc_id].get('features', [])\n",
        "                    similarity_score = calculate_feature_similarity_improved(\n",
        "                        query_features,\n",
        "                        doc_features,\n",
        "                        feature_weights_tfidf=feature_weights_tfidf\n",
        "                    )\n",
        "                    scored_documents.append((doc_id, similarity_score))\n",
        "            scored_documents.sort(key=lambda item: item[1], reverse=True)\n",
        "            ranked_results[query_id] = [doc_id for doc_id, score in scored_documents]\n",
        "        else:\n",
        "            ranked_results[query_id] = initial_ranking\n",
        "    return ranked_results\n",
        "\n",
        "# --- Prepare Feature Weights using TF-IDF ---\n",
        "feature_weights_tfidf = prepare_feature_weights_tfidf(train_gold_mapping, queries_content_train, documents_content_train)\n",
        "\n",
        "# --- Perform Creative Reranking on Test Set ---\n",
        "reranked_predictions = creative_reranking(\n",
        "    pre_ranking_test_filtered,\n",
        "    queries_content,\n",
        "    documents_content,\n",
        "    feature_weights_tfidf=feature_weights_tfidf\n",
        ")\n",
        "\n",
        "# --- Save Predictions ---\n",
        "with open(test_predictions_file, 'w') as f:\n",
        "    json.dump(reranked_predictions, f, indent=4)\n",
        "\n",
        "print(f\"\\nCreative test set predictions saved to: {test_predictions_file}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KJUIacEyPmfU",
        "outputId": "8ee87e1a-f0af-4833-be9b-4b3567c35ada"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fuzzywuzzy\n",
            "  Downloading fuzzywuzzy-0.18.0-py2.py3-none-any.whl.metadata (4.9 kB)\n",
            "Downloading fuzzywuzzy-0.18.0-py2.py3-none-any.whl (18 kB)\n",
            "Installing collected packages: fuzzywuzzy\n",
            "Successfully installed fuzzywuzzy-0.18.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/fuzzywuzzy/fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
            "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Necessary data files found.\n",
            "Loaded and processed 0 query content items.\n",
            "Loaded and processed 0 document content items.\n",
            "\n",
            "Creative test set predictions saved to: creative_predictions.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1VWRmb2NfHm8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}