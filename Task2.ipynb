{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers torch\n",
        "\n",
        "import os\n",
        "import json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Dq4ZzzFroSe",
        "outputId": "9c6e072c-0a27-46fb-ddff-32a245602d97"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# PATHS\n",
        "train_queries_file = \"train_queries.json\"\n",
        "test_queries_file = \"test_queries.json\"\n",
        "train_gold_mapping_file = \"train_gold_mapping.json\"\n",
        "shuffled_pre_ranking_file = \"shuffled_pre_ranking.json\"\n",
        "queries_content_file = \"queries_content_with_features.json\"\n",
        "documents_content_file = \"documents_content_with_features.json\"\n",
        "\n",
        "test_predictions_file = \"prediction2.json\"\n",
        "\n",
        "if not all(os.path.exists(f) for f in [test_queries_file, shuffled_pre_ranking_file, queries_content_file, documents_content_file]):\n",
        "    print(\"Error: One or more necessary data files for the test set are missing.\")\n",
        "    exit()\n",
        "else:\n",
        "    print(\"Necessary data files for the test set found.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zx5y8Z_-rrbn",
        "outputId": "faa457b0-f3af-420d-8e0c-27c7aad145c2"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Necessary data files for the test set found.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# LOAD\n",
        "\n",
        "def load_json_file(file_path):\n",
        "    with open(file_path, 'r') as f:\n",
        "        return json.load(f)\n",
        "\n",
        "test_queries = load_json_file(test_queries_file)\n",
        "print(f\"Loaded {len(test_queries)} test queries.\")\n",
        "\n",
        "pre_ranking_test = load_json_file(shuffled_pre_ranking_file)\n",
        "# filter pre-ranking to include only test queries (important!)\n",
        "pre_ranking_test_filtered = {fan: docs for fan, docs in pre_ranking_test.items() if fan in test_queries}\n",
        "print(f\"Filtered pre-ranking to {len(pre_ranking_test_filtered)} test queries.\")\n",
        "\n",
        "\n",
        "queries_content = load_json_file(queries_content_file)\n",
        "documents_content = load_json_file(documents_content_file)\n",
        "\n",
        "\n",
        "if not os.path.exists(\"cross_encoder_reranking_train.py\"):\n",
        "    print(\"Error: The 'cross_encoder_reranking_train.py' script is missing. Please upload it.\")\n",
        "    exit()\n",
        "else:\n",
        "    print(\"Reranking script found.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X5F18z5rsAn_",
        "outputId": "622e02f0-6857-4a49-882e-9065033cf792"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 10 test queries.\n",
            "Filtered pre-ranking to 10 test queries.\n",
            "Reranking script found.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# E5 LARGE ta\n",
        "\n",
        "best_model_name = \"intfloat/e5-large-v2\"\n",
        "best_text_type = \"ta\"\n",
        "max_length = 512\n",
        "\n",
        "print(\"\\nRunning reranking on the test set...\")\n",
        "!python cross_encoder_reranking_train.py \\\n",
        "    --model_name \"{best_model_name}\" \\\n",
        "    --text_type \"{best_text_type}\" \\\n",
        "    --pre_ranking \"{shuffled_pre_ranking_file}\" \\\n",
        "    --queries_list \"{test_queries_file}\" \\\n",
        "    --queries_content \"{queries_content_file}\" \\\n",
        "    --documents_content \"{documents_content_file}\" \\\n",
        "    --output \"{test_predictions_file}\" \\\n",
        "    --max_length {max_length}\n",
        "\n",
        "print(f\"\\nTest set predictions saved to: {test_predictions_file}\")"
      ],
      "metadata": {
        "id": "SciWrL5J6-8Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# E5 LARGE tac1\n",
        "\n",
        "best_model_name = \"intfloat/e5-large-v2\"\n",
        "best_text_type = \"tac1\"\n",
        "max_length = 512\n",
        "\n",
        "print(\"\\nRunning reranking on the test set...\")\n",
        "!python cross_encoder_reranking_train.py \\\n",
        "    --model_name \"{best_model_name}\" \\\n",
        "    --text_type \"{best_text_type}\" \\\n",
        "    --pre_ranking \"{shuffled_pre_ranking_file}\" \\\n",
        "    --queries_list \"{test_queries_file}\" \\\n",
        "    --queries_content \"{queries_content_file}\" \\\n",
        "    --documents_content \"{documents_content_file}\" \\\n",
        "    --output \"{test_predictions_file}\" \\\n",
        "    --max_length {max_length}\n",
        "\n",
        "print(f\"\\nTest set predictions saved to: {test_predictions_file}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nHgGozrztq7I",
        "outputId": "cb6a12b8-0e8e-42f2-e461-1ee3a0291a94"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Running reranking on the test set...\n",
            "Loading training queries from test_queries.json...\n",
            "Loaded 10 training queries\n",
            "Loading pre-ranking data from shuffled_pre_ranking.json...\n",
            "Filtered pre-ranking to 10 training queries\n",
            "Loading query content from queries_content_with_features.json...\n",
            "Loading document content from documents_content_with_features.json...\n",
            "Loading model intfloat/e5-large-v2...\n",
            "2025-04-07 10:15:39.328529: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1744020939.720766   39376 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1744020939.816623   39376 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-04-07 10:15:40.624398: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Starting re-ranking process for training queries...\n",
            "Processing queries:   0% 0/10 [00:00<?, ?it/s]\n",
            "Re-ranking 30 documents for training query 103964109\n",
            "Original pre-ranking (first 3): ['94596291', '65451984', '81098918']\n",
            "\n",
            "Scoring documents:   0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "Scoring documents:  12% 1/8 [00:35<04:09, 35.66s/it]\u001b[A\n",
            "Scoring documents:  25% 2/8 [01:14<03:46, 37.68s/it]\u001b[A\n",
            "Scoring documents:  38% 3/8 [01:48<02:59, 35.85s/it]\u001b[A\n",
            "Scoring documents:  50% 4/8 [02:15<02:08, 32.23s/it]\u001b[A\n",
            "Scoring documents:  62% 5/8 [02:44<01:33, 31.33s/it]\u001b[A\n",
            "Scoring documents:  75% 6/8 [03:11<00:59, 29.72s/it]\u001b[A\n",
            "Scoring documents:  88% 7/8 [03:44<00:30, 30.82s/it]\u001b[A\n",
            "Scoring documents: 100% 8/8 [04:00<00:00, 26.22s/it]\u001b[A\n",
            "Processing queries:  10% 1/10 [04:00<36:08, 240.90s/it]\n",
            "Re-ranking 30 documents for training query 72214279\n",
            "Original pre-ranking (first 3): ['4112132', '7126783', '66704221']\n",
            "\n",
            "Scoring documents:   0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "Scoring documents:  12% 1/8 [00:32<03:46, 32.42s/it]\u001b[A\n",
            "Scoring documents:  25% 2/8 [01:05<03:18, 33.05s/it]\u001b[A\n",
            "Scoring documents:  38% 3/8 [01:38<02:43, 32.74s/it]\u001b[A\n",
            "Scoring documents:  50% 4/8 [02:04<02:00, 30.10s/it]\u001b[A\n",
            "Scoring documents:  62% 5/8 [02:37<01:34, 31.34s/it]\u001b[A\n",
            "Scoring documents:  75% 6/8 [03:10<01:03, 31.74s/it]\u001b[A\n",
            "Scoring documents:  88% 7/8 [03:41<00:31, 31.66s/it]\u001b[A\n",
            "Scoring documents: 100% 8/8 [03:55<00:00, 25.86s/it]\u001b[A\n",
            "Processing queries:  20% 2/10 [07:56<31:40, 237.62s/it]\n",
            "Re-ranking 30 documents for training query 68249923\n",
            "Original pre-ranking (first 3): ['35246469', '80619246', '7191590']\n",
            "\n",
            "Scoring documents:   0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "Scoring documents:  12% 1/8 [00:32<03:45, 32.25s/it]\u001b[A\n",
            "Scoring documents:  25% 2/8 [01:05<03:17, 32.88s/it]\u001b[A\n",
            "Scoring documents:  38% 3/8 [01:37<02:42, 32.59s/it]\u001b[A\n",
            "Scoring documents:  50% 4/8 [02:11<02:11, 32.96s/it]\u001b[A\n",
            "Scoring documents:  62% 5/8 [02:43<01:38, 32.71s/it]\u001b[A\n",
            "Scoring documents:  75% 6/8 [03:23<01:10, 35.19s/it]\u001b[A\n",
            "Scoring documents:  88% 7/8 [03:57<00:34, 34.86s/it]\u001b[A\n",
            "Scoring documents: 100% 8/8 [04:16<00:00, 29.87s/it]\u001b[A\n",
            "Processing queries:  30% 3/10 [12:13<28:45, 246.46s/it]\n",
            "Re-ranking 30 documents for training query 79740635\n",
            "Original pre-ranking (first 3): ['72282319', '80126462', '6507475']\n",
            "\n",
            "Scoring documents:   0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "Scoring documents:  12% 1/8 [00:33<03:55, 33.67s/it]\u001b[A\n",
            "Scoring documents:  25% 2/8 [01:05<03:16, 32.80s/it]\u001b[A\n",
            "Scoring documents:  38% 3/8 [01:39<02:45, 33.17s/it]\u001b[A\n",
            "Scoring documents:  50% 4/8 [02:11<02:11, 32.82s/it]\u001b[A\n",
            "Scoring documents:  62% 5/8 [02:45<01:39, 33.02s/it]\u001b[A\n",
            "Scoring documents:  75% 6/8 [03:17<01:05, 32.73s/it]\u001b[A\n",
            "Scoring documents:  88% 7/8 [03:50<00:32, 32.90s/it]\u001b[A\n",
            "Scoring documents: 100% 8/8 [04:09<00:00, 28.41s/it]\u001b[A\n",
            "Processing queries:  40% 4/10 [16:22<24:45, 247.60s/it]\n",
            "Re-ranking 30 documents for training query 100251983\n",
            "Original pre-ranking (first 3): ['82028496', '89295990', '78872192']\n",
            "\n",
            "Scoring documents:   0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "Scoring documents:  12% 1/8 [00:32<03:48, 32.64s/it]\u001b[A\n",
            "Scoring documents:  25% 2/8 [01:05<03:16, 32.67s/it]\u001b[A\n",
            "Scoring documents:  38% 3/8 [01:37<02:43, 32.66s/it]\u001b[A\n",
            "Scoring documents:  50% 4/8 [02:08<02:07, 31.99s/it]\u001b[A\n",
            "Scoring documents:  62% 5/8 [02:41<01:36, 32.04s/it]\u001b[A\n",
            "Scoring documents:  75% 6/8 [03:14<01:05, 32.51s/it]\u001b[A\n",
            "Scoring documents:  88% 7/8 [03:43<00:31, 31.26s/it]\u001b[A\n",
            "Scoring documents: 100% 8/8 [04:00<00:00, 26.97s/it]\u001b[A\n",
            "Processing queries:  50% 5/10 [20:23<20:26, 245.21s/it]\n",
            "Re-ranking 30 documents for training query 76109416\n",
            "Original pre-ranking (first 3): ['67929958', '1212129', '74871145']\n",
            "\n",
            "Scoring documents:   0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "Scoring documents:  12% 1/8 [00:33<03:53, 33.41s/it]\u001b[A\n",
            "Scoring documents:  25% 2/8 [01:05<03:16, 32.75s/it]\u001b[A\n",
            "Scoring documents:  38% 3/8 [01:39<02:45, 33.07s/it]\u001b[A\n",
            "Scoring documents:  50% 4/8 [02:11<02:11, 32.75s/it]\u001b[A\n",
            "Scoring documents:  62% 5/8 [02:44<01:38, 32.97s/it]\u001b[A\n",
            "Scoring documents:  75% 6/8 [03:17<01:05, 32.73s/it]\u001b[A\n",
            "Scoring documents:  88% 7/8 [03:50<00:33, 33.00s/it]\u001b[A\n",
            "Scoring documents: 100% 8/8 [04:09<00:00, 28.41s/it]\u001b[A\n",
            "Processing queries:  60% 6/10 [24:32<16:26, 246.56s/it]\n",
            "Re-ranking 30 documents for training query 85685768\n",
            "Original pre-ranking (first 3): ['5399125', '103242931', '96992323']\n",
            "\n",
            "Scoring documents:   0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "Scoring documents:  12% 1/8 [00:32<03:50, 32.94s/it]\u001b[A\n",
            "Scoring documents:  25% 2/8 [01:04<03:13, 32.27s/it]\u001b[A\n",
            "Scoring documents:  38% 3/8 [01:38<02:43, 32.73s/it]\u001b[A\n",
            "Scoring documents:  50% 4/8 [02:09<02:09, 32.31s/it]\u001b[A\n",
            "Scoring documents:  62% 5/8 [02:42<01:37, 32.53s/it]\u001b[A\n",
            "Scoring documents:  75% 6/8 [03:14<01:04, 32.26s/it]\u001b[A\n",
            "Scoring documents:  88% 7/8 [03:46<00:32, 32.39s/it]\u001b[A\n",
            "Scoring documents: 100% 8/8 [04:05<00:00, 28.07s/it]\u001b[A\n",
            "Processing queries:  70% 7/10 [28:38<12:18, 246.31s/it]\n",
            "Re-ranking 30 documents for training query 70563808\n",
            "Original pre-ranking (first 3): ['76578541', '66791385', '44519425']\n",
            "\n",
            "Scoring documents:   0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "Scoring documents:  12% 1/8 [00:25<02:59, 25.60s/it]\u001b[A\n",
            "Scoring documents:  25% 2/8 [00:49<02:27, 24.60s/it]\u001b[A\n",
            "Scoring documents:  38% 3/8 [01:21<02:19, 27.86s/it]\u001b[A\n",
            "Scoring documents:  50% 4/8 [01:54<01:59, 29.88s/it]\u001b[A\n",
            "Scoring documents:  62% 5/8 [02:25<01:31, 30.53s/it]\u001b[A\n",
            "Scoring documents:  75% 6/8 [02:49<00:56, 28.25s/it]\u001b[A\n",
            "Scoring documents:  88% 7/8 [03:21<00:29, 29.36s/it]\u001b[A\n",
            "Scoring documents: 100% 8/8 [03:41<00:00, 26.32s/it]\u001b[A\n",
            "Processing queries:  80% 8/10 [32:19<07:56, 238.31s/it]\n",
            "Re-ranking 30 documents for training query 79482665\n",
            "Original pre-ranking (first 3): ['87571623', '92503480', '68914227']\n",
            "\n",
            "Scoring documents:   0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "Scoring documents:  12% 1/8 [00:31<03:41, 31.61s/it]\u001b[A\n",
            "Scoring documents:  25% 2/8 [01:04<03:14, 32.37s/it]\u001b[A\n",
            "Scoring documents:  38% 3/8 [01:36<02:40, 32.07s/it]\u001b[A\n",
            "Scoring documents:  50% 4/8 [02:09<02:10, 32.51s/it]\u001b[A\n",
            "Scoring documents:  62% 5/8 [02:41<01:36, 32.18s/it]\u001b[A\n",
            "Scoring documents:  75% 6/8 [03:13<01:04, 32.34s/it]\u001b[A\n",
            "Scoring documents:  88% 7/8 [03:45<00:32, 32.24s/it]\u001b[A\n",
            "Scoring documents: 100% 8/8 [04:04<00:00, 27.97s/it]\u001b[A\n",
            "Processing queries:  90% 9/10 [36:24<04:00, 240.26s/it]\n",
            "Re-ranking 30 documents for training query 75800075\n",
            "Original pre-ranking (first 3): ['33464411', '34284570', '74966633']\n",
            "\n",
            "Scoring documents:   0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "Scoring documents:  12% 1/8 [00:32<03:47, 32.55s/it]\u001b[A\n",
            "Scoring documents:  25% 2/8 [01:02<03:07, 31.31s/it]\u001b[A\n",
            "Scoring documents:  38% 3/8 [01:34<02:37, 31.50s/it]\u001b[A\n",
            "Scoring documents:  50% 4/8 [02:06<02:06, 31.55s/it]\u001b[A\n",
            "Scoring documents:  62% 5/8 [02:38<01:35, 31.67s/it]\u001b[A\n",
            "Scoring documents:  75% 6/8 [03:09<01:02, 31.48s/it]\u001b[A\n",
            "Scoring documents:  88% 7/8 [03:41<00:31, 31.56s/it]\u001b[A\n",
            "Scoring documents: 100% 8/8 [04:00<00:00, 27.63s/it]\u001b[A\n",
            "Processing queries: 100% 10/10 [40:24<00:00, 242.45s/it]\n",
            "Saving re-ranked results to prediction2.json...\n",
            "Re-ranking complete!\n",
            "Number of training queries processed: 10\n",
            "\n",
            "Test set predictions saved to: prediction2.json\n",
            "python3: can't open file '/content/evaluate_train_ranking.py': [Errno 2] No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# E5 LARGE CLAIMS\n",
        "\n",
        "best_model_name = \"intfloat/e5-large-v2\"\n",
        "best_text_type = \"claims\"\n",
        "max_length = 512\n",
        "\n",
        "print(\"\\nRunning reranking on the test set...\")\n",
        "!python cross_encoder_reranking_train.py \\\n",
        "    --model_name \"{best_model_name}\" \\\n",
        "    --text_type \"{best_text_type}\" \\\n",
        "    --pre_ranking \"{shuffled_pre_ranking_file}\" \\\n",
        "    --queries_list \"{test_queries_file}\" \\\n",
        "    --queries_content \"{queries_content_file}\" \\\n",
        "    --documents_content \"{documents_content_file}\" \\\n",
        "    --output \"{test_predictions_file}\" \\\n",
        "    --max_length {max_length}\n",
        "\n",
        "print(f\"\\nTest set predictions saved to: {test_predictions_file}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ksH7DVKFsSKm",
        "outputId": "343b9cd2-b72e-43b6-9c14-ee97b77dc3f8"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Running reranking on the test set...\n",
            "Loading training queries from test_queries.json...\n",
            "Loaded 10 training queries\n",
            "Loading pre-ranking data from shuffled_pre_ranking.json...\n",
            "Filtered pre-ranking to 10 training queries\n",
            "Loading query content from queries_content_with_features.json...\n",
            "Loading document content from documents_content_with_features.json...\n",
            "Loading model intfloat/e5-large-v2...\n",
            "2025-04-07 17:58:02.882627: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1744048682.917781  149992 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1744048682.928136  149992 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-04-07 17:58:02.964933: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Starting re-ranking process for training queries...\n",
            "Processing queries:   0% 0/10 [00:00<?, ?it/s]\n",
            "Re-ranking 30 documents for training query 103964109\n",
            "Original pre-ranking (first 3): ['94596291', '65451984', '81098918']\n",
            "\n",
            "Scoring documents:   0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "Scoring documents:  12% 1/8 [00:49<05:45, 49.32s/it]\u001b[A\n",
            "Scoring documents:  25% 2/8 [01:24<04:06, 41.02s/it]\u001b[A\n",
            "Scoring documents:  38% 3/8 [01:57<03:05, 37.15s/it]\u001b[A\n",
            "Scoring documents:  50% 4/8 [02:30<02:23, 35.79s/it]\u001b[A\n",
            "Scoring documents:  62% 5/8 [03:03<01:43, 34.52s/it]\u001b[A\n",
            "Scoring documents:  75% 6/8 [03:46<01:15, 37.66s/it]\u001b[A\n",
            "Scoring documents:  88% 7/8 [04:20<00:36, 36.35s/it]\u001b[A\n",
            "Scoring documents: 100% 8/8 [04:39<00:00, 30.90s/it]\u001b[A\n",
            "Processing queries:  10% 1/10 [04:39<41:57, 279.70s/it]\n",
            "Re-ranking 30 documents for training query 72214279\n",
            "Original pre-ranking (first 3): ['4112132', '7126783', '66704221']\n",
            "\n",
            "Scoring documents:   0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "Scoring documents:  12% 1/8 [00:34<04:04, 34.91s/it]\u001b[A\n",
            "Scoring documents:  25% 2/8 [01:09<03:28, 34.82s/it]\u001b[A\n",
            "Scoring documents:  38% 3/8 [01:42<02:49, 33.83s/it]\u001b[A\n",
            "Scoring documents:  50% 4/8 [02:15<02:14, 33.68s/it]\u001b[A\n",
            "Scoring documents:  62% 5/8 [02:48<01:39, 33.17s/it]\u001b[A\n",
            "Scoring documents:  75% 6/8 [03:21<01:06, 33.28s/it]\u001b[A\n",
            "Scoring documents:  88% 7/8 [03:53<00:32, 32.94s/it]\u001b[A\n",
            "Scoring documents: 100% 8/8 [04:14<00:00, 28.98s/it]\u001b[A\n",
            "Processing queries:  20% 2/10 [08:53<35:17, 264.73s/it]\n",
            "Re-ranking 30 documents for training query 68249923\n",
            "Original pre-ranking (first 3): ['35246469', '80619246', '7191590']\n",
            "\n",
            "Scoring documents:   0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "Scoring documents:  12% 1/8 [00:32<03:45, 32.23s/it]\u001b[A\n",
            "Scoring documents:  25% 2/8 [01:08<03:26, 34.44s/it]\u001b[A\n",
            "Scoring documents:  38% 3/8 [01:42<02:52, 34.50s/it]\u001b[A\n",
            "Scoring documents:  50% 4/8 [02:22<02:26, 36.50s/it]\u001b[A\n",
            "Scoring documents:  62% 5/8 [02:56<01:46, 35.55s/it]\u001b[A\n",
            "Scoring documents:  75% 6/8 [03:28<01:09, 34.50s/it]\u001b[A\n",
            "Scoring documents:  88% 7/8 [04:02<00:34, 34.17s/it]\u001b[A\n",
            "Scoring documents: 100% 8/8 [04:21<00:00, 29.41s/it]\u001b[A\n",
            "Processing queries:  30% 3/10 [13:15<30:42, 263.22s/it]\n",
            "Re-ranking 30 documents for training query 79740635\n",
            "Original pre-ranking (first 3): ['72282319', '80126462', '6507475']\n",
            "\n",
            "Scoring documents:   0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "Scoring documents:  12% 1/8 [00:33<03:54, 33.47s/it]\u001b[A\n",
            "Scoring documents:  25% 2/8 [01:05<03:16, 32.75s/it]\u001b[A\n",
            "Scoring documents:  38% 3/8 [01:39<02:45, 33.05s/it]\u001b[A\n",
            "Scoring documents:  50% 4/8 [02:12<02:12, 33.21s/it]\u001b[A\n",
            "Scoring documents:  62% 5/8 [02:46<01:39, 33.33s/it]\u001b[A\n",
            "Scoring documents:  75% 6/8 [03:18<01:05, 32.94s/it]\u001b[A\n",
            "Scoring documents:  88% 7/8 [03:51<00:33, 33.11s/it]\u001b[A\n",
            "Scoring documents: 100% 8/8 [04:11<00:00, 28.79s/it]\u001b[A\n",
            "Processing queries:  40% 4/10 [17:26<25:51, 258.52s/it]\n",
            "Re-ranking 30 documents for training query 100251983\n",
            "Original pre-ranking (first 3): ['82028496', '89295990', '78872192']\n",
            "\n",
            "Scoring documents:   0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "Scoring documents:  12% 1/8 [00:33<03:55, 33.64s/it]\u001b[A\n",
            "Scoring documents:  25% 2/8 [01:06<03:17, 32.92s/it]\u001b[A\n",
            "Scoring documents:  38% 3/8 [01:39<02:45, 33.15s/it]\u001b[A\n",
            "Scoring documents:  50% 4/8 [02:11<02:11, 32.82s/it]\u001b[A\n",
            "Scoring documents:  62% 5/8 [02:46<01:40, 33.56s/it]\u001b[A\n",
            "Scoring documents:  75% 6/8 [03:18<01:06, 33.11s/it]\u001b[A\n",
            "Scoring documents:  88% 7/8 [03:55<00:34, 34.28s/it]\u001b[A\n",
            "Scoring documents: 100% 8/8 [04:15<00:00, 29.77s/it]\u001b[A\n",
            "Processing queries:  50% 5/10 [21:42<21:27, 257.51s/it]\n",
            "Re-ranking 30 documents for training query 76109416\n",
            "Original pre-ranking (first 3): ['67929958', '1212129', '74871145']\n",
            "\n",
            "Scoring documents:   0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "Scoring documents:  12% 1/8 [00:33<03:51, 33.11s/it]\u001b[A\n",
            "Scoring documents:  25% 2/8 [01:06<03:19, 33.23s/it]\u001b[A\n",
            "Scoring documents:  38% 3/8 [01:39<02:45, 33.09s/it]\u001b[A\n",
            "Scoring documents:  50% 4/8 [02:13<02:14, 33.52s/it]\u001b[A\n",
            "Scoring documents:  62% 5/8 [02:47<01:40, 33.55s/it]\u001b[A\n",
            "Scoring documents:  75% 6/8 [03:21<01:07, 33.78s/it]\u001b[A\n",
            "Scoring documents:  88% 7/8 [03:54<00:33, 33.45s/it]\u001b[A\n",
            "Scoring documents: 100% 8/8 [04:14<00:00, 29.38s/it]\u001b[A\n",
            "Processing queries:  60% 6/10 [25:57<17:06, 256.58s/it]\n",
            "Re-ranking 30 documents for training query 85685768\n",
            "Original pre-ranking (first 3): ['5399125', '103242931', '96992323']\n",
            "\n",
            "Scoring documents:   0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "Scoring documents:  12% 1/8 [00:32<03:46, 32.29s/it]\u001b[A\n",
            "Scoring documents:  25% 2/8 [01:05<03:18, 33.04s/it]\u001b[A\n",
            "Scoring documents:  38% 3/8 [01:38<02:43, 32.78s/it]\u001b[A\n",
            "Scoring documents:  50% 4/8 [02:16<02:20, 35.04s/it]\u001b[A\n",
            "Scoring documents:  62% 5/8 [02:53<01:46, 35.59s/it]\u001b[A\n",
            "Scoring documents:  75% 6/8 [03:27<01:10, 35.20s/it]\u001b[A\n",
            "Scoring documents:  88% 7/8 [04:00<00:34, 34.42s/it]\u001b[A\n",
            "Scoring documents: 100% 8/8 [04:20<00:00, 29.88s/it]\u001b[A\n",
            "Processing queries:  70% 7/10 [30:17<12:53, 257.96s/it]\n",
            "Re-ranking 30 documents for training query 70563808\n",
            "Original pre-ranking (first 3): ['76578541', '66791385', '44519425']\n",
            "\n",
            "Scoring documents:   0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "Scoring documents:  12% 1/8 [00:32<03:49, 32.85s/it]\u001b[A\n",
            "Scoring documents:  25% 2/8 [01:11<03:36, 36.13s/it]\u001b[A\n",
            "Scoring documents:  38% 3/8 [01:44<02:54, 34.82s/it]\u001b[A\n",
            "Scoring documents:  50% 4/8 [02:18<02:17, 34.45s/it]\u001b[A\n",
            "Scoring documents:  62% 5/8 [02:50<01:41, 33.72s/it]\u001b[A\n",
            "Scoring documents:  75% 6/8 [03:24<01:07, 33.72s/it]\u001b[A\n",
            "Scoring documents:  88% 7/8 [03:58<00:33, 33.74s/it]\u001b[A\n",
            "Scoring documents: 100% 8/8 [04:18<00:00, 29.56s/it]\u001b[A\n",
            "Processing queries:  80% 8/10 [34:36<08:36, 258.28s/it]\n",
            "Re-ranking 30 documents for training query 79482665\n",
            "Original pre-ranking (first 3): ['87571623', '92503480', '68914227']\n",
            "\n",
            "Scoring documents:   0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "Scoring documents:  12% 1/8 [00:32<03:46, 32.41s/it]\u001b[A\n",
            "Scoring documents:  25% 2/8 [01:06<03:18, 33.15s/it]\u001b[A\n",
            "Scoring documents:  38% 3/8 [01:38<02:44, 32.86s/it]\u001b[A\n",
            "Scoring documents:  50% 4/8 [02:12<02:12, 33.12s/it]\u001b[A\n",
            "Scoring documents:  62% 5/8 [02:44<01:38, 32.85s/it]\u001b[A\n",
            "Scoring documents:  75% 6/8 [03:18<01:06, 33.19s/it]\u001b[A\n",
            "Scoring documents:  88% 7/8 [03:50<00:32, 32.89s/it]\u001b[A\n",
            "Scoring documents: 100% 8/8 [04:12<00:00, 29.39s/it]\u001b[A\n",
            "Processing queries:  90% 9/10 [38:49<04:16, 256.48s/it]\n",
            "Re-ranking 30 documents for training query 75800075\n",
            "Original pre-ranking (first 3): ['33464411', '34284570', '74966633']\n",
            "\n",
            "Scoring documents:   0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "Scoring documents:  12% 1/8 [00:32<03:47, 32.45s/it]\u001b[A\n",
            "Scoring documents:  25% 2/8 [01:05<03:18, 33.10s/it]\u001b[A\n",
            "Scoring documents:  38% 3/8 [01:38<02:43, 32.79s/it]\u001b[A\n",
            "Scoring documents:  50% 4/8 [02:12<02:12, 33.15s/it]\u001b[A\n",
            "Scoring documents:  62% 5/8 [02:44<01:38, 32.88s/it]\u001b[A\n",
            "Scoring documents:  75% 6/8 [03:18<01:06, 33.12s/it]\u001b[A\n",
            "Scoring documents:  88% 7/8 [03:50<00:32, 32.86s/it]\u001b[A\n",
            "Scoring documents: 100% 8/8 [04:11<00:00, 28.95s/it]\u001b[A\n",
            "Processing queries: 100% 10/10 [43:00<00:00, 258.05s/it]\n",
            "Saving re-ranked results to prediction2.json...\n",
            "Re-ranking complete!\n",
            "Number of training queries processed: 10\n",
            "\n",
            "Test set predictions saved to: prediction2.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# MPNET TA\n",
        "\n",
        "best_model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
        "\n",
        "best_text_type = \"TA\"\n",
        "max_length = 512\n",
        "\n",
        "print(\"\\nRunning reranking on the test set...\")\n",
        "!python cross_encoder_reranking_train.py \\\n",
        "    --model_name \"{best_model_name}\" \\\n",
        "    --text_type \"{best_text_type}\" \\\n",
        "    --pre_ranking \"{shuffled_pre_ranking_file}\" \\\n",
        "    --queries_list \"{test_queries_file}\" \\\n",
        "    --queries_content \"{queries_content_file}\" \\\n",
        "    --documents_content \"{documents_content_file}\" \\\n",
        "    --output \"{test_predictions_file}\" \\\n",
        "    --max_length {max_length}\n",
        "\n",
        "print(f\"\\nTest set predictions saved to: {test_predictions_file}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NjvHOL-ju0xA",
        "outputId": "378c7668-cc15-4fc6-cabd-d70078811960"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Running reranking on the test set...\n",
            "Loading training queries from test_queries.json...\n",
            "Loaded 10 training queries\n",
            "Loading pre-ranking data from shuffled_pre_ranking.json...\n",
            "Filtered pre-ranking to 10 training queries\n",
            "Loading query content from queries_content_with_features.json...\n",
            "Loading document content from documents_content_with_features.json...\n",
            "Loading model sentence-transformers/all-mpnet-base-v2...\n",
            "tokenizer_config.json: 100% 363/363 [00:00<00:00, 1.55MB/s]\n",
            "vocab.txt: 100% 232k/232k [00:00<00:00, 6.33MB/s]\n",
            "tokenizer.json: 100% 466k/466k [00:00<00:00, 28.5MB/s]\n",
            "special_tokens_map.json: 100% 239/239 [00:00<00:00, 1.04MB/s]\n",
            "config.json: 100% 571/571 [00:00<00:00, 2.83MB/s]\n",
            "2025-04-06 21:54:12.458387: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1743976452.498408   26612 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1743976452.510664   26612 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-04-06 21:54:12.552027: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "model.safetensors: 100% 438M/438M [00:06<00:00, 63.9MB/s]\n",
            "Starting re-ranking process for training queries...\n",
            "Processing queries:   0% 0/10 [00:00<?, ?it/s]\n",
            "Re-ranking 30 documents for training query 103964109\n",
            "Original pre-ranking (first 3): ['94596291', '65451984', '81098918']\n",
            "\n",
            "Scoring documents:   0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "Scoring documents:  12% 1/8 [00:05<00:36,  5.26s/it]\u001b[A\n",
            "Scoring documents:  25% 2/8 [00:11<00:34,  5.71s/it]\u001b[A\n",
            "Scoring documents:  38% 3/8 [00:16<00:27,  5.42s/it]\u001b[A\n",
            "Scoring documents:  50% 4/8 [00:21<00:20,  5.22s/it]\u001b[A\n",
            "Scoring documents:  62% 5/8 [00:27<00:16,  5.56s/it]\u001b[A\n",
            "Scoring documents:  75% 6/8 [00:32<00:10,  5.32s/it]\u001b[A\n",
            "Scoring documents:  88% 7/8 [00:38<00:05,  5.52s/it]\u001b[A\n",
            "Scoring documents: 100% 8/8 [00:41<00:00,  4.79s/it]\u001b[A\n",
            "Processing queries:  10% 1/10 [00:41<06:12, 41.43s/it]\n",
            "Re-ranking 30 documents for training query 72214279\n",
            "Original pre-ranking (first 3): ['4112132', '7126783', '66704221']\n",
            "\n",
            "Scoring documents:   0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "Scoring documents:  12% 1/8 [00:05<00:35,  5.03s/it]\u001b[A\n",
            "Scoring documents:  25% 2/8 [00:09<00:29,  4.99s/it]\u001b[A\n",
            "Scoring documents:  38% 3/8 [00:14<00:24,  4.95s/it]\u001b[A\n",
            "Scoring documents:  50% 4/8 [00:18<00:16,  4.24s/it]\u001b[A\n",
            "Scoring documents:  62% 5/8 [00:22<00:12,  4.25s/it]\u001b[A\n",
            "Scoring documents:  75% 6/8 [00:28<00:09,  4.91s/it]\u001b[A\n",
            "Scoring documents:  88% 7/8 [00:31<00:04,  4.44s/it]\u001b[A\n",
            "Scoring documents: 100% 8/8 [00:33<00:00,  3.65s/it]\u001b[A\n",
            "Processing queries:  20% 2/10 [01:15<04:56, 37.02s/it]\n",
            "Re-ranking 30 documents for training query 68249923\n",
            "Original pre-ranking (first 3): ['35246469', '80619246', '7191590']\n",
            "\n",
            "Scoring documents:   0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "Scoring documents:  12% 1/8 [00:05<00:38,  5.54s/it]\u001b[A\n",
            "Scoring documents:  25% 2/8 [00:09<00:28,  4.81s/it]\u001b[A\n",
            "Scoring documents:  38% 3/8 [00:14<00:22,  4.59s/it]\u001b[A\n",
            "Scoring documents:  50% 4/8 [00:19<00:19,  4.95s/it]\u001b[A\n",
            "Scoring documents:  62% 5/8 [00:23<00:14,  4.71s/it]\u001b[A\n",
            "Scoring documents:  75% 6/8 [00:28<00:09,  4.73s/it]\u001b[A\n",
            "Scoring documents:  88% 7/8 [00:33<00:04,  4.83s/it]\u001b[A\n",
            "Scoring documents: 100% 8/8 [00:36<00:00,  4.13s/it]\u001b[A\n",
            "Processing queries:  30% 3/10 [01:51<04:17, 36.73s/it]\n",
            "Re-ranking 30 documents for training query 79740635\n",
            "Original pre-ranking (first 3): ['72282319', '80126462', '6507475']\n",
            "\n",
            "Scoring documents:   0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "Scoring documents:  12% 1/8 [00:03<00:24,  3.57s/it]\u001b[A\n",
            "Scoring documents:  25% 2/8 [00:08<00:26,  4.35s/it]\u001b[A\n",
            "Scoring documents:  38% 3/8 [00:12<00:21,  4.21s/it]\u001b[A\n",
            "Scoring documents:  50% 4/8 [00:16<00:16,  4.09s/it]\u001b[A\n",
            "Scoring documents:  62% 5/8 [00:21<00:13,  4.35s/it]\u001b[A\n",
            "Scoring documents:  75% 6/8 [00:25<00:08,  4.43s/it]\u001b[A\n",
            "Scoring documents:  88% 7/8 [00:29<00:04,  4.16s/it]\u001b[A\n",
            "Scoring documents: 100% 8/8 [00:31<00:00,  3.57s/it]\u001b[A\n",
            "Processing queries:  40% 4/10 [02:23<03:28, 34.76s/it]\n",
            "Re-ranking 30 documents for training query 100251983\n",
            "Original pre-ranking (first 3): ['82028496', '89295990', '78872192']\n",
            "\n",
            "Scoring documents:   0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "Scoring documents:  12% 1/8 [00:05<00:36,  5.16s/it]\u001b[A\n",
            "Scoring documents:  25% 2/8 [00:09<00:26,  4.49s/it]\u001b[A\n",
            "Scoring documents:  38% 3/8 [00:12<00:20,  4.17s/it]\u001b[A\n",
            "Scoring documents:  50% 4/8 [00:18<00:19,  4.90s/it]\u001b[A\n",
            "Scoring documents:  62% 5/8 [00:23<00:14,  4.75s/it]\u001b[A\n",
            "Scoring documents:  75% 6/8 [00:28<00:09,  4.87s/it]\u001b[A\n",
            "Scoring documents:  88% 7/8 [00:32<00:04,  4.67s/it]\u001b[A\n",
            "Scoring documents: 100% 8/8 [00:35<00:00,  3.92s/it]\u001b[A\n",
            "Processing queries:  50% 5/10 [02:58<02:54, 34.90s/it]\n",
            "Re-ranking 30 documents for training query 76109416\n",
            "Original pre-ranking (first 3): ['67929958', '1212129', '74871145']\n",
            "\n",
            "Scoring documents:   0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "Scoring documents:  12% 1/8 [00:04<00:28,  4.02s/it]\u001b[A\n",
            "Scoring documents:  25% 2/8 [00:09<00:30,  5.04s/it]\u001b[A\n",
            "Scoring documents:  38% 3/8 [00:13<00:22,  4.57s/it]\u001b[A\n",
            "Scoring documents:  50% 4/8 [00:17<00:17,  4.34s/it]\u001b[A\n",
            "Scoring documents:  62% 5/8 [00:23<00:14,  4.69s/it]\u001b[A\n",
            "Scoring documents:  75% 6/8 [00:27<00:08,  4.46s/it]\u001b[A\n",
            "Scoring documents:  88% 7/8 [00:31<00:04,  4.32s/it]\u001b[A\n",
            "Scoring documents: 100% 8/8 [00:35<00:00,  4.34s/it]\u001b[A\n",
            "Processing queries:  60% 6/10 [03:34<02:20, 35.11s/it]\n",
            "Re-ranking 30 documents for training query 85685768\n",
            "Original pre-ranking (first 3): ['5399125', '103242931', '96992323']\n",
            "\n",
            "Scoring documents:   0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "Scoring documents:  12% 1/8 [00:05<00:38,  5.48s/it]\u001b[A\n",
            "Scoring documents:  25% 2/8 [00:11<00:33,  5.63s/it]\u001b[A\n",
            "Scoring documents:  38% 3/8 [00:17<00:29,  5.81s/it]\u001b[A\n",
            "Scoring documents:  50% 4/8 [00:22<00:22,  5.65s/it]\u001b[A\n",
            "Scoring documents:  62% 5/8 [00:29<00:18,  6.00s/it]\u001b[A\n",
            "Scoring documents:  75% 6/8 [00:34<00:11,  5.74s/it]\u001b[A\n",
            "Scoring documents:  88% 7/8 [00:40<00:05,  5.98s/it]\u001b[A\n",
            "Scoring documents: 100% 8/8 [00:44<00:00,  5.07s/it]\u001b[A\n",
            "Processing queries:  70% 7/10 [04:18<01:54, 38.05s/it]\n",
            "Re-ranking 30 documents for training query 70563808\n",
            "Original pre-ranking (first 3): ['76578541', '66791385', '44519425']\n",
            "\n",
            "Scoring documents:   0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "Scoring documents:  12% 1/8 [00:04<00:28,  4.06s/it]\u001b[A\n",
            "Scoring documents:  25% 2/8 [00:09<00:30,  5.04s/it]\u001b[A\n",
            "Scoring documents:  38% 3/8 [00:13<00:22,  4.59s/it]\u001b[A\n",
            "Scoring documents:  50% 4/8 [00:17<00:17,  4.37s/it]\u001b[A\n",
            "Scoring documents:  62% 5/8 [00:23<00:14,  4.92s/it]\u001b[A\n",
            "Scoring documents:  75% 6/8 [00:27<00:09,  4.62s/it]\u001b[A\n",
            "Scoring documents:  88% 7/8 [00:32<00:04,  4.51s/it]\u001b[A\n",
            "Scoring documents: 100% 8/8 [00:36<00:00,  4.38s/it]\u001b[A\n",
            "Processing queries:  80% 8/10 [04:54<01:14, 37.46s/it]\n",
            "Re-ranking 30 documents for training query 79482665\n",
            "Original pre-ranking (first 3): ['87571623', '92503480', '68914227']\n",
            "\n",
            "Scoring documents:   0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "Scoring documents:  12% 1/8 [00:04<00:29,  4.28s/it]\u001b[A\n",
            "Scoring documents:  25% 2/8 [00:08<00:25,  4.26s/it]\u001b[A\n",
            "Scoring documents:  38% 3/8 [00:14<00:25,  5.02s/it]\u001b[A\n",
            "Scoring documents:  50% 4/8 [00:19<00:19,  4.99s/it]\u001b[A\n",
            "Scoring documents:  62% 5/8 [00:23<00:14,  4.84s/it]\u001b[A\n",
            "Scoring documents:  75% 6/8 [00:30<00:11,  5.58s/it]\u001b[A\n",
            "Scoring documents:  88% 7/8 [00:35<00:05,  5.14s/it]\u001b[A\n",
            "Scoring documents: 100% 8/8 [00:38<00:00,  4.62s/it]\u001b[A\n",
            "Processing queries:  90% 9/10 [05:33<00:37, 37.86s/it]\n",
            "Re-ranking 30 documents for training query 75800075\n",
            "Original pre-ranking (first 3): ['33464411', '34284570', '74966633']\n",
            "\n",
            "Scoring documents:   0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "Scoring documents:  12% 1/8 [00:05<00:35,  5.06s/it]\u001b[A\n",
            "Scoring documents:  25% 2/8 [00:08<00:25,  4.32s/it]\u001b[A\n",
            "Scoring documents:  38% 3/8 [00:13<00:22,  4.50s/it]\u001b[A\n",
            "Scoring documents:  50% 4/8 [00:17<00:17,  4.43s/it]\u001b[A\n",
            "Scoring documents:  62% 5/8 [00:22<00:13,  4.43s/it]\u001b[A\n",
            "Scoring documents:  75% 6/8 [00:28<00:10,  5.17s/it]\u001b[A\n",
            "Scoring documents:  88% 7/8 [00:32<00:04,  4.71s/it]\u001b[A\n",
            "Scoring documents: 100% 8/8 [00:35<00:00,  4.25s/it]\u001b[A\n",
            "Processing queries: 100% 10/10 [06:09<00:00, 36.91s/it]\n",
            "Saving re-ranked results to predictions_test.json...\n",
            "Re-ranking complete!\n",
            "Number of training queries processed: 10\n",
            "\n",
            "Test set predictions saved to: predictions_test.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# MPNET CLAIMS\n",
        "\n",
        "best_model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
        "\n",
        "best_text_type = \"claims\"\n",
        "max_length = 512\n",
        "\n",
        "print(\"\\nRunning reranking on the test set...\")\n",
        "!python cross_encoder_reranking_train.py \\\n",
        "    --model_name \"{best_model_name}\" \\\n",
        "    --text_type \"{best_text_type}\" \\\n",
        "    --pre_ranking \"{shuffled_pre_ranking_file}\" \\\n",
        "    --queries_list \"{test_queries_file}\" \\\n",
        "    --queries_content \"{queries_content_file}\" \\\n",
        "    --documents_content \"{documents_content_file}\" \\\n",
        "    --output \"{test_predictions_file}\" \\\n",
        "    --max_length {max_length}\n",
        "\n",
        "print(f\"\\nTest set predictions saved to: {test_predictions_file}\")"
      ],
      "metadata": {
        "id": "EbxxPdIlskVt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# BGE TA\n",
        "\n",
        "best_model_name = \"BAAI/bge-large-en\"\n",
        "\n",
        "best_text_type = \"tac1\"\n",
        "max_length = 512\n",
        "\n",
        "print(\"\\nRunning reranking on the test set...\")\n",
        "!python cross_encoder_reranking_train.py \\\n",
        "    --model_name \"{best_model_name}\" \\\n",
        "    --text_type \"{best_text_type}\" \\\n",
        "    --pre_ranking \"{shuffled_pre_ranking_file}\" \\\n",
        "    --queries_list \"{test_queries_file}\" \\\n",
        "    --queries_content \"{queries_content_file}\" \\\n",
        "    --documents_content \"{documents_content_file}\" \\\n",
        "    --output \"{test_predictions_file}\" \\\n",
        "    --max_length {max_length}\n",
        "\n",
        "print(f\"\\nTest set predictions saved to: {test_predictions_file}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mj086sJHGIfc",
        "outputId": "d6ab11c5-d2a2-4aa6-97b5-496a4c07026a"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Running reranking on the test set...\n",
            "Loading training queries from test_queries.json...\n",
            "Loaded 10 training queries\n",
            "Loading pre-ranking data from shuffled_pre_ranking.json...\n",
            "Filtered pre-ranking to 10 training queries\n",
            "Loading query content from queries_content_with_features.json...\n",
            "Loading document content from documents_content_with_features.json...\n",
            "Loading model BAAI/bge-large-en...\n",
            "2025-04-07 16:57:57.396788: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1744045077.475814  135585 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1744045077.496851  135585 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-04-07 16:57:57.597356: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Starting re-ranking process for training queries...\n",
            "Processing queries:   0% 0/10 [00:00<?, ?it/s]\n",
            "Re-ranking 30 documents for training query 103964109\n",
            "Original pre-ranking (first 3): ['94596291', '65451984', '81098918']\n",
            "\n",
            "Scoring documents:   0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "Scoring documents:  12% 1/8 [00:34<04:00, 34.31s/it]\u001b[A\n",
            "Scoring documents:  25% 2/8 [01:06<03:17, 32.91s/it]\u001b[A\n",
            "Scoring documents:  38% 3/8 [01:40<02:46, 33.38s/it]\u001b[A\n",
            "Scoring documents:  50% 4/8 [02:06<02:02, 30.63s/it]\u001b[A\n",
            "Scoring documents:  62% 5/8 [02:35<01:30, 30.16s/it]\u001b[A\n",
            "Scoring documents:  75% 6/8 [03:03<00:58, 29.42s/it]\u001b[A\n",
            "Scoring documents:  88% 7/8 [03:36<00:30, 30.46s/it]\u001b[A\n",
            "Scoring documents: 100% 8/8 [03:52<00:00, 26.01s/it]\u001b[A\n",
            "Processing queries:  10% 1/10 [03:52<34:56, 233.00s/it]\n",
            "Re-ranking 30 documents for training query 72214279\n",
            "Original pre-ranking (first 3): ['4112132', '7126783', '66704221']\n",
            "\n",
            "Scoring documents:   0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "Scoring documents:  12% 1/8 [00:32<03:44, 32.02s/it]\u001b[A\n",
            "Scoring documents:  25% 2/8 [01:05<03:16, 32.73s/it]\u001b[A\n",
            "Scoring documents:  38% 3/8 [01:37<02:41, 32.39s/it]\u001b[A\n",
            "Scoring documents:  50% 4/8 [02:02<01:58, 29.67s/it]\u001b[A\n",
            "Scoring documents:  62% 5/8 [02:35<01:32, 30.91s/it]\u001b[A\n",
            "Scoring documents:  75% 6/8 [03:07<01:02, 31.28s/it]\u001b[A\n",
            "Scoring documents:  88% 7/8 [03:37<00:30, 30.85s/it]\u001b[A\n",
            "Scoring documents: 100% 8/8 [03:51<00:00, 25.29s/it]\u001b[A\n",
            "Processing queries:  20% 2/10 [07:44<30:55, 231.94s/it]\n",
            "Re-ranking 30 documents for training query 68249923\n",
            "Original pre-ranking (first 3): ['35246469', '80619246', '7191590']\n",
            "\n",
            "Scoring documents:   0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "Scoring documents:  12% 1/8 [00:33<03:52, 33.26s/it]\u001b[A\n",
            "Scoring documents:  25% 2/8 [01:05<03:14, 32.46s/it]\u001b[A\n",
            "Scoring documents:  38% 3/8 [01:38<02:44, 32.80s/it]\u001b[A\n",
            "Scoring documents:  50% 4/8 [02:12<02:12, 33.15s/it]\u001b[A\n",
            "Scoring documents:  62% 5/8 [02:45<01:39, 33.20s/it]\u001b[A\n",
            "Scoring documents:  75% 6/8 [03:17<01:05, 32.79s/it]\u001b[A\n",
            "Scoring documents:  88% 7/8 [03:50<00:32, 32.92s/it]\u001b[A\n",
            "Scoring documents: 100% 8/8 [04:09<00:00, 28.35s/it]\u001b[A\n",
            "Processing queries:  30% 3/10 [11:53<27:58, 239.77s/it]\n",
            "Re-ranking 30 documents for training query 79740635\n",
            "Original pre-ranking (first 3): ['72282319', '80126462', '6507475']\n",
            "\n",
            "Scoring documents:   0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "Scoring documents:  12% 1/8 [00:33<03:52, 33.17s/it]\u001b[A\n",
            "Scoring documents:  25% 2/8 [01:05<03:14, 32.43s/it]\u001b[A\n",
            "Scoring documents:  38% 3/8 [01:38<02:43, 32.76s/it]\u001b[A\n",
            "Scoring documents:  50% 4/8 [02:10<02:09, 32.45s/it]\u001b[A\n",
            "Scoring documents:  62% 5/8 [02:43<01:38, 32.67s/it]\u001b[A\n",
            "Scoring documents:  75% 6/8 [03:15<01:04, 32.45s/it]\u001b[A\n",
            "Scoring documents:  88% 7/8 [03:48<00:32, 32.71s/it]\u001b[A\n",
            "Scoring documents: 100% 8/8 [04:07<00:00, 28.19s/it]\u001b[A\n",
            "Processing queries:  40% 4/10 [16:00<24:15, 242.64s/it]\n",
            "Re-ranking 30 documents for training query 100251983\n",
            "Original pre-ranking (first 3): ['82028496', '89295990', '78872192']\n",
            "\n",
            "Scoring documents:   0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "Scoring documents:  12% 1/8 [00:34<04:00, 34.35s/it]\u001b[A\n",
            "Scoring documents:  25% 2/8 [01:06<03:18, 33.12s/it]\u001b[A\n",
            "Scoring documents:  38% 3/8 [01:39<02:44, 32.96s/it]\u001b[A\n",
            "Scoring documents:  50% 4/8 [02:08<02:06, 31.58s/it]\u001b[A\n",
            "Scoring documents:  62% 5/8 [02:40<01:35, 31.72s/it]\u001b[A\n",
            "Scoring documents:  75% 6/8 [03:14<01:04, 32.25s/it]\u001b[A\n",
            "Scoring documents:  88% 7/8 [03:42<00:31, 31.02s/it]\u001b[A\n",
            "Scoring documents: 100% 8/8 [04:00<00:00, 26.74s/it]\u001b[A\n",
            "Processing queries:  50% 5/10 [20:00<20:08, 241.75s/it]\n",
            "Re-ranking 30 documents for training query 76109416\n",
            "Original pre-ranking (first 3): ['67929958', '1212129', '74871145']\n",
            "\n",
            "Scoring documents:   0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "Scoring documents:  12% 1/8 [00:33<03:53, 33.35s/it]\u001b[A\n",
            "Scoring documents:  25% 2/8 [01:05<03:15, 32.66s/it]\u001b[A\n",
            "Scoring documents:  38% 3/8 [01:40<02:47, 33.53s/it]\u001b[A\n",
            "Scoring documents:  50% 4/8 [02:12<02:12, 33.03s/it]\u001b[A\n",
            "Scoring documents:  62% 5/8 [02:45<01:39, 33.12s/it]\u001b[A\n",
            "Scoring documents:  75% 6/8 [03:17<01:05, 32.77s/it]\u001b[A\n",
            "Scoring documents:  88% 7/8 [03:51<00:32, 32.98s/it]\u001b[A\n",
            "Scoring documents: 100% 8/8 [04:09<00:00, 28.40s/it]\u001b[A\n",
            "Processing queries:  60% 6/10 [24:10<16:17, 244.45s/it]\n",
            "Re-ranking 30 documents for training query 85685768\n",
            "Original pre-ranking (first 3): ['5399125', '103242931', '96992323']\n",
            "\n",
            "Scoring documents:   0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "Scoring documents:  12% 1/8 [00:32<03:48, 32.62s/it]\u001b[A\n",
            "Scoring documents:  25% 2/8 [01:04<03:12, 32.16s/it]\u001b[A\n",
            "Scoring documents:  38% 3/8 [01:36<02:39, 31.95s/it]\u001b[A\n",
            "Scoring documents:  50% 4/8 [02:09<02:09, 32.33s/it]\u001b[A\n",
            "Scoring documents:  62% 5/8 [02:41<01:37, 32.54s/it]\u001b[A\n",
            "Scoring documents:  75% 6/8 [03:14<01:05, 32.64s/it]\u001b[A\n",
            "Scoring documents:  88% 7/8 [03:46<00:32, 32.33s/it]\u001b[A\n",
            "Scoring documents: 100% 8/8 [04:06<00:00, 28.37s/it]\u001b[A\n",
            "Processing queries:  70% 7/10 [28:16<12:15, 245.09s/it]\n",
            "Re-ranking 30 documents for training query 70563808\n",
            "Original pre-ranking (first 3): ['76578541', '66791385', '44519425']\n",
            "\n",
            "Scoring documents:   0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "Scoring documents:  12% 1/8 [00:25<03:00, 25.72s/it]\u001b[A\n",
            "Scoring documents:  25% 2/8 [00:48<02:23, 23.94s/it]\u001b[A\n",
            "Scoring documents:  38% 3/8 [01:21<02:20, 28.01s/it]\u001b[A\n",
            "Scoring documents:  50% 4/8 [01:52<01:57, 29.44s/it]\u001b[A\n",
            "Scoring documents:  62% 5/8 [02:25<01:31, 30.59s/it]\u001b[A\n",
            "Scoring documents:  75% 6/8 [02:48<00:56, 28.16s/it]\u001b[A\n",
            "Scoring documents:  88% 7/8 [03:22<00:29, 29.79s/it]\u001b[A\n",
            "Scoring documents: 100% 8/8 [03:41<00:00, 26.48s/it]\u001b[A\n",
            "Processing queries:  80% 8/10 [31:58<07:55, 237.58s/it]\n",
            "Re-ranking 30 documents for training query 79482665\n",
            "Original pre-ranking (first 3): ['87571623', '92503480', '68914227']\n",
            "\n",
            "Scoring documents:   0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "Scoring documents:  12% 1/8 [00:32<03:44, 32.10s/it]\u001b[A\n",
            "Scoring documents:  25% 2/8 [01:03<03:11, 31.95s/it]\u001b[A\n",
            "Scoring documents:  38% 3/8 [01:36<02:41, 32.36s/it]\u001b[A\n",
            "Scoring documents:  50% 4/8 [02:08<02:08, 32.13s/it]\u001b[A\n",
            "Scoring documents:  62% 5/8 [02:41<01:37, 32.36s/it]\u001b[A\n",
            "Scoring documents:  75% 6/8 [03:12<01:04, 32.11s/it]\u001b[A\n",
            "Scoring documents:  88% 7/8 [03:45<00:32, 32.35s/it]\u001b[A\n",
            "Scoring documents: 100% 8/8 [04:04<00:00, 27.98s/it]\u001b[A\n",
            "Processing queries:  90% 9/10 [36:02<03:59, 239.72s/it]\n",
            "Re-ranking 30 documents for training query 75800075\n",
            "Original pre-ranking (first 3): ['33464411', '34284570', '74966633']\n",
            "\n",
            "Scoring documents:   0% 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "Scoring documents:  12% 1/8 [00:34<03:58, 34.02s/it]\u001b[A\n",
            "Scoring documents:  25% 2/8 [01:04<03:11, 31.94s/it]\u001b[A\n",
            "Scoring documents:  38% 3/8 [01:34<02:36, 31.27s/it]\u001b[A\n",
            "Scoring documents:  50% 4/8 [02:07<02:07, 31.90s/it]\u001b[A\n",
            "Scoring documents:  62% 5/8 [02:39<01:35, 31.71s/it]\u001b[A\n",
            "Scoring documents:  75% 6/8 [03:10<01:03, 31.67s/it]\u001b[A\n",
            "Scoring documents:  88% 7/8 [03:42<00:31, 31.71s/it]\u001b[A\n",
            "Scoring documents: 100% 8/8 [04:00<00:00, 27.32s/it]\u001b[A\n",
            "Processing queries: 100% 10/10 [40:03<00:00, 240.31s/it]\n",
            "Saving re-ranked results to prediction2.json...\n",
            "Re-ranking complete!\n",
            "Number of training queries processed: 10\n",
            "\n",
            "Test set predictions saved to: prediction2.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# GEMINI TRY\n",
        "\n",
        "import time\n",
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from google import genai\n",
        "\n",
        "# Replace with your actual Gemini API key\n",
        "GEMINI_API_KEY = \"ton api\"\n",
        "\n",
        "# Initialize Gemini Client\n",
        "client = genai.Client(api_key=GEMINI_API_KEY)\n",
        "embedding_model_name = \"gemini-embedding-exp-03-07\"\n",
        "\n",
        "# Define the paths to your data files\n",
        "train_queries_file = \"train_queries.json\"\n",
        "test_queries_file = \"test_queries.json\"\n",
        "train_gold_mapping_file = \"train_gold_mapping.json\"\n",
        "shuffled_pre_ranking_file = \"shuffled_pre_ranking.json\"\n",
        "queries_content_file = \"queries_content_with_features.json\"\n",
        "documents_content_file = \"documents_content_with_features.json\"\n",
        "\n",
        "# Define the output file for test predictions\n",
        "test_predictions_file = \"predictions_gemini_exp.json\"\n",
        "\n",
        "# Check if necessary data files exist\n",
        "if not all(os.path.exists(f) for f in [test_queries_file, shuffled_pre_ranking_file, queries_content_file, documents_content_file]):\n",
        "    print(\"Error: One or more necessary data files for the test set are missing.\")\n",
        "    exit()\n",
        "else:\n",
        "    print(\"Necessary data files for the test set found.\")\n",
        "\n",
        "def load_json_file(file_path):\n",
        "    \"\"\"Load JSON data from a file\"\"\"\n",
        "    with open(file_path, 'r') as f:\n",
        "        return json.load(f)\n",
        "\n",
        "def save_json_file(data, file_path):\n",
        "    \"\"\"Save data to a JSON file\"\"\"\n",
        "    with open(file_path, 'w') as f:\n",
        "        json.dump(data, f, indent=2)\n",
        "\n",
        "def load_content_data(file_path):\n",
        "    \"\"\"Load content data from a JSON file\"\"\"\n",
        "    with open(file_path, 'r') as f:\n",
        "        data = json.load(f)\n",
        "    # Create a dictionary mapping FAN to Content\n",
        "    content_dict = {item['FAN']: item['Content'] for item in data}\n",
        "    return content_dict\n",
        "\n",
        "def extract_text(content_dict, text_type=\"full\"):\n",
        "    \"\"\"Extract text from patent content based on text_type\"\"\"\n",
        "    if text_type == \"TA\" or text_type == \"title_abstract\":\n",
        "        title = content_dict.get(\"title\", \"\")\n",
        "        abstract = content_dict.get(\"pa01\", \"\")\n",
        "        return f\"{title} {abstract}\".strip()\n",
        "    elif text_type == \"claims\":\n",
        "        claims = \" \".join([v for k, v in content_dict.items() if k.startswith('c-')])\n",
        "        return claims.strip()\n",
        "    elif text_type == \"description\":\n",
        "        description = \" \".join([v for k, v in content_dict.items() if k.startswith('p')])\n",
        "        return description.strip()\n",
        "    elif text_type == \"full\":\n",
        "        all_text = []\n",
        "        if \"title\" in content_dict:\n",
        "            all_text.append(content_dict[\"title\"])\n",
        "        if \"pa01\" in content_dict:\n",
        "            all_text.append(content_dict[\"pa01\"])\n",
        "        for key, value in content_dict.items():\n",
        "            if key not in [\"title\", \"pa01\"]:\n",
        "                all_text.append(value)\n",
        "        return \" \".join(all_text).strip()\n",
        "    elif text_type == \"tac1\":\n",
        "        title = content_dict.get(\"title\", \"\")\n",
        "        abstract = content_dict.get(\"pa01\", \"\")\n",
        "        first_claim = next((v for k, v in content_dict.items() if k.startswith('c-')), \"\")\n",
        "        return f\"{title} {abstract} {first_claim}\".strip()\n",
        "    return \"\"\n",
        "\n",
        "def get_embedding_gemini(text_list, batch_size=1):  # Adjust batch_size\n",
        "    \"\"\"Get embeddings for a list of texts using the specified Gemini embedding model with batching.\"\"\"\n",
        "    embeddings = []\n",
        "    for i in range(0, len(text_list), batch_size):\n",
        "        batch = text_list[i:i + batch_size]\n",
        "        try:\n",
        "            result = client.models.embed_content(\n",
        "                model=embedding_model_name,\n",
        "                contents=batch,  # Send a list of texts\n",
        "            )\n",
        "            if result.embeddings:\n",
        "                for embedding in result.embeddings:\n",
        "                    embeddings.append(embedding.values)\n",
        "            else:\n",
        "                print(f\"Warning: No embeddings returned for the batch starting at index {i}\")\n",
        "                return None\n",
        "            time.sleep(1)  # Add a small delay to avoid rate limits\n",
        "        except Exception as e:\n",
        "            print(f\"Error getting Gemini embeddings for batch starting at index {i}: {e}\")\n",
        "            return None\n",
        "    return embeddings\n",
        "\n",
        "# Load test queries and pre-ranking\n",
        "test_queries = load_json_file(test_queries_file)\n",
        "print(f\"Loaded {len(test_queries)} test queries.\")\n",
        "pre_ranking_test = load_json_file(shuffled_pre_ranking_file)\n",
        "pre_ranking_test_filtered = {fan: docs for fan, docs in pre_ranking_test.items() if fan in test_queries}\n",
        "print(f\"Filtered pre-ranking to {len(pre_ranking_test_filtered)} test queries.\")\n",
        "\n",
        "# Load content data\n",
        "queries_content = load_content_data(queries_content_file)\n",
        "documents_content = load_content_data(documents_content_file)\n",
        "\n",
        "# Rerank using Gemini embeddings\n",
        "re_ranked_predictions = {}\n",
        "best_text_type = \"claims\"  # You can experiment with other text types\n",
        "\n",
        "print(\"\\nStarting reranking process for test queries using Gemini Embedding Model...\")\n",
        "for query_fan, pre_ranked_docs in tqdm(pre_ranking_test_filtered.items(), desc=\"Processing queries\"):\n",
        "    if query_fan not in queries_content:\n",
        "        print(f\"Warning: Query FAN {query_fan} not found in content.\")\n",
        "        re_ranked_predictions[query_fan] = pre_ranked_docs  # Keep original ranking\n",
        "        continue\n",
        "\n",
        "    query_text = extract_text(queries_content[query_fan], best_text_type)\n",
        "    doc_texts = []\n",
        "    doc_fans = []\n",
        "    for doc_fan in pre_ranked_docs:\n",
        "        if doc_fan in documents_content:\n",
        "            doc_texts.append(extract_text(documents_content[doc_fan], best_text_type))\n",
        "            doc_fans.append(doc_fan)\n",
        "        else:\n",
        "            print(f\"Warning: Document FAN {doc_fan} not found in content.\")\n",
        "\n",
        "    if not doc_texts:\n",
        "        re_ranked_predictions[query_fan] = []\n",
        "        continue\n",
        "\n",
        "    all_texts = [query_text] + doc_texts\n",
        "    embeddings = get_embedding_gemini(all_texts)\n",
        "\n",
        "    if embeddings and len(embeddings) == len(all_texts):\n",
        "        query_embedding = embeddings[0]\n",
        "        doc_embeddings = embeddings[1:]\n",
        "        similarity_scores = cosine_similarity([query_embedding], doc_embeddings)[0]\n",
        "        ranked_indices = np.argsort(similarity_scores)[::-1]\n",
        "        re_ranked_predictions[query_fan] = [doc_fans[i] for i in ranked_indices]\n",
        "    else:\n",
        "        print(f\"Warning: Could not get embeddings for query {query_fan}. Keeping original ranking.\")\n",
        "        re_ranked_predictions[query_fan] = pre_ranked_docs\n",
        "\n",
        "# Save the re-ranked predictions\n",
        "save_json_file(re_ranked_predictions, test_predictions_file)\n",
        "print(f\"\\nTest set predictions saved to: {test_predictions_file}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CvRu6UgbN9yi",
        "outputId": "635dc882-4090-48b0-d0ab-66b7778333d1"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Necessary data files for the test set found.\n",
            "Loaded 10 test queries.\n",
            "Filtered pre-ranking to 10 test queries.\n",
            "\n",
            "Starting reranking process for test queries using Gemini Embedding Model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing queries:  10%|█         | 1/10 [00:12<01:53, 12.65s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error getting Gemini embeddings for batch starting at index 6: 429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'Resource has been exhausted (e.g. check quota).', 'status': 'RESOURCE_EXHAUSTED'}}\n",
            "Warning: Could not get embeddings for query 103964109. Keeping original ranking.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing queries:  40%|████      | 4/10 [00:13<00:12,  2.04s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error getting Gemini embeddings for batch starting at index 0: 429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'Resource has been exhausted (e.g. check quota).', 'status': 'RESOURCE_EXHAUSTED'}}\n",
            "Warning: Could not get embeddings for query 72214279. Keeping original ranking.\n",
            "Error getting Gemini embeddings for batch starting at index 0: 429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'Resource has been exhausted (e.g. check quota).', 'status': 'RESOURCE_EXHAUSTED'}}\n",
            "Warning: Could not get embeddings for query 68249923. Keeping original ranking.\n",
            "Error getting Gemini embeddings for batch starting at index 0: 429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'Resource has been exhausted (e.g. check quota).', 'status': 'RESOURCE_EXHAUSTED'}}\n",
            "Warning: Could not get embeddings for query 79740635. Keeping original ranking.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing queries:  60%|██████    | 6/10 [00:13<00:04,  1.11s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error getting Gemini embeddings for batch starting at index 0: 429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'Resource has been exhausted (e.g. check quota).', 'status': 'RESOURCE_EXHAUSTED'}}\n",
            "Warning: Could not get embeddings for query 100251983. Keeping original ranking.\n",
            "Error getting Gemini embeddings for batch starting at index 0: 429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'Resource has been exhausted (e.g. check quota).', 'status': 'RESOURCE_EXHAUSTED'}}\n",
            "Warning: Could not get embeddings for query 76109416. Keeping original ranking.\n",
            "Error getting Gemini embeddings for batch starting at index 0: 429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'Resource has been exhausted (e.g. check quota).', 'status': 'RESOURCE_EXHAUSTED'}}\n",
            "Warning: Could not get embeddings for query 85685768. Keeping original ranking.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing queries: 100%|██████████| 10/10 [00:13<00:00,  1.35s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error getting Gemini embeddings for batch starting at index 0: 429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'Resource has been exhausted (e.g. check quota).', 'status': 'RESOURCE_EXHAUSTED'}}\n",
            "Warning: Could not get embeddings for query 70563808. Keeping original ranking.\n",
            "Error getting Gemini embeddings for batch starting at index 0: 429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'Resource has been exhausted (e.g. check quota).', 'status': 'RESOURCE_EXHAUSTED'}}\n",
            "Warning: Could not get embeddings for query 79482665. Keeping original ranking.\n",
            "Error getting Gemini embeddings for batch starting at index 0: 429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'Resource has been exhausted (e.g. check quota).', 'status': 'RESOURCE_EXHAUSTED'}}\n",
            "Warning: Could not get embeddings for query 75800075. Keeping original ranking.\n",
            "\n",
            "Test set predictions saved to: predictions_gemini_exp.json\n",
            "\n",
            "Remember to download '{test_predictions_file}' and submit it to Codabench.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fuzzywuzzy\n",
        "import os\n",
        "import json\n",
        "import math\n",
        "from collections import Counter\n",
        "from fuzzywuzzy import fuzz  # pip install fuzzywuzzy\n",
        "from nltk.corpus import wordnet  # pip install nltk (and download wordnet data: import nltk; nltk.download('wordnet'))\n",
        "\n",
        "# PATHS\n",
        "train_queries_file = \"train_queries.json\"\n",
        "test_queries_file = \"test_queries.json\"\n",
        "train_gold_mapping_file = \"train_gold_mapping.json\"\n",
        "shuffled_pre_ranking_file = \"shuffled_pre_ranking.json\"\n",
        "queries_content_file = \"queries_content_with_features.json\"\n",
        "documents_content_file = \"documents_content_with_features.json\"\n",
        "test_predictions_file = \"creative_predictions.json\"\n",
        "\n",
        "def load_json_file(file_path):\n",
        "    with open(file_path, 'r') as f:\n",
        "        return json.load(f)\n",
        "\n",
        "if not all(os.path.exists(f) for f in [test_queries_file, shuffled_pre_ranking_file, queries_content_file, documents_content_file, train_gold_mapping_file, queries_content_file, documents_content_file, train_queries_file]):\n",
        "    print(\"Error: One or more necessary data files are missing.\")\n",
        "    exit()\n",
        "else:\n",
        "    print(\"Necessary data files found.\")\n",
        "\n",
        "test_queries = load_json_file(test_queries_file)\n",
        "pre_ranking_test = load_json_file(shuffled_pre_ranking_file)\n",
        "pre_ranking_test_filtered = {fan: docs for fan, docs in pre_ranking_test.items() if fan in test_queries}\n",
        "train_gold_mapping = load_json_file(train_gold_mapping_file)\n",
        "train_queries = load_json_file(train_queries_file)\n",
        "\n",
        "# --- Load and Transform queries_content ---\n",
        "queries_content_list = load_json_file(queries_content_file)\n",
        "queries_content = {}\n",
        "for item in queries_content_list:\n",
        "    patent_id = item.get(\"patent_id\")\n",
        "    if patent_id:\n",
        "        queries_content[patent_id] = item\n",
        "print(f\"Loaded and processed {len(queries_content)} query content items.\")\n",
        "\n",
        "# --- Load and Transform documents_content ---\n",
        "documents_content_list = load_json_file(documents_content_file)\n",
        "documents_content = {}\n",
        "for item in documents_content_list:\n",
        "    patent_id = item.get(\"patent_id\")\n",
        "    if patent_id:\n",
        "        documents_content[patent_id] = item\n",
        "print(f\"Loaded and processed {len(documents_content)} document content items.\")\n",
        "\n",
        "queries_content_train = {k: v for k, v in queries_content.items() if k in train_queries}\n",
        "documents_content_train = documents_content\n",
        "\n",
        "def prepare_feature_weights_tfidf(train_gold_mapping, queries_content, documents_content):\n",
        "    doc_feature_counts = {}\n",
        "    all_docs = {**queries_content, **documents_content}\n",
        "    total_num_docs = len(all_docs)\n",
        "\n",
        "    for doc_id, content in all_docs.items():\n",
        "        features = content.get('features', [])\n",
        "        doc_feature_counts[doc_id] = Counter(features)\n",
        "\n",
        "    feature_doc_frequency = Counter()\n",
        "    for doc_id, counts in doc_feature_counts.items():\n",
        "        for feature in counts:\n",
        "            feature_doc_frequency[feature] += 1\n",
        "\n",
        "    feature_weights_tfidf = {}\n",
        "    for doc_id, counts in doc_feature_counts.items():\n",
        "        for feature, count in counts.items():\n",
        "            tf = count / (sum(counts.values()) + 1e-6)\n",
        "            idf = math.log(total_num_docs / (feature_doc_frequency[feature] + 1) + 1e-6)\n",
        "            feature_weights_tfidf[feature] = feature_weights_tfidf.get(feature, 0) + tf * idf\n",
        "\n",
        "    return dict(feature_weights_tfidf)\n",
        "\n",
        "def calculate_feature_similarity_improved(query_features, doc_features, feature_weights_tfidf=None):\n",
        "    score = 0\n",
        "\n",
        "    # 1. TF-IDF Weighted Overlap\n",
        "    if feature_weights_tfidf:\n",
        "        common_features = set(query_features) & set(doc_features)\n",
        "        for feature in common_features:\n",
        "            score += feature_weights_tfidf.get(feature, 0)\n",
        "\n",
        "    # 2. Fuzzy Matching\n",
        "    fuzzy_score = 0\n",
        "    for q_feature in query_features:\n",
        "        for d_feature in doc_features:\n",
        "            ratio = fuzz.ratio(q_feature, d_feature)\n",
        "            if ratio > 85:  # Increased threshold\n",
        "                fuzzy_score += ratio / 100.0 * 0.2  # Reduced weight\n",
        "\n",
        "    score += fuzzy_score\n",
        "\n",
        "    # 3. N-gram Overlap (bi-grams)\n",
        "    def get_ngrams(text, n):\n",
        "        n_grams = set()\n",
        "        words = text.split()\n",
        "        for i in range(len(words) - n + 1):\n",
        "            n_grams.add(\" \".join(words[i:i+n]))\n",
        "        return n_grams\n",
        "\n",
        "    ngram_overlap_score = 0\n",
        "    for q_feature in query_features:\n",
        "        for d_feature in doc_features:\n",
        "            q_2grams = get_ngrams(q_feature, 2)\n",
        "            d_2grams = get_ngrams(d_feature, 2)\n",
        "            overlap = len(q_2grams & d_2grams)\n",
        "            union = len(q_2grams | d_2grams)\n",
        "            if union > 0:\n",
        "                ngram_overlap_score += overlap / union * 0.1  # Jaccard-like\n",
        "\n",
        "    score += ngram_overlap_score\n",
        "\n",
        "    return score\n",
        "\n",
        "def creative_reranking(pre_ranking, queries_content, documents_content, feature_weights_tfidf=None):\n",
        "    ranked_results = {}\n",
        "    for query_id, initial_ranking in pre_ranking.items():\n",
        "        if query_id in queries_content:\n",
        "            query_features = queries_content[query_id].get('features', [])\n",
        "            scored_documents = []\n",
        "            for doc_id in initial_ranking:\n",
        "                if doc_id in documents_content:\n",
        "                    doc_features = documents_content[doc_id].get('features', [])\n",
        "                    similarity_score = calculate_feature_similarity_improved(\n",
        "                        query_features,\n",
        "                        doc_features,\n",
        "                        feature_weights_tfidf=feature_weights_tfidf\n",
        "                    )\n",
        "                    scored_documents.append((doc_id, similarity_score))\n",
        "            scored_documents.sort(key=lambda item: item[1], reverse=True)\n",
        "            ranked_results[query_id] = [doc_id for doc_id, score in scored_documents]\n",
        "        else:\n",
        "            ranked_results[query_id] = initial_ranking\n",
        "    return ranked_results\n",
        "\n",
        "# --- Prepare Feature Weights using TF-IDF ---\n",
        "feature_weights_tfidf = prepare_feature_weights_tfidf(train_gold_mapping, queries_content_train, documents_content_train)\n",
        "\n",
        "# --- Perform Creative Reranking on Test Set ---\n",
        "reranked_predictions = creative_reranking(\n",
        "    pre_ranking_test_filtered,\n",
        "    queries_content,\n",
        "    documents_content,\n",
        "    feature_weights_tfidf=feature_weights_tfidf\n",
        ")\n",
        "\n",
        "# --- Save Predictions ---\n",
        "with open(test_predictions_file, 'w') as f:\n",
        "    json.dump(reranked_predictions, f, indent=4)\n",
        "\n",
        "print(f\"\\nCreative test set predictions saved to: {test_predictions_file}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KJUIacEyPmfU",
        "outputId": "8ee87e1a-f0af-4833-be9b-4b3567c35ada"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fuzzywuzzy\n",
            "  Downloading fuzzywuzzy-0.18.0-py2.py3-none-any.whl.metadata (4.9 kB)\n",
            "Downloading fuzzywuzzy-0.18.0-py2.py3-none-any.whl (18 kB)\n",
            "Installing collected packages: fuzzywuzzy\n",
            "Successfully installed fuzzywuzzy-0.18.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/fuzzywuzzy/fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
            "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Necessary data files found.\n",
            "Loaded and processed 0 query content items.\n",
            "Loaded and processed 0 document content items.\n",
            "\n",
            "Creative test set predictions saved to: creative_predictions.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1VWRmb2NfHm8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}